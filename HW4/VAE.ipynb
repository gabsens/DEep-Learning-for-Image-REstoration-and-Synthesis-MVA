{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Add, Multiply, Reshape, Flatten, Dropout, BatchNormalization, Activation, ZeroPadding2D, MaxPooling2D, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import mse, binary_crossentropy\n",
    "from keras import backend as K\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_15 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 10), (None, 10),  412180    \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 784)               407824    \n",
      "=================================================================\n",
      "Total params: 820,004\n",
      "Trainable params: 820,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class variational_autoencoder():\n",
    "    def __init__(self,dataset_name='mnist',architecture='mlp'):\n",
    "        \n",
    "        X_train = self.load_data(dataset_name)\n",
    "        optimizer = 'adadelta'#Adam(0.0002, 0.5) #\n",
    "\n",
    "        # image parameters\n",
    "        self.epochs = 30000\n",
    "        self.error_list = np.zeros((self.epochs,1))\n",
    "        self.img_rows = X_train.shape[1]\n",
    "        self.img_cols = X_train.shape[2]\n",
    "        self.img_channels = X_train.shape[3]\n",
    "        self.img_size = X_train.shape[1] * X_train.shape[2] * X_train.shape[3]\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.img_channels)\n",
    "        self.z_dim = 10\n",
    "        self.architecture = architecture\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.vae = self.build_vae()\n",
    "        self.vae.summary()\n",
    "\n",
    "    def build_vae(self):\n",
    "\n",
    "        n_pixels = self.img_rows*self.img_cols*self.img_channels\n",
    "\n",
    "        #BEGIN INSERT CODE\n",
    "        #encoder\n",
    "        input_img = Input(shape=(self.img_rows,self.img_cols,self.img_channels))\n",
    "        input_img_flatten = Flatten()(input_img)\n",
    "        z = Dense(512)(input_img_flatten)\n",
    "        z = LeakyReLU(alpha=0.2)(z)\n",
    "        # mean and variance parameters\n",
    "        z_mean = Dense(self.z_dim)(z)\n",
    "        z_log_var = Dense(self.z_dim)(z)\n",
    "\n",
    "        #sample the latent vector\n",
    "        z_rand = Lambda(self.sampling, output_shape=(self.z_dim,))([z_mean, z_log_var])\n",
    "        #save the encoder\n",
    "        self.encoder = Model(input_img, [z_mean, z_log_var, z_rand], name='encoder')\n",
    "\n",
    "        #build decoder\n",
    "        latent_inputs = Input(shape=(self.z_dim,), name='z_sampling')\n",
    "        y = Dense(512)(latent_inputs)\n",
    "        output_img = LeakyReLU(alpha=0.2)(y)\n",
    "        output_img = Dense(n_pixels)(output_img)\n",
    "        output_img = Activation('sigmoid')(output_img)\n",
    "        self.decoder = Model(latent_inputs, output_img, name='decoder')\n",
    "\n",
    "        #build encoder + decoder (total model)\n",
    "        output_img = self.decoder(self.encoder(input_img)[2])\n",
    "        vae = Model(input_img, output_img, name='vae_mlp')\n",
    "\n",
    "        #create the total model\n",
    "        vae = Model(input_img,output_img)\n",
    "        #define the loss\n",
    "        vae_loss = self.vae_loss(input_img_flatten, output_img,z_mean,z_log_var)\n",
    "        vae.add_loss(vae_loss)\n",
    "        vae.compile(optimizer='adam')\n",
    "\n",
    "        return vae\n",
    "\n",
    "    def vae_loss(self,x,y,z_mean,z_log_var):\n",
    "        # BEGIN INSERT CODE\n",
    "\n",
    "        # reconstruction loss\n",
    "        reconstruction_loss = K.sum(K.binary_crossentropy(x,y), axis=-1)\n",
    "        # KL divergence\n",
    "        kl_loss = K.sum(K.exp(z_log_var) - z_log_var + K.square(z_mean) -1, axis=-1)\n",
    "        # total loss\n",
    "        vae_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        # END FILL IN CODE\n",
    "        return vae_loss\n",
    "\n",
    "    def sampling(self,args):\n",
    "        #Reparameterization trick\n",
    "        z_mean, z_log_var = args\n",
    "        batch_size = K.shape(z_mean)[0]\n",
    "        # sample random vector of size (batch_size,z_dim)\n",
    "        epsilon = K.random_normal(shape=(batch_size,self.z_dim))\n",
    "        z_sigma = K.exp(0.5 * z_log_var)\n",
    "        z_epsilon = Multiply()([z_sigma,epsilon])\n",
    "        z_rand = Add()([z_mean,z_epsilon])\n",
    "        return z_rand\n",
    "\n",
    "    def load_data(self,dataset_name):\n",
    "        # Load the dataset\n",
    "        if(dataset_name == 'mnist'):\n",
    "            (X_train, _), (_, _) = mnist.load_data()\n",
    "        else:\n",
    "            print('Error, unknown database')\n",
    "\n",
    "        # normalise images between 0 and 1\n",
    "        X_train = X_train/255.0\n",
    "        #add a channel dimension, if need be (for mnist data)\n",
    "        if(X_train.ndim ==3):\n",
    "            X_train = np.expand_dims(X_train, axis=3)\n",
    "        return X_train\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "        \n",
    "        #load dataset\n",
    "        X_train = self.load_data(self.dataset_name)\n",
    "\n",
    "        for i in range(0,epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train variational autoencoder\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            curr_batch = X_train[idx,:,:,:]\n",
    "            # Autoencoder training\n",
    "            loss = self.vae.train_on_batch(curr_batch,None)\n",
    "\n",
    "            # print the losses\n",
    "            #print(\"%d [Loss: %f]\" % (i, loss))\n",
    "            #self.error_list[i] = loss\n",
    "\n",
    "            # Save some random generated images and the models at every sample_interval iterations\n",
    "            if (i % sample_interval == 0):\n",
    "                n_images = 5\n",
    "                idx = np.random.randint(0, X_train.shape[0], n_images)\n",
    "                test_imgs = X_train[idx,:,:,:]\n",
    "                self.reconstruct_images(test_imgs,'images/'+self.dataset_name+'_reconstruction_%06d.png' % i)\n",
    "                self.sample_images('images/'+self.dataset_name+'_random_samples_%06d.png' % i)\n",
    "\n",
    "    def reconstruct_images(self, test_imgs, image_filename):\n",
    "        n_images = test_imgs.shape[0]\n",
    "        #get output images\n",
    "        output_imgs = np.reshape(self.vae.predict( test_imgs ),(n_images,self.img_rows,self.img_cols,self.img_channels))\n",
    "        r = 2\n",
    "        c = n_images\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        for j in range(c):\n",
    "            #black and white images\n",
    "            axs[0,j].imshow(test_imgs[j, :,:,0], cmap='gray')\n",
    "            axs[0,j].axis('off')\n",
    "            axs[1,j].imshow(output_imgs[j, :,:,0], cmap='gray')\n",
    "            axs[1,j].axis('off')\n",
    "        fig.savefig(image_filename)\n",
    "        plt.close()\n",
    "\n",
    "    def sample_images(self, image_filename):\n",
    "\n",
    "        n_images = 5    #number of random images to sample\n",
    "        #get output images\n",
    "        z_sample = np.random.normal(0,1,(n_images,self.z_dim))\n",
    "        r = 1\n",
    "        c = n_images\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        for j in range(c):\n",
    "            x_decoded = np.reshape(self.decoder.predict(z_sample) , (n_images,self.img_rows,self.img_cols,self.img_channels))\n",
    "            #black and white images\n",
    "            axs[j].imshow(x_decoded[j, :,:,0], cmap='gray')\n",
    "            axs[j].axis('off')\n",
    "        fig.savefig(image_filename)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    #choose dataset\n",
    "    dataset_name = 'mnist'#\n",
    "\n",
    "    #create AE model\n",
    "    architecture = 'convolutional'#'mlp'#\n",
    "    vae = variational_autoencoder(dataset_name,architecture)#,\n",
    "    is_training = 1\n",
    "\n",
    "    if (is_training ==1):\n",
    "        vae.train(epochs=vae.epochs, batch_size=64, sample_interval=100)\n",
    "        #plt.plot(vae.error_list[30:])\n",
    "        plt.show()\n",
    "    else:\n",
    "        vae.test_images('images/test_images.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000005e-07, 1.00000005e-07])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = K.variable(value=np.array([1,1]), dtype='float64')\n",
    "test2 = K.variable(value=np.array([1,2]), dtype='float64')\n",
    "K.eval(K.binary_crossentropy(test1, test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
