{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, BatchNormalization, Activation, ZeroPadding2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder():\n",
    "    def __init__(self,dataset_name='mnist',architecture='mlp'):\n",
    "        \n",
    "        X_train = self.load_data(dataset_name)\n",
    "        optimizer = 'adadelta'#Adam(0.0002, 0.5) #\n",
    "\n",
    "        # image parameters\n",
    "        self.epochs = 4000\n",
    "        self.error_list = np.zeros((self.epochs,1))\n",
    "        self.img_rows = X_train.shape[1]\n",
    "        self.img_cols = X_train.shape[2]\n",
    "        self.img_channels = X_train.shape[3]\n",
    "        self.img_size = X_train.shape[1] * X_train.shape[2] * X_train.shape[3]\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.img_channels)\n",
    "        self.z_dim = 100\n",
    "        self.architecture = architecture\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        # Build and compile the autoencoder\n",
    "        self.ae = self.build_ae()\n",
    "        self.ae.summary()\n",
    "        #binary cross-entropy loss, because mnist is grey-scale\n",
    "        #you can try out the mse loss as well if you like\n",
    "        self.ae.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "\n",
    "    def build_ae(self):\n",
    "\n",
    "        n_pixels = self.img_rows*self.img_cols*self.img_channels\n",
    "\n",
    "        if (self.architecture == 'mlp'):\n",
    "            # FULLY CONNECTED (MLP)\n",
    "\n",
    "            #BEGIN INSERT CODE\n",
    "            #encoder\n",
    "            input_img = Input(shape=(self.img_rows,self.img_cols,self.img_channels))\n",
    "            z = Flatten()(input_img)\n",
    "            z = Dense(self.z_dim)(z)\n",
    "            z = LeakyReLU(alpha=0.2)(z)\n",
    "            #decoder\n",
    "            #output_img = Input(shape=self.z_dim)\n",
    "            output_img = Dense(n_pixels)(z)\n",
    "            output_img = Activation('sigmoid')(output_img)\n",
    "            output_img = Reshape((self.img_rows,self.img_cols,self.img_channels))(output_img)\n",
    "\n",
    "\n",
    "            #END INSERT CODE\n",
    "        elif(self.architecture == 'convolutional'):\n",
    "            # CONVOLUTIONAL MODEL\n",
    "\n",
    "            #BEGIN INSERT CODE\n",
    "            #encoder\n",
    "            input_img = Input(shape=(self.img_rows,self.img_cols,self.img_channels))\n",
    "            z = Conv2D(8, (3,3), strides=(2,2), padding='same')(input_img)\n",
    "            z = LeakyReLU(alpha=0.2)(z)\n",
    "            z = Conv2D(4, (3,3), strides=(2,2), padding='same')(z)\n",
    "            z = LeakyReLU(alpha=0.2)(z)\n",
    "            z = Flatten()(z)\n",
    "            z = Dense(self.z_dim)(z)\n",
    "            #decoder\n",
    "            output_img = Dense(196)(z)\n",
    "            output_img = LeakyReLU(alpha=0.2)(output_img)\n",
    "            output_img = Reshape((7,7,4))(output_img)\n",
    "            output_img = Conv2DTranspose(4, (3,3), strides=(2,2), padding='same')(output_img)\n",
    "            output_img = LeakyReLU(alpha=0.2)(output_img)\n",
    "            output_img = Conv2DTranspose(1, (3,3), strides=(2,2), padding='same')(output_img)\n",
    "            output_img = Activation('sigmoid')(output_img)\n",
    "            \n",
    "            #END INSERTs CODE\n",
    "\n",
    "        #output the model\n",
    "        return Model(input_img, output_img)\n",
    "\n",
    "\n",
    "    def load_data(self,dataset_name):\n",
    "        # Load the dataset\n",
    "        if(dataset_name == 'mnist'):\n",
    "            (X_train, _), (_, _) = mnist.load_data()\n",
    "        else:\n",
    "            print('Error, unknown database')\n",
    "\n",
    "        # normalise images between 0 and 1\n",
    "        X_train = X_train/255.0\n",
    "        #add a channel dimension, if need be (for mnist data)\n",
    "        if(X_train.ndim ==3):\n",
    "            X_train = np.expand_dims(X_train, axis=3)\n",
    "        return X_train\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "        \n",
    "        #load dataset\n",
    "        X_train = self.load_data(self.dataset_name)\n",
    "\n",
    "        sigma = 20.0/255.0\n",
    "\n",
    "        for i in range(0,epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Autoencoder\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            curr_batch = X_train[idx,:,:,:]\n",
    "            # Autoencoder training\n",
    "            noise = np.expand_dims(np.random.normal(scale=20/255, size=(self.img_rows,self.img_rows)), axis=2)\n",
    "            loss = self.ae.train_on_batch(curr_batch+noise,curr_batch)\n",
    "\n",
    "            # print the losses\n",
    "            print(\"%d [Loss: %f]\" % (i, loss))\n",
    "            self.error_list[i] = loss\n",
    "\n",
    "            # Save some random generated images and the models at every sample_interval iterations\n",
    "            if (i % sample_interval == 0):\n",
    "                n_images = 5\n",
    "                idx = np.random.randint(0, X_train.shape[0], n_images)\n",
    "                test_imgs = X_train[idx,:,:,:]\n",
    "                noise = np.expand_dims(np.random.normal(scale=20/255, size=(n_images,self.img_rows,self.img_rows)),\n",
    "                                   axis=3)\n",
    "                curr_batch = test_imgs + noise\n",
    "                self.test_images(curr_batch,'images/'+self.dataset_name+self.architecture+str(self.z_dim)+'_reconstruction_%06d.png' % i)\n",
    "        self.ae.save('ae_'+self.architecture+'.h5')\n",
    "    def test_images(self, test_imgs, image_filename):\n",
    "        n_images = test_imgs.shape[0]\n",
    "        #get output imagesq\n",
    "        output_imgs = self.ae.predict( test_imgs )\n",
    "        \n",
    "        r = 2\n",
    "        c = n_images\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        for j in range(c):\n",
    "            #black and white images\n",
    "            axs[0,j].imshow(test_imgs[j, :,:,0], cmap='gray')\n",
    "            axs[0,j].axis('off')\n",
    "            axs[1,j].imshow(output_imgs[j, :,:,0], cmap='gray')\n",
    "            axs[1,j].axis('off')\n",
    "        fig.savefig(image_filename)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_17 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)   (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 784)               79184     \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "reshape_17 (Reshape)         (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 157,684\n",
      "Trainable params: 157,684\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0 [Loss: 0.694766]\n",
      "1 [Loss: 0.694207]\n",
      "2 [Loss: 0.691861]\n",
      "3 [Loss: 0.690081]\n",
      "4 [Loss: 0.688026]\n",
      "5 [Loss: 0.684624]\n",
      "6 [Loss: 0.683989]\n",
      "7 [Loss: 0.681661]\n",
      "8 [Loss: 0.679148]\n",
      "9 [Loss: 0.676930]\n",
      "10 [Loss: 0.675036]\n",
      "11 [Loss: 0.671658]\n",
      "12 [Loss: 0.667960]\n",
      "13 [Loss: 0.661936]\n",
      "14 [Loss: 0.658728]\n",
      "15 [Loss: 0.653690]\n",
      "16 [Loss: 0.652364]\n",
      "17 [Loss: 0.643923]\n",
      "18 [Loss: 0.637089]\n",
      "19 [Loss: 0.632917]\n",
      "20 [Loss: 0.628726]\n",
      "21 [Loss: 0.615825]\n",
      "22 [Loss: 0.604527]\n",
      "23 [Loss: 0.601453]\n",
      "24 [Loss: 0.590220]\n",
      "25 [Loss: 0.585047]\n",
      "26 [Loss: 0.569160]\n",
      "27 [Loss: 0.560900]\n",
      "28 [Loss: 0.542203]\n",
      "29 [Loss: 0.532633]\n",
      "30 [Loss: 0.521155]\n",
      "31 [Loss: 0.514275]\n",
      "32 [Loss: 0.506496]\n",
      "33 [Loss: 0.485071]\n",
      "34 [Loss: 0.475722]\n",
      "35 [Loss: 0.459527]\n",
      "36 [Loss: 0.451680]\n",
      "37 [Loss: 0.448555]\n",
      "38 [Loss: 0.431536]\n",
      "39 [Loss: 0.428272]\n",
      "40 [Loss: 0.403213]\n",
      "41 [Loss: 0.396839]\n",
      "42 [Loss: 0.389604]\n",
      "43 [Loss: 0.387467]\n",
      "44 [Loss: 0.370978]\n",
      "45 [Loss: 0.384643]\n",
      "46 [Loss: 0.367862]\n",
      "47 [Loss: 0.362475]\n",
      "48 [Loss: 0.348584]\n",
      "49 [Loss: 0.348807]\n",
      "50 [Loss: 0.341467]\n",
      "51 [Loss: 0.344923]\n",
      "52 [Loss: 0.329797]\n",
      "53 [Loss: 0.332114]\n",
      "54 [Loss: 0.326333]\n",
      "55 [Loss: 0.325023]\n",
      "56 [Loss: 0.303967]\n",
      "57 [Loss: 0.318559]\n",
      "58 [Loss: 0.331055]\n",
      "59 [Loss: 0.322480]\n",
      "60 [Loss: 0.321921]\n",
      "61 [Loss: 0.302329]\n",
      "62 [Loss: 0.312033]\n",
      "63 [Loss: 0.312415]\n",
      "64 [Loss: 0.330784]\n",
      "65 [Loss: 0.303367]\n",
      "66 [Loss: 0.304819]\n",
      "67 [Loss: 0.309241]\n",
      "68 [Loss: 0.302676]\n",
      "69 [Loss: 0.306262]\n",
      "70 [Loss: 0.291432]\n",
      "71 [Loss: 0.296845]\n",
      "72 [Loss: 0.302527]\n",
      "73 [Loss: 0.296876]\n",
      "74 [Loss: 0.295752]\n",
      "75 [Loss: 0.295438]\n",
      "76 [Loss: 0.286989]\n",
      "77 [Loss: 0.286437]\n",
      "78 [Loss: 0.284189]\n",
      "79 [Loss: 0.291654]\n",
      "80 [Loss: 0.288715]\n",
      "81 [Loss: 0.293816]\n",
      "82 [Loss: 0.295776]\n",
      "83 [Loss: 0.291987]\n",
      "84 [Loss: 0.282943]\n",
      "85 [Loss: 0.282885]\n",
      "86 [Loss: 0.290783]\n",
      "87 [Loss: 0.285983]\n",
      "88 [Loss: 0.293748]\n",
      "89 [Loss: 0.293809]\n",
      "90 [Loss: 0.276079]\n",
      "91 [Loss: 0.287677]\n",
      "92 [Loss: 0.287983]\n",
      "93 [Loss: 0.279536]\n",
      "94 [Loss: 0.282343]\n",
      "95 [Loss: 0.281382]\n",
      "96 [Loss: 0.286087]\n",
      "97 [Loss: 0.274305]\n",
      "98 [Loss: 0.271809]\n",
      "99 [Loss: 0.279606]\n",
      "100 [Loss: 0.282001]\n",
      "101 [Loss: 0.284935]\n",
      "102 [Loss: 0.275958]\n",
      "103 [Loss: 0.275586]\n",
      "104 [Loss: 0.293014]\n",
      "105 [Loss: 0.281962]\n",
      "106 [Loss: 0.268077]\n",
      "107 [Loss: 0.278765]\n",
      "108 [Loss: 0.281791]\n",
      "109 [Loss: 0.280953]\n",
      "110 [Loss: 0.269065]\n",
      "111 [Loss: 0.277085]\n",
      "112 [Loss: 0.280247]\n",
      "113 [Loss: 0.284050]\n",
      "114 [Loss: 0.287444]\n",
      "115 [Loss: 0.282161]\n",
      "116 [Loss: 0.274185]\n",
      "117 [Loss: 0.270075]\n",
      "118 [Loss: 0.283374]\n",
      "119 [Loss: 0.287960]\n",
      "120 [Loss: 0.278451]\n",
      "121 [Loss: 0.286963]\n",
      "122 [Loss: 0.270063]\n",
      "123 [Loss: 0.274873]\n",
      "124 [Loss: 0.268703]\n",
      "125 [Loss: 0.281040]\n",
      "126 [Loss: 0.290494]\n",
      "127 [Loss: 0.274024]\n",
      "128 [Loss: 0.282576]\n",
      "129 [Loss: 0.281521]\n",
      "130 [Loss: 0.268244]\n",
      "131 [Loss: 0.268792]\n",
      "132 [Loss: 0.282347]\n",
      "133 [Loss: 0.279633]\n",
      "134 [Loss: 0.279913]\n",
      "135 [Loss: 0.271231]\n",
      "136 [Loss: 0.266275]\n",
      "137 [Loss: 0.282305]\n",
      "138 [Loss: 0.279353]\n",
      "139 [Loss: 0.274669]\n",
      "140 [Loss: 0.283044]\n",
      "141 [Loss: 0.291633]\n",
      "142 [Loss: 0.276599]\n",
      "143 [Loss: 0.275795]\n",
      "144 [Loss: 0.276006]\n",
      "145 [Loss: 0.290631]\n",
      "146 [Loss: 0.267758]\n",
      "147 [Loss: 0.281544]\n",
      "148 [Loss: 0.273362]\n",
      "149 [Loss: 0.274171]\n",
      "150 [Loss: 0.267146]\n",
      "151 [Loss: 0.278892]\n",
      "152 [Loss: 0.265209]\n",
      "153 [Loss: 0.278193]\n",
      "154 [Loss: 0.270735]\n",
      "155 [Loss: 0.279113]\n",
      "156 [Loss: 0.289861]\n",
      "157 [Loss: 0.280186]\n",
      "158 [Loss: 0.275863]\n",
      "159 [Loss: 0.286383]\n",
      "160 [Loss: 0.279858]\n",
      "161 [Loss: 0.285147]\n",
      "162 [Loss: 0.270345]\n",
      "163 [Loss: 0.273903]\n",
      "164 [Loss: 0.272501]\n",
      "165 [Loss: 0.263620]\n",
      "166 [Loss: 0.268446]\n",
      "167 [Loss: 0.266165]\n",
      "168 [Loss: 0.265316]\n",
      "169 [Loss: 0.259487]\n",
      "170 [Loss: 0.275260]\n",
      "171 [Loss: 0.271262]\n",
      "172 [Loss: 0.269893]\n",
      "173 [Loss: 0.266755]\n",
      "174 [Loss: 0.268827]\n",
      "175 [Loss: 0.268740]\n",
      "176 [Loss: 0.263355]\n",
      "177 [Loss: 0.271954]\n",
      "178 [Loss: 0.263686]\n",
      "179 [Loss: 0.283885]\n",
      "180 [Loss: 0.261871]\n",
      "181 [Loss: 0.275292]\n",
      "182 [Loss: 0.280954]\n",
      "183 [Loss: 0.263480]\n",
      "184 [Loss: 0.274067]\n",
      "185 [Loss: 0.276799]\n",
      "186 [Loss: 0.274679]\n",
      "187 [Loss: 0.279093]\n",
      "188 [Loss: 0.260333]\n",
      "189 [Loss: 0.276517]\n",
      "190 [Loss: 0.274111]\n",
      "191 [Loss: 0.277295]\n",
      "192 [Loss: 0.266515]\n",
      "193 [Loss: 0.269462]\n",
      "194 [Loss: 0.268900]\n",
      "195 [Loss: 0.269669]\n",
      "196 [Loss: 0.269133]\n",
      "197 [Loss: 0.260303]\n",
      "198 [Loss: 0.266408]\n",
      "199 [Loss: 0.278838]\n",
      "200 [Loss: 0.270371]\n",
      "201 [Loss: 0.258533]\n",
      "202 [Loss: 0.265776]\n",
      "203 [Loss: 0.260125]\n",
      "204 [Loss: 0.258676]\n",
      "205 [Loss: 0.255636]\n",
      "206 [Loss: 0.262511]\n",
      "207 [Loss: 0.282474]\n",
      "208 [Loss: 0.251476]\n",
      "209 [Loss: 0.264183]\n",
      "210 [Loss: 0.264660]\n",
      "211 [Loss: 0.266361]\n",
      "212 [Loss: 0.258641]\n",
      "213 [Loss: 0.271813]\n",
      "214 [Loss: 0.269401]\n",
      "215 [Loss: 0.248392]\n",
      "216 [Loss: 0.268508]\n",
      "217 [Loss: 0.264984]\n",
      "218 [Loss: 0.266963]\n",
      "219 [Loss: 0.269318]\n",
      "220 [Loss: 0.264545]\n",
      "221 [Loss: 0.265376]\n",
      "222 [Loss: 0.268122]\n",
      "223 [Loss: 0.266598]\n",
      "224 [Loss: 0.264919]\n",
      "225 [Loss: 0.261798]\n",
      "226 [Loss: 0.272105]\n",
      "227 [Loss: 0.260799]\n",
      "228 [Loss: 0.264819]\n",
      "229 [Loss: 0.263907]\n",
      "230 [Loss: 0.260448]\n",
      "231 [Loss: 0.262252]\n",
      "232 [Loss: 0.260645]\n",
      "233 [Loss: 0.259524]\n",
      "234 [Loss: 0.259457]\n",
      "235 [Loss: 0.268997]\n",
      "236 [Loss: 0.269019]\n",
      "237 [Loss: 0.268483]\n",
      "238 [Loss: 0.255255]\n",
      "239 [Loss: 0.273642]\n",
      "240 [Loss: 0.271740]\n",
      "241 [Loss: 0.266391]\n",
      "242 [Loss: 0.262068]\n",
      "243 [Loss: 0.255934]\n",
      "244 [Loss: 0.276912]\n",
      "245 [Loss: 0.263650]\n",
      "246 [Loss: 0.270240]\n",
      "247 [Loss: 0.263139]\n",
      "248 [Loss: 0.255707]\n",
      "249 [Loss: 0.249120]\n",
      "250 [Loss: 0.261326]\n",
      "251 [Loss: 0.260892]\n",
      "252 [Loss: 0.272467]\n",
      "253 [Loss: 0.256292]\n",
      "254 [Loss: 0.258890]\n",
      "255 [Loss: 0.292645]\n",
      "256 [Loss: 0.270486]\n",
      "257 [Loss: 0.257816]\n",
      "258 [Loss: 0.270376]\n",
      "259 [Loss: 0.274069]\n",
      "260 [Loss: 0.264617]\n",
      "261 [Loss: 0.269959]\n",
      "262 [Loss: 0.263035]\n",
      "263 [Loss: 0.255075]\n",
      "264 [Loss: 0.268966]\n",
      "265 [Loss: 0.263428]\n",
      "266 [Loss: 0.262874]\n",
      "267 [Loss: 0.266341]\n",
      "268 [Loss: 0.267710]\n",
      "269 [Loss: 0.261478]\n",
      "270 [Loss: 0.252010]\n",
      "271 [Loss: 0.269917]\n",
      "272 [Loss: 0.263075]\n",
      "273 [Loss: 0.258707]\n",
      "274 [Loss: 0.258999]\n",
      "275 [Loss: 0.257049]\n",
      "276 [Loss: 0.270984]\n",
      "277 [Loss: 0.255400]\n",
      "278 [Loss: 0.254369]\n",
      "279 [Loss: 0.262139]\n",
      "280 [Loss: 0.266375]\n",
      "281 [Loss: 0.264583]\n",
      "282 [Loss: 0.255154]\n",
      "283 [Loss: 0.268004]\n",
      "284 [Loss: 0.260832]\n",
      "285 [Loss: 0.267600]\n",
      "286 [Loss: 0.252718]\n",
      "287 [Loss: 0.253538]\n",
      "288 [Loss: 0.255330]\n",
      "289 [Loss: 0.264101]\n",
      "290 [Loss: 0.259090]\n",
      "291 [Loss: 0.256970]\n",
      "292 [Loss: 0.268620]\n",
      "293 [Loss: 0.254413]\n",
      "294 [Loss: 0.268626]\n",
      "295 [Loss: 0.249183]\n",
      "296 [Loss: 0.258296]\n",
      "297 [Loss: 0.256768]\n",
      "298 [Loss: 0.257418]\n",
      "299 [Loss: 0.264483]\n",
      "300 [Loss: 0.266393]\n",
      "301 [Loss: 0.244116]\n",
      "302 [Loss: 0.248735]\n",
      "303 [Loss: 0.257699]\n",
      "304 [Loss: 0.260366]\n",
      "305 [Loss: 0.255639]\n",
      "306 [Loss: 0.256713]\n",
      "307 [Loss: 0.251490]\n",
      "308 [Loss: 0.258480]\n",
      "309 [Loss: 0.259608]\n",
      "310 [Loss: 0.246494]\n",
      "311 [Loss: 0.254224]\n",
      "312 [Loss: 0.267577]\n",
      "313 [Loss: 0.261277]\n",
      "314 [Loss: 0.258345]\n",
      "315 [Loss: 0.254837]\n",
      "316 [Loss: 0.260056]\n",
      "317 [Loss: 0.238511]\n",
      "318 [Loss: 0.257103]\n",
      "319 [Loss: 0.260984]\n",
      "320 [Loss: 0.258044]\n",
      "321 [Loss: 0.244477]\n",
      "322 [Loss: 0.257310]\n",
      "323 [Loss: 0.244021]\n",
      "324 [Loss: 0.251965]\n",
      "325 [Loss: 0.257532]\n",
      "326 [Loss: 0.269075]\n",
      "327 [Loss: 0.256245]\n",
      "328 [Loss: 0.255213]\n",
      "329 [Loss: 0.262554]\n",
      "330 [Loss: 0.260232]\n",
      "331 [Loss: 0.248954]\n",
      "332 [Loss: 0.261158]\n",
      "333 [Loss: 0.262083]\n",
      "334 [Loss: 0.250646]\n",
      "335 [Loss: 0.260080]\n",
      "336 [Loss: 0.265772]\n",
      "337 [Loss: 0.246369]\n",
      "338 [Loss: 0.250146]\n",
      "339 [Loss: 0.246918]\n",
      "340 [Loss: 0.257006]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "341 [Loss: 0.258969]\n",
      "342 [Loss: 0.264751]\n",
      "343 [Loss: 0.251209]\n",
      "344 [Loss: 0.257185]\n",
      "345 [Loss: 0.243586]\n",
      "346 [Loss: 0.258910]\n",
      "347 [Loss: 0.252685]\n",
      "348 [Loss: 0.243720]\n",
      "349 [Loss: 0.249108]\n",
      "350 [Loss: 0.241536]\n",
      "351 [Loss: 0.247477]\n",
      "352 [Loss: 0.246898]\n",
      "353 [Loss: 0.250206]\n",
      "354 [Loss: 0.242308]\n",
      "355 [Loss: 0.250453]\n",
      "356 [Loss: 0.252562]\n",
      "357 [Loss: 0.263800]\n",
      "358 [Loss: 0.257298]\n",
      "359 [Loss: 0.243052]\n",
      "360 [Loss: 0.251384]\n",
      "361 [Loss: 0.254852]\n",
      "362 [Loss: 0.240351]\n",
      "363 [Loss: 0.245907]\n",
      "364 [Loss: 0.244762]\n",
      "365 [Loss: 0.254535]\n",
      "366 [Loss: 0.247019]\n",
      "367 [Loss: 0.249830]\n",
      "368 [Loss: 0.251373]\n",
      "369 [Loss: 0.250328]\n",
      "370 [Loss: 0.251247]\n",
      "371 [Loss: 0.248682]\n",
      "372 [Loss: 0.249510]\n",
      "373 [Loss: 0.257071]\n",
      "374 [Loss: 0.247088]\n",
      "375 [Loss: 0.261170]\n",
      "376 [Loss: 0.254757]\n",
      "377 [Loss: 0.252140]\n",
      "378 [Loss: 0.253437]\n",
      "379 [Loss: 0.252372]\n",
      "380 [Loss: 0.230490]\n",
      "381 [Loss: 0.249209]\n",
      "382 [Loss: 0.252184]\n",
      "383 [Loss: 0.258177]\n",
      "384 [Loss: 0.254876]\n",
      "385 [Loss: 0.249846]\n",
      "386 [Loss: 0.245049]\n",
      "387 [Loss: 0.235542]\n",
      "388 [Loss: 0.251024]\n",
      "389 [Loss: 0.241496]\n",
      "390 [Loss: 0.257289]\n",
      "391 [Loss: 0.253651]\n",
      "392 [Loss: 0.245014]\n",
      "393 [Loss: 0.254963]\n",
      "394 [Loss: 0.253640]\n",
      "395 [Loss: 0.235591]\n",
      "396 [Loss: 0.255259]\n",
      "397 [Loss: 0.243954]\n",
      "398 [Loss: 0.236241]\n",
      "399 [Loss: 0.238027]\n",
      "400 [Loss: 0.248594]\n",
      "401 [Loss: 0.253437]\n",
      "402 [Loss: 0.251611]\n",
      "403 [Loss: 0.238729]\n",
      "404 [Loss: 0.242045]\n",
      "405 [Loss: 0.249482]\n",
      "406 [Loss: 0.253710]\n",
      "407 [Loss: 0.242530]\n",
      "408 [Loss: 0.242879]\n",
      "409 [Loss: 0.256114]\n",
      "410 [Loss: 0.253240]\n",
      "411 [Loss: 0.253303]\n",
      "412 [Loss: 0.252132]\n",
      "413 [Loss: 0.250734]\n",
      "414 [Loss: 0.235187]\n",
      "415 [Loss: 0.257315]\n",
      "416 [Loss: 0.240362]\n",
      "417 [Loss: 0.242404]\n",
      "418 [Loss: 0.243280]\n",
      "419 [Loss: 0.242303]\n",
      "420 [Loss: 0.253253]\n",
      "421 [Loss: 0.240216]\n",
      "422 [Loss: 0.242591]\n",
      "423 [Loss: 0.247541]\n",
      "424 [Loss: 0.239619]\n",
      "425 [Loss: 0.241644]\n",
      "426 [Loss: 0.245439]\n",
      "427 [Loss: 0.241104]\n",
      "428 [Loss: 0.232327]\n",
      "429 [Loss: 0.239355]\n",
      "430 [Loss: 0.249662]\n",
      "431 [Loss: 0.258216]\n",
      "432 [Loss: 0.248139]\n",
      "433 [Loss: 0.236128]\n",
      "434 [Loss: 0.243876]\n",
      "435 [Loss: 0.234987]\n",
      "436 [Loss: 0.222214]\n",
      "437 [Loss: 0.244232]\n",
      "438 [Loss: 0.247501]\n",
      "439 [Loss: 0.252152]\n",
      "440 [Loss: 0.227665]\n",
      "441 [Loss: 0.243047]\n",
      "442 [Loss: 0.245207]\n",
      "443 [Loss: 0.251760]\n",
      "444 [Loss: 0.246630]\n",
      "445 [Loss: 0.240444]\n",
      "446 [Loss: 0.229975]\n",
      "447 [Loss: 0.229538]\n",
      "448 [Loss: 0.244470]\n",
      "449 [Loss: 0.234417]\n",
      "450 [Loss: 0.222288]\n",
      "451 [Loss: 0.233795]\n",
      "452 [Loss: 0.227515]\n",
      "453 [Loss: 0.237667]\n",
      "454 [Loss: 0.239015]\n",
      "455 [Loss: 0.238738]\n",
      "456 [Loss: 0.236194]\n",
      "457 [Loss: 0.243142]\n",
      "458 [Loss: 0.237668]\n",
      "459 [Loss: 0.233160]\n",
      "460 [Loss: 0.238365]\n",
      "461 [Loss: 0.231808]\n",
      "462 [Loss: 0.231748]\n",
      "463 [Loss: 0.237912]\n",
      "464 [Loss: 0.236982]\n",
      "465 [Loss: 0.233989]\n",
      "466 [Loss: 0.244468]\n",
      "467 [Loss: 0.231806]\n",
      "468 [Loss: 0.245928]\n",
      "469 [Loss: 0.243495]\n",
      "470 [Loss: 0.231741]\n",
      "471 [Loss: 0.248641]\n",
      "472 [Loss: 0.229425]\n",
      "473 [Loss: 0.227047]\n",
      "474 [Loss: 0.244044]\n",
      "475 [Loss: 0.233826]\n",
      "476 [Loss: 0.235104]\n",
      "477 [Loss: 0.233734]\n",
      "478 [Loss: 0.238110]\n",
      "479 [Loss: 0.235311]\n",
      "480 [Loss: 0.240655]\n",
      "481 [Loss: 0.242251]\n",
      "482 [Loss: 0.240751]\n",
      "483 [Loss: 0.230188]\n",
      "484 [Loss: 0.236634]\n",
      "485 [Loss: 0.234880]\n",
      "486 [Loss: 0.226151]\n",
      "487 [Loss: 0.242354]\n",
      "488 [Loss: 0.233505]\n",
      "489 [Loss: 0.232817]\n",
      "490 [Loss: 0.238491]\n",
      "491 [Loss: 0.243855]\n",
      "492 [Loss: 0.239865]\n",
      "493 [Loss: 0.236178]\n",
      "494 [Loss: 0.223971]\n",
      "495 [Loss: 0.230479]\n",
      "496 [Loss: 0.238419]\n",
      "497 [Loss: 0.236853]\n",
      "498 [Loss: 0.244419]\n",
      "499 [Loss: 0.244605]\n",
      "500 [Loss: 0.223937]\n",
      "501 [Loss: 0.223741]\n",
      "502 [Loss: 0.236316]\n",
      "503 [Loss: 0.241738]\n",
      "504 [Loss: 0.252010]\n",
      "505 [Loss: 0.234209]\n",
      "506 [Loss: 0.221756]\n",
      "507 [Loss: 0.232182]\n",
      "508 [Loss: 0.232723]\n",
      "509 [Loss: 0.240397]\n",
      "510 [Loss: 0.225155]\n",
      "511 [Loss: 0.233210]\n",
      "512 [Loss: 0.233104]\n",
      "513 [Loss: 0.245130]\n",
      "514 [Loss: 0.224326]\n",
      "515 [Loss: 0.234995]\n",
      "516 [Loss: 0.233733]\n",
      "517 [Loss: 0.238335]\n",
      "518 [Loss: 0.229970]\n",
      "519 [Loss: 0.245040]\n",
      "520 [Loss: 0.230195]\n",
      "521 [Loss: 0.238284]\n",
      "522 [Loss: 0.229772]\n",
      "523 [Loss: 0.232521]\n",
      "524 [Loss: 0.228343]\n",
      "525 [Loss: 0.240162]\n",
      "526 [Loss: 0.243094]\n",
      "527 [Loss: 0.223602]\n",
      "528 [Loss: 0.219791]\n",
      "529 [Loss: 0.215604]\n",
      "530 [Loss: 0.229728]\n",
      "531 [Loss: 0.223195]\n",
      "532 [Loss: 0.237441]\n",
      "533 [Loss: 0.230359]\n",
      "534 [Loss: 0.234852]\n",
      "535 [Loss: 0.230932]\n",
      "536 [Loss: 0.241551]\n",
      "537 [Loss: 0.234081]\n",
      "538 [Loss: 0.217408]\n",
      "539 [Loss: 0.224764]\n",
      "540 [Loss: 0.219967]\n",
      "541 [Loss: 0.220869]\n",
      "542 [Loss: 0.235693]\n",
      "543 [Loss: 0.226168]\n",
      "544 [Loss: 0.223529]\n",
      "545 [Loss: 0.239388]\n",
      "546 [Loss: 0.239795]\n",
      "547 [Loss: 0.243057]\n",
      "548 [Loss: 0.219581]\n",
      "549 [Loss: 0.233111]\n",
      "550 [Loss: 0.229877]\n",
      "551 [Loss: 0.232862]\n",
      "552 [Loss: 0.223971]\n",
      "553 [Loss: 0.227753]\n",
      "554 [Loss: 0.225398]\n",
      "555 [Loss: 0.226556]\n",
      "556 [Loss: 0.223849]\n",
      "557 [Loss: 0.221361]\n",
      "558 [Loss: 0.227150]\n",
      "559 [Loss: 0.232759]\n",
      "560 [Loss: 0.234778]\n",
      "561 [Loss: 0.214498]\n",
      "562 [Loss: 0.228165]\n",
      "563 [Loss: 0.230186]\n",
      "564 [Loss: 0.233567]\n",
      "565 [Loss: 0.221176]\n",
      "566 [Loss: 0.240376]\n",
      "567 [Loss: 0.220081]\n",
      "568 [Loss: 0.232496]\n",
      "569 [Loss: 0.228318]\n",
      "570 [Loss: 0.218660]\n",
      "571 [Loss: 0.218666]\n",
      "572 [Loss: 0.217664]\n",
      "573 [Loss: 0.220578]\n",
      "574 [Loss: 0.218591]\n",
      "575 [Loss: 0.227915]\n",
      "576 [Loss: 0.223114]\n",
      "577 [Loss: 0.224559]\n",
      "578 [Loss: 0.244392]\n",
      "579 [Loss: 0.233598]\n",
      "580 [Loss: 0.215814]\n",
      "581 [Loss: 0.231836]\n",
      "582 [Loss: 0.239141]\n",
      "583 [Loss: 0.221954]\n",
      "584 [Loss: 0.227943]\n",
      "585 [Loss: 0.225032]\n",
      "586 [Loss: 0.230191]\n",
      "587 [Loss: 0.236961]\n",
      "588 [Loss: 0.215863]\n",
      "589 [Loss: 0.219223]\n",
      "590 [Loss: 0.211171]\n",
      "591 [Loss: 0.233879]\n",
      "592 [Loss: 0.221292]\n",
      "593 [Loss: 0.226975]\n",
      "594 [Loss: 0.226243]\n",
      "595 [Loss: 0.226672]\n",
      "596 [Loss: 0.229799]\n",
      "597 [Loss: 0.220078]\n",
      "598 [Loss: 0.212833]\n",
      "599 [Loss: 0.225113]\n",
      "600 [Loss: 0.228737]\n",
      "601 [Loss: 0.219911]\n",
      "602 [Loss: 0.229831]\n",
      "603 [Loss: 0.227001]\n",
      "604 [Loss: 0.216961]\n",
      "605 [Loss: 0.228274]\n",
      "606 [Loss: 0.226092]\n",
      "607 [Loss: 0.221329]\n",
      "608 [Loss: 0.231006]\n",
      "609 [Loss: 0.221787]\n",
      "610 [Loss: 0.218699]\n",
      "611 [Loss: 0.234476]\n",
      "612 [Loss: 0.222335]\n",
      "613 [Loss: 0.223185]\n",
      "614 [Loss: 0.224715]\n",
      "615 [Loss: 0.213548]\n",
      "616 [Loss: 0.216195]\n",
      "617 [Loss: 0.222793]\n",
      "618 [Loss: 0.227526]\n",
      "619 [Loss: 0.214004]\n",
      "620 [Loss: 0.221463]\n",
      "621 [Loss: 0.224878]\n",
      "622 [Loss: 0.225220]\n",
      "623 [Loss: 0.216772]\n",
      "624 [Loss: 0.219945]\n",
      "625 [Loss: 0.228847]\n",
      "626 [Loss: 0.219052]\n",
      "627 [Loss: 0.206038]\n",
      "628 [Loss: 0.229711]\n",
      "629 [Loss: 0.218281]\n",
      "630 [Loss: 0.211135]\n",
      "631 [Loss: 0.222220]\n",
      "632 [Loss: 0.213020]\n",
      "633 [Loss: 0.225209]\n",
      "634 [Loss: 0.219158]\n",
      "635 [Loss: 0.212191]\n",
      "636 [Loss: 0.219646]\n",
      "637 [Loss: 0.218861]\n",
      "638 [Loss: 0.222862]\n",
      "639 [Loss: 0.226219]\n",
      "640 [Loss: 0.215753]\n",
      "641 [Loss: 0.221262]\n",
      "642 [Loss: 0.222444]\n",
      "643 [Loss: 0.208722]\n",
      "644 [Loss: 0.221194]\n",
      "645 [Loss: 0.220799]\n",
      "646 [Loss: 0.228226]\n",
      "647 [Loss: 0.213067]\n",
      "648 [Loss: 0.217820]\n",
      "649 [Loss: 0.225755]\n",
      "650 [Loss: 0.212783]\n",
      "651 [Loss: 0.205456]\n",
      "652 [Loss: 0.204236]\n",
      "653 [Loss: 0.215773]\n",
      "654 [Loss: 0.220804]\n",
      "655 [Loss: 0.213539]\n",
      "656 [Loss: 0.217711]\n",
      "657 [Loss: 0.217146]\n",
      "658 [Loss: 0.214614]\n",
      "659 [Loss: 0.223483]\n",
      "660 [Loss: 0.205185]\n",
      "661 [Loss: 0.213982]\n",
      "662 [Loss: 0.210640]\n",
      "663 [Loss: 0.215235]\n",
      "664 [Loss: 0.207243]\n",
      "665 [Loss: 0.227007]\n",
      "666 [Loss: 0.220747]\n",
      "667 [Loss: 0.212507]\n",
      "668 [Loss: 0.209811]\n",
      "669 [Loss: 0.209122]\n",
      "670 [Loss: 0.218181]\n",
      "671 [Loss: 0.229773]\n",
      "672 [Loss: 0.211750]\n",
      "673 [Loss: 0.227892]\n",
      "674 [Loss: 0.227718]\n",
      "675 [Loss: 0.221109]\n",
      "676 [Loss: 0.216307]\n",
      "677 [Loss: 0.226191]\n",
      "678 [Loss: 0.209329]\n",
      "679 [Loss: 0.208110]\n",
      "680 [Loss: 0.217951]\n",
      "681 [Loss: 0.212369]\n",
      "682 [Loss: 0.206638]\n",
      "683 [Loss: 0.213059]\n",
      "684 [Loss: 0.216367]\n",
      "685 [Loss: 0.231349]\n",
      "686 [Loss: 0.221834]\n",
      "687 [Loss: 0.203498]\n",
      "688 [Loss: 0.222683]\n",
      "689 [Loss: 0.217031]\n",
      "690 [Loss: 0.210451]\n",
      "691 [Loss: 0.221979]\n",
      "692 [Loss: 0.217552]\n",
      "693 [Loss: 0.202144]\n",
      "694 [Loss: 0.198963]\n",
      "695 [Loss: 0.206117]\n",
      "696 [Loss: 0.215504]\n",
      "697 [Loss: 0.226350]\n",
      "698 [Loss: 0.213107]\n",
      "699 [Loss: 0.201972]\n",
      "700 [Loss: 0.212146]\n",
      "701 [Loss: 0.222230]\n",
      "702 [Loss: 0.205566]\n",
      "703 [Loss: 0.205819]\n",
      "704 [Loss: 0.221654]\n",
      "705 [Loss: 0.215583]\n",
      "706 [Loss: 0.209252]\n",
      "707 [Loss: 0.217832]\n",
      "708 [Loss: 0.208884]\n",
      "709 [Loss: 0.210550]\n",
      "710 [Loss: 0.205037]\n",
      "711 [Loss: 0.215312]\n",
      "712 [Loss: 0.217458]\n",
      "713 [Loss: 0.213983]\n",
      "714 [Loss: 0.202969]\n",
      "715 [Loss: 0.210326]\n",
      "716 [Loss: 0.210403]\n",
      "717 [Loss: 0.204136]\n",
      "718 [Loss: 0.216945]\n",
      "719 [Loss: 0.205325]\n",
      "720 [Loss: 0.206250]\n",
      "721 [Loss: 0.209102]\n",
      "722 [Loss: 0.208696]\n",
      "723 [Loss: 0.214853]\n",
      "724 [Loss: 0.197962]\n",
      "725 [Loss: 0.221464]\n",
      "726 [Loss: 0.219671]\n",
      "727 [Loss: 0.219931]\n",
      "728 [Loss: 0.208517]\n",
      "729 [Loss: 0.210768]\n",
      "730 [Loss: 0.212124]\n",
      "731 [Loss: 0.211380]\n",
      "732 [Loss: 0.211321]\n",
      "733 [Loss: 0.207560]\n",
      "734 [Loss: 0.209417]\n",
      "735 [Loss: 0.196690]\n",
      "736 [Loss: 0.229318]\n",
      "737 [Loss: 0.213770]\n",
      "738 [Loss: 0.203409]\n",
      "739 [Loss: 0.219533]\n",
      "740 [Loss: 0.220454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "741 [Loss: 0.220544]\n",
      "742 [Loss: 0.220136]\n",
      "743 [Loss: 0.209384]\n",
      "744 [Loss: 0.210715]\n",
      "745 [Loss: 0.232204]\n",
      "746 [Loss: 0.217462]\n",
      "747 [Loss: 0.204003]\n",
      "748 [Loss: 0.205516]\n",
      "749 [Loss: 0.218620]\n",
      "750 [Loss: 0.228744]\n",
      "751 [Loss: 0.200334]\n",
      "752 [Loss: 0.216553]\n",
      "753 [Loss: 0.216506]\n",
      "754 [Loss: 0.205893]\n",
      "755 [Loss: 0.211039]\n",
      "756 [Loss: 0.216022]\n",
      "757 [Loss: 0.208821]\n",
      "758 [Loss: 0.213637]\n",
      "759 [Loss: 0.205958]\n",
      "760 [Loss: 0.208756]\n",
      "761 [Loss: 0.221154]\n",
      "762 [Loss: 0.204115]\n",
      "763 [Loss: 0.196966]\n",
      "764 [Loss: 0.205289]\n",
      "765 [Loss: 0.202239]\n",
      "766 [Loss: 0.199824]\n",
      "767 [Loss: 0.219444]\n",
      "768 [Loss: 0.193790]\n",
      "769 [Loss: 0.210461]\n",
      "770 [Loss: 0.204813]\n",
      "771 [Loss: 0.200804]\n",
      "772 [Loss: 0.190478]\n",
      "773 [Loss: 0.211490]\n",
      "774 [Loss: 0.202966]\n",
      "775 [Loss: 0.209147]\n",
      "776 [Loss: 0.207104]\n",
      "777 [Loss: 0.209759]\n",
      "778 [Loss: 0.213899]\n",
      "779 [Loss: 0.210389]\n",
      "780 [Loss: 0.216835]\n",
      "781 [Loss: 0.213082]\n",
      "782 [Loss: 0.215767]\n",
      "783 [Loss: 0.211950]\n",
      "784 [Loss: 0.215750]\n",
      "785 [Loss: 0.206432]\n",
      "786 [Loss: 0.220255]\n",
      "787 [Loss: 0.209824]\n",
      "788 [Loss: 0.201484]\n",
      "789 [Loss: 0.208671]\n",
      "790 [Loss: 0.211482]\n",
      "791 [Loss: 0.200411]\n",
      "792 [Loss: 0.209254]\n",
      "793 [Loss: 0.210005]\n",
      "794 [Loss: 0.191667]\n",
      "795 [Loss: 0.198573]\n",
      "796 [Loss: 0.213932]\n",
      "797 [Loss: 0.209779]\n",
      "798 [Loss: 0.219400]\n",
      "799 [Loss: 0.212005]\n",
      "800 [Loss: 0.208219]\n",
      "801 [Loss: 0.206972]\n",
      "802 [Loss: 0.210548]\n",
      "803 [Loss: 0.194536]\n",
      "804 [Loss: 0.214229]\n",
      "805 [Loss: 0.214638]\n",
      "806 [Loss: 0.209013]\n",
      "807 [Loss: 0.204612]\n",
      "808 [Loss: 0.202159]\n",
      "809 [Loss: 0.206520]\n",
      "810 [Loss: 0.215916]\n",
      "811 [Loss: 0.207520]\n",
      "812 [Loss: 0.199156]\n",
      "813 [Loss: 0.215880]\n",
      "814 [Loss: 0.207442]\n",
      "815 [Loss: 0.196312]\n",
      "816 [Loss: 0.211154]\n",
      "817 [Loss: 0.209263]\n",
      "818 [Loss: 0.216240]\n",
      "819 [Loss: 0.200353]\n",
      "820 [Loss: 0.207469]\n",
      "821 [Loss: 0.201957]\n",
      "822 [Loss: 0.186025]\n",
      "823 [Loss: 0.208618]\n",
      "824 [Loss: 0.201161]\n",
      "825 [Loss: 0.196433]\n",
      "826 [Loss: 0.201369]\n",
      "827 [Loss: 0.201953]\n",
      "828 [Loss: 0.210217]\n",
      "829 [Loss: 0.206537]\n",
      "830 [Loss: 0.198984]\n",
      "831 [Loss: 0.207700]\n",
      "832 [Loss: 0.197175]\n",
      "833 [Loss: 0.212895]\n",
      "834 [Loss: 0.194183]\n",
      "835 [Loss: 0.201748]\n",
      "836 [Loss: 0.194764]\n",
      "837 [Loss: 0.207126]\n",
      "838 [Loss: 0.212261]\n",
      "839 [Loss: 0.203666]\n",
      "840 [Loss: 0.208202]\n",
      "841 [Loss: 0.203287]\n",
      "842 [Loss: 0.206992]\n",
      "843 [Loss: 0.213013]\n",
      "844 [Loss: 0.202651]\n",
      "845 [Loss: 0.200920]\n",
      "846 [Loss: 0.201505]\n",
      "847 [Loss: 0.215497]\n",
      "848 [Loss: 0.193891]\n",
      "849 [Loss: 0.206368]\n",
      "850 [Loss: 0.202162]\n",
      "851 [Loss: 0.207953]\n",
      "852 [Loss: 0.199601]\n",
      "853 [Loss: 0.192262]\n",
      "854 [Loss: 0.186891]\n",
      "855 [Loss: 0.206881]\n",
      "856 [Loss: 0.206002]\n",
      "857 [Loss: 0.205011]\n",
      "858 [Loss: 0.201983]\n",
      "859 [Loss: 0.199208]\n",
      "860 [Loss: 0.198032]\n",
      "861 [Loss: 0.207499]\n",
      "862 [Loss: 0.196675]\n",
      "863 [Loss: 0.195413]\n",
      "864 [Loss: 0.207510]\n",
      "865 [Loss: 0.198246]\n",
      "866 [Loss: 0.212121]\n",
      "867 [Loss: 0.212080]\n",
      "868 [Loss: 0.202753]\n",
      "869 [Loss: 0.201727]\n",
      "870 [Loss: 0.202563]\n",
      "871 [Loss: 0.197452]\n",
      "872 [Loss: 0.211615]\n",
      "873 [Loss: 0.204912]\n",
      "874 [Loss: 0.200249]\n",
      "875 [Loss: 0.207905]\n",
      "876 [Loss: 0.200513]\n",
      "877 [Loss: 0.201837]\n",
      "878 [Loss: 0.206432]\n",
      "879 [Loss: 0.199846]\n",
      "880 [Loss: 0.200456]\n",
      "881 [Loss: 0.202159]\n",
      "882 [Loss: 0.202160]\n",
      "883 [Loss: 0.183586]\n",
      "884 [Loss: 0.205468]\n",
      "885 [Loss: 0.201667]\n",
      "886 [Loss: 0.211608]\n",
      "887 [Loss: 0.190856]\n",
      "888 [Loss: 0.209539]\n",
      "889 [Loss: 0.194218]\n",
      "890 [Loss: 0.218799]\n",
      "891 [Loss: 0.217509]\n",
      "892 [Loss: 0.192861]\n",
      "893 [Loss: 0.197223]\n",
      "894 [Loss: 0.194046]\n",
      "895 [Loss: 0.197606]\n",
      "896 [Loss: 0.195885]\n",
      "897 [Loss: 0.191979]\n",
      "898 [Loss: 0.211226]\n",
      "899 [Loss: 0.192963]\n",
      "900 [Loss: 0.213460]\n",
      "901 [Loss: 0.200969]\n",
      "902 [Loss: 0.205053]\n",
      "903 [Loss: 0.195414]\n",
      "904 [Loss: 0.204628]\n",
      "905 [Loss: 0.193264]\n",
      "906 [Loss: 0.195282]\n",
      "907 [Loss: 0.197278]\n",
      "908 [Loss: 0.203517]\n",
      "909 [Loss: 0.192492]\n",
      "910 [Loss: 0.207083]\n",
      "911 [Loss: 0.195754]\n",
      "912 [Loss: 0.192130]\n",
      "913 [Loss: 0.195361]\n",
      "914 [Loss: 0.211721]\n",
      "915 [Loss: 0.184321]\n",
      "916 [Loss: 0.194496]\n",
      "917 [Loss: 0.192255]\n",
      "918 [Loss: 0.194407]\n",
      "919 [Loss: 0.196975]\n",
      "920 [Loss: 0.195700]\n",
      "921 [Loss: 0.192922]\n",
      "922 [Loss: 0.194303]\n",
      "923 [Loss: 0.191359]\n",
      "924 [Loss: 0.189214]\n",
      "925 [Loss: 0.204349]\n",
      "926 [Loss: 0.195644]\n",
      "927 [Loss: 0.196439]\n",
      "928 [Loss: 0.194591]\n",
      "929 [Loss: 0.199758]\n",
      "930 [Loss: 0.199582]\n",
      "931 [Loss: 0.183238]\n",
      "932 [Loss: 0.186342]\n",
      "933 [Loss: 0.200739]\n",
      "934 [Loss: 0.206280]\n",
      "935 [Loss: 0.210136]\n",
      "936 [Loss: 0.204844]\n",
      "937 [Loss: 0.204057]\n",
      "938 [Loss: 0.201634]\n",
      "939 [Loss: 0.188342]\n",
      "940 [Loss: 0.197168]\n",
      "941 [Loss: 0.197037]\n",
      "942 [Loss: 0.206052]\n",
      "943 [Loss: 0.189307]\n",
      "944 [Loss: 0.199236]\n",
      "945 [Loss: 0.188290]\n",
      "946 [Loss: 0.183737]\n",
      "947 [Loss: 0.188692]\n",
      "948 [Loss: 0.192613]\n",
      "949 [Loss: 0.191925]\n",
      "950 [Loss: 0.186302]\n",
      "951 [Loss: 0.191479]\n",
      "952 [Loss: 0.199366]\n",
      "953 [Loss: 0.192086]\n",
      "954 [Loss: 0.190814]\n",
      "955 [Loss: 0.183793]\n",
      "956 [Loss: 0.200827]\n",
      "957 [Loss: 0.192759]\n",
      "958 [Loss: 0.191654]\n",
      "959 [Loss: 0.194109]\n",
      "960 [Loss: 0.189694]\n",
      "961 [Loss: 0.206059]\n",
      "962 [Loss: 0.196268]\n",
      "963 [Loss: 0.190427]\n",
      "964 [Loss: 0.196662]\n",
      "965 [Loss: 0.186393]\n",
      "966 [Loss: 0.194141]\n",
      "967 [Loss: 0.191033]\n",
      "968 [Loss: 0.191202]\n",
      "969 [Loss: 0.188499]\n",
      "970 [Loss: 0.195456]\n",
      "971 [Loss: 0.187976]\n",
      "972 [Loss: 0.196238]\n",
      "973 [Loss: 0.189575]\n",
      "974 [Loss: 0.205955]\n",
      "975 [Loss: 0.200541]\n",
      "976 [Loss: 0.203187]\n",
      "977 [Loss: 0.196179]\n",
      "978 [Loss: 0.198872]\n",
      "979 [Loss: 0.192261]\n",
      "980 [Loss: 0.188990]\n",
      "981 [Loss: 0.190722]\n",
      "982 [Loss: 0.198270]\n",
      "983 [Loss: 0.198304]\n",
      "984 [Loss: 0.197830]\n",
      "985 [Loss: 0.193633]\n",
      "986 [Loss: 0.190765]\n",
      "987 [Loss: 0.205464]\n",
      "988 [Loss: 0.192661]\n",
      "989 [Loss: 0.197548]\n",
      "990 [Loss: 0.188169]\n",
      "991 [Loss: 0.196090]\n",
      "992 [Loss: 0.193619]\n",
      "993 [Loss: 0.191245]\n",
      "994 [Loss: 0.191385]\n",
      "995 [Loss: 0.195462]\n",
      "996 [Loss: 0.195480]\n",
      "997 [Loss: 0.203545]\n",
      "998 [Loss: 0.206686]\n",
      "999 [Loss: 0.192478]\n",
      "1000 [Loss: 0.188888]\n",
      "1001 [Loss: 0.185357]\n",
      "1002 [Loss: 0.205073]\n",
      "1003 [Loss: 0.196895]\n",
      "1004 [Loss: 0.190983]\n",
      "1005 [Loss: 0.191611]\n",
      "1006 [Loss: 0.191281]\n",
      "1007 [Loss: 0.193569]\n",
      "1008 [Loss: 0.198960]\n",
      "1009 [Loss: 0.193314]\n",
      "1010 [Loss: 0.196378]\n",
      "1011 [Loss: 0.188841]\n",
      "1012 [Loss: 0.192850]\n",
      "1013 [Loss: 0.196244]\n",
      "1014 [Loss: 0.194366]\n",
      "1015 [Loss: 0.198374]\n",
      "1016 [Loss: 0.202563]\n",
      "1017 [Loss: 0.197154]\n",
      "1018 [Loss: 0.187440]\n",
      "1019 [Loss: 0.191637]\n",
      "1020 [Loss: 0.193415]\n",
      "1021 [Loss: 0.194762]\n",
      "1022 [Loss: 0.196873]\n",
      "1023 [Loss: 0.185364]\n",
      "1024 [Loss: 0.180763]\n",
      "1025 [Loss: 0.177794]\n",
      "1026 [Loss: 0.198152]\n",
      "1027 [Loss: 0.189813]\n",
      "1028 [Loss: 0.182367]\n",
      "1029 [Loss: 0.200325]\n",
      "1030 [Loss: 0.186530]\n",
      "1031 [Loss: 0.192150]\n",
      "1032 [Loss: 0.193696]\n",
      "1033 [Loss: 0.192864]\n",
      "1034 [Loss: 0.189148]\n",
      "1035 [Loss: 0.198143]\n",
      "1036 [Loss: 0.183174]\n",
      "1037 [Loss: 0.193870]\n",
      "1038 [Loss: 0.182793]\n",
      "1039 [Loss: 0.197322]\n",
      "1040 [Loss: 0.192304]\n",
      "1041 [Loss: 0.196597]\n",
      "1042 [Loss: 0.196916]\n",
      "1043 [Loss: 0.195102]\n",
      "1044 [Loss: 0.200478]\n",
      "1045 [Loss: 0.194010]\n",
      "1046 [Loss: 0.207299]\n",
      "1047 [Loss: 0.182412]\n",
      "1048 [Loss: 0.178882]\n",
      "1049 [Loss: 0.198536]\n",
      "1050 [Loss: 0.182924]\n",
      "1051 [Loss: 0.184935]\n",
      "1052 [Loss: 0.187546]\n",
      "1053 [Loss: 0.190164]\n",
      "1054 [Loss: 0.200100]\n",
      "1055 [Loss: 0.183068]\n",
      "1056 [Loss: 0.182026]\n",
      "1057 [Loss: 0.198063]\n",
      "1058 [Loss: 0.194442]\n",
      "1059 [Loss: 0.195161]\n",
      "1060 [Loss: 0.195769]\n",
      "1061 [Loss: 0.190232]\n",
      "1062 [Loss: 0.195359]\n",
      "1063 [Loss: 0.179049]\n",
      "1064 [Loss: 0.190437]\n",
      "1065 [Loss: 0.204300]\n",
      "1066 [Loss: 0.182862]\n",
      "1067 [Loss: 0.193245]\n",
      "1068 [Loss: 0.187701]\n",
      "1069 [Loss: 0.179398]\n",
      "1070 [Loss: 0.187047]\n",
      "1071 [Loss: 0.186621]\n",
      "1072 [Loss: 0.190385]\n",
      "1073 [Loss: 0.182272]\n",
      "1074 [Loss: 0.191534]\n",
      "1075 [Loss: 0.200138]\n",
      "1076 [Loss: 0.177263]\n",
      "1077 [Loss: 0.189587]\n",
      "1078 [Loss: 0.195815]\n",
      "1079 [Loss: 0.185948]\n",
      "1080 [Loss: 0.189653]\n",
      "1081 [Loss: 0.200919]\n",
      "1082 [Loss: 0.188622]\n",
      "1083 [Loss: 0.184009]\n",
      "1084 [Loss: 0.182019]\n",
      "1085 [Loss: 0.183953]\n",
      "1086 [Loss: 0.179104]\n",
      "1087 [Loss: 0.193643]\n",
      "1088 [Loss: 0.191385]\n",
      "1089 [Loss: 0.184552]\n",
      "1090 [Loss: 0.186743]\n",
      "1091 [Loss: 0.196749]\n",
      "1092 [Loss: 0.182703]\n",
      "1093 [Loss: 0.198047]\n",
      "1094 [Loss: 0.186924]\n",
      "1095 [Loss: 0.185358]\n",
      "1096 [Loss: 0.183036]\n",
      "1097 [Loss: 0.191185]\n",
      "1098 [Loss: 0.188992]\n",
      "1099 [Loss: 0.198548]\n",
      "1100 [Loss: 0.183773]\n",
      "1101 [Loss: 0.193798]\n",
      "1102 [Loss: 0.190566]\n",
      "1103 [Loss: 0.184116]\n",
      "1104 [Loss: 0.194940]\n",
      "1105 [Loss: 0.191941]\n",
      "1106 [Loss: 0.186177]\n",
      "1107 [Loss: 0.190445]\n",
      "1108 [Loss: 0.184595]\n",
      "1109 [Loss: 0.179776]\n",
      "1110 [Loss: 0.185452]\n",
      "1111 [Loss: 0.189129]\n",
      "1112 [Loss: 0.187519]\n",
      "1113 [Loss: 0.191849]\n",
      "1114 [Loss: 0.197757]\n",
      "1115 [Loss: 0.180690]\n",
      "1116 [Loss: 0.188289]\n",
      "1117 [Loss: 0.192312]\n",
      "1118 [Loss: 0.189047]\n",
      "1119 [Loss: 0.188243]\n",
      "1120 [Loss: 0.188501]\n",
      "1121 [Loss: 0.180790]\n",
      "1122 [Loss: 0.187678]\n",
      "1123 [Loss: 0.171856]\n",
      "1124 [Loss: 0.185247]\n",
      "1125 [Loss: 0.188044]\n",
      "1126 [Loss: 0.189361]\n",
      "1127 [Loss: 0.189850]\n",
      "1128 [Loss: 0.193376]\n",
      "1129 [Loss: 0.180031]\n",
      "1130 [Loss: 0.175847]\n",
      "1131 [Loss: 0.183346]\n",
      "1132 [Loss: 0.183515]\n",
      "1133 [Loss: 0.181819]\n",
      "1134 [Loss: 0.197278]\n",
      "1135 [Loss: 0.197012]\n",
      "1136 [Loss: 0.187940]\n",
      "1137 [Loss: 0.190830]\n",
      "1138 [Loss: 0.186405]\n",
      "1139 [Loss: 0.191830]\n",
      "1140 [Loss: 0.185840]\n",
      "1141 [Loss: 0.180211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1142 [Loss: 0.190507]\n",
      "1143 [Loss: 0.198774]\n",
      "1144 [Loss: 0.191259]\n",
      "1145 [Loss: 0.182987]\n",
      "1146 [Loss: 0.186208]\n",
      "1147 [Loss: 0.187160]\n",
      "1148 [Loss: 0.187064]\n",
      "1149 [Loss: 0.186478]\n",
      "1150 [Loss: 0.183162]\n",
      "1151 [Loss: 0.187112]\n",
      "1152 [Loss: 0.196832]\n",
      "1153 [Loss: 0.187546]\n",
      "1154 [Loss: 0.205714]\n",
      "1155 [Loss: 0.195671]\n",
      "1156 [Loss: 0.188842]\n",
      "1157 [Loss: 0.190812]\n",
      "1158 [Loss: 0.190826]\n",
      "1159 [Loss: 0.178123]\n",
      "1160 [Loss: 0.184159]\n",
      "1161 [Loss: 0.184091]\n",
      "1162 [Loss: 0.182061]\n",
      "1163 [Loss: 0.193760]\n",
      "1164 [Loss: 0.175304]\n",
      "1165 [Loss: 0.186718]\n",
      "1166 [Loss: 0.177877]\n",
      "1167 [Loss: 0.186193]\n",
      "1168 [Loss: 0.179755]\n",
      "1169 [Loss: 0.188196]\n",
      "1170 [Loss: 0.204107]\n",
      "1171 [Loss: 0.200603]\n",
      "1172 [Loss: 0.186172]\n",
      "1173 [Loss: 0.184912]\n",
      "1174 [Loss: 0.186733]\n",
      "1175 [Loss: 0.186827]\n",
      "1176 [Loss: 0.178468]\n",
      "1177 [Loss: 0.185811]\n",
      "1178 [Loss: 0.177016]\n",
      "1179 [Loss: 0.187631]\n",
      "1180 [Loss: 0.189208]\n",
      "1181 [Loss: 0.192000]\n",
      "1182 [Loss: 0.184902]\n",
      "1183 [Loss: 0.195412]\n",
      "1184 [Loss: 0.188760]\n",
      "1185 [Loss: 0.191819]\n",
      "1186 [Loss: 0.189892]\n",
      "1187 [Loss: 0.187304]\n",
      "1188 [Loss: 0.187627]\n",
      "1189 [Loss: 0.201588]\n",
      "1190 [Loss: 0.178165]\n",
      "1191 [Loss: 0.175397]\n",
      "1192 [Loss: 0.179545]\n",
      "1193 [Loss: 0.186554]\n",
      "1194 [Loss: 0.190353]\n",
      "1195 [Loss: 0.177024]\n",
      "1196 [Loss: 0.183060]\n",
      "1197 [Loss: 0.191350]\n",
      "1198 [Loss: 0.176626]\n",
      "1199 [Loss: 0.182579]\n",
      "1200 [Loss: 0.177558]\n",
      "1201 [Loss: 0.190861]\n",
      "1202 [Loss: 0.190480]\n",
      "1203 [Loss: 0.180541]\n",
      "1204 [Loss: 0.183456]\n",
      "1205 [Loss: 0.184050]\n",
      "1206 [Loss: 0.185057]\n",
      "1207 [Loss: 0.177093]\n",
      "1208 [Loss: 0.181700]\n",
      "1209 [Loss: 0.179322]\n",
      "1210 [Loss: 0.167018]\n",
      "1211 [Loss: 0.172757]\n",
      "1212 [Loss: 0.172273]\n",
      "1213 [Loss: 0.182703]\n",
      "1214 [Loss: 0.175118]\n",
      "1215 [Loss: 0.178824]\n",
      "1216 [Loss: 0.187498]\n",
      "1217 [Loss: 0.187829]\n",
      "1218 [Loss: 0.179484]\n",
      "1219 [Loss: 0.179204]\n",
      "1220 [Loss: 0.188963]\n",
      "1221 [Loss: 0.169874]\n",
      "1222 [Loss: 0.205103]\n",
      "1223 [Loss: 0.184664]\n",
      "1224 [Loss: 0.183195]\n",
      "1225 [Loss: 0.179090]\n",
      "1226 [Loss: 0.184768]\n",
      "1227 [Loss: 0.187732]\n",
      "1228 [Loss: 0.178805]\n",
      "1229 [Loss: 0.181556]\n",
      "1230 [Loss: 0.186847]\n",
      "1231 [Loss: 0.184108]\n",
      "1232 [Loss: 0.178354]\n",
      "1233 [Loss: 0.184670]\n",
      "1234 [Loss: 0.174396]\n",
      "1235 [Loss: 0.186632]\n",
      "1236 [Loss: 0.190479]\n",
      "1237 [Loss: 0.179629]\n",
      "1238 [Loss: 0.184263]\n",
      "1239 [Loss: 0.178931]\n",
      "1240 [Loss: 0.183696]\n",
      "1241 [Loss: 0.173424]\n",
      "1242 [Loss: 0.178455]\n",
      "1243 [Loss: 0.187681]\n",
      "1244 [Loss: 0.179988]\n",
      "1245 [Loss: 0.187838]\n",
      "1246 [Loss: 0.178015]\n",
      "1247 [Loss: 0.185810]\n",
      "1248 [Loss: 0.181829]\n",
      "1249 [Loss: 0.170790]\n",
      "1250 [Loss: 0.176641]\n",
      "1251 [Loss: 0.175184]\n",
      "1252 [Loss: 0.182374]\n",
      "1253 [Loss: 0.181510]\n",
      "1254 [Loss: 0.188736]\n",
      "1255 [Loss: 0.188257]\n",
      "1256 [Loss: 0.187722]\n",
      "1257 [Loss: 0.187643]\n",
      "1258 [Loss: 0.177066]\n",
      "1259 [Loss: 0.185107]\n",
      "1260 [Loss: 0.178389]\n",
      "1261 [Loss: 0.181363]\n",
      "1262 [Loss: 0.189314]\n",
      "1263 [Loss: 0.174045]\n",
      "1264 [Loss: 0.169632]\n",
      "1265 [Loss: 0.190165]\n",
      "1266 [Loss: 0.183764]\n",
      "1267 [Loss: 0.179564]\n",
      "1268 [Loss: 0.179865]\n",
      "1269 [Loss: 0.191982]\n",
      "1270 [Loss: 0.183656]\n",
      "1271 [Loss: 0.175963]\n",
      "1272 [Loss: 0.170590]\n",
      "1273 [Loss: 0.176854]\n",
      "1274 [Loss: 0.181436]\n",
      "1275 [Loss: 0.187460]\n",
      "1276 [Loss: 0.183626]\n",
      "1277 [Loss: 0.183699]\n",
      "1278 [Loss: 0.177614]\n",
      "1279 [Loss: 0.175497]\n",
      "1280 [Loss: 0.172019]\n",
      "1281 [Loss: 0.181248]\n",
      "1282 [Loss: 0.188237]\n",
      "1283 [Loss: 0.190544]\n",
      "1284 [Loss: 0.176042]\n",
      "1285 [Loss: 0.176809]\n",
      "1286 [Loss: 0.193035]\n",
      "1287 [Loss: 0.172190]\n",
      "1288 [Loss: 0.175297]\n",
      "1289 [Loss: 0.181018]\n",
      "1290 [Loss: 0.169936]\n",
      "1291 [Loss: 0.161699]\n",
      "1292 [Loss: 0.181813]\n",
      "1293 [Loss: 0.181410]\n",
      "1294 [Loss: 0.178175]\n",
      "1295 [Loss: 0.188529]\n",
      "1296 [Loss: 0.189362]\n",
      "1297 [Loss: 0.186460]\n",
      "1298 [Loss: 0.182587]\n",
      "1299 [Loss: 0.176747]\n",
      "1300 [Loss: 0.168652]\n",
      "1301 [Loss: 0.191317]\n",
      "1302 [Loss: 0.180169]\n",
      "1303 [Loss: 0.180091]\n",
      "1304 [Loss: 0.183705]\n",
      "1305 [Loss: 0.194285]\n",
      "1306 [Loss: 0.182500]\n",
      "1307 [Loss: 0.176922]\n",
      "1308 [Loss: 0.188084]\n",
      "1309 [Loss: 0.181605]\n",
      "1310 [Loss: 0.175030]\n",
      "1311 [Loss: 0.179622]\n",
      "1312 [Loss: 0.177307]\n",
      "1313 [Loss: 0.190626]\n",
      "1314 [Loss: 0.179379]\n",
      "1315 [Loss: 0.176151]\n",
      "1316 [Loss: 0.163883]\n",
      "1317 [Loss: 0.166816]\n",
      "1318 [Loss: 0.168707]\n",
      "1319 [Loss: 0.169409]\n",
      "1320 [Loss: 0.175458]\n",
      "1321 [Loss: 0.178336]\n",
      "1322 [Loss: 0.180435]\n",
      "1323 [Loss: 0.175332]\n",
      "1324 [Loss: 0.174340]\n",
      "1325 [Loss: 0.179453]\n",
      "1326 [Loss: 0.187182]\n",
      "1327 [Loss: 0.180651]\n",
      "1328 [Loss: 0.177034]\n",
      "1329 [Loss: 0.175916]\n",
      "1330 [Loss: 0.172537]\n",
      "1331 [Loss: 0.169599]\n",
      "1332 [Loss: 0.178174]\n",
      "1333 [Loss: 0.177133]\n",
      "1334 [Loss: 0.177585]\n",
      "1335 [Loss: 0.182807]\n",
      "1336 [Loss: 0.182902]\n",
      "1337 [Loss: 0.179690]\n",
      "1338 [Loss: 0.188400]\n",
      "1339 [Loss: 0.194196]\n",
      "1340 [Loss: 0.174282]\n",
      "1341 [Loss: 0.169515]\n",
      "1342 [Loss: 0.175137]\n",
      "1343 [Loss: 0.195071]\n",
      "1344 [Loss: 0.181005]\n",
      "1345 [Loss: 0.175645]\n",
      "1346 [Loss: 0.169983]\n",
      "1347 [Loss: 0.179103]\n",
      "1348 [Loss: 0.163453]\n",
      "1349 [Loss: 0.171905]\n",
      "1350 [Loss: 0.168072]\n",
      "1351 [Loss: 0.164050]\n",
      "1352 [Loss: 0.186763]\n",
      "1353 [Loss: 0.175874]\n",
      "1354 [Loss: 0.170426]\n",
      "1355 [Loss: 0.185014]\n",
      "1356 [Loss: 0.193094]\n",
      "1357 [Loss: 0.167166]\n",
      "1358 [Loss: 0.171891]\n",
      "1359 [Loss: 0.183047]\n",
      "1360 [Loss: 0.177021]\n",
      "1361 [Loss: 0.176448]\n",
      "1362 [Loss: 0.168989]\n",
      "1363 [Loss: 0.182476]\n",
      "1364 [Loss: 0.174577]\n",
      "1365 [Loss: 0.179320]\n",
      "1366 [Loss: 0.180856]\n",
      "1367 [Loss: 0.177826]\n",
      "1368 [Loss: 0.176199]\n",
      "1369 [Loss: 0.179182]\n",
      "1370 [Loss: 0.172553]\n",
      "1371 [Loss: 0.178435]\n",
      "1372 [Loss: 0.182691]\n",
      "1373 [Loss: 0.175924]\n",
      "1374 [Loss: 0.181926]\n",
      "1375 [Loss: 0.176794]\n",
      "1376 [Loss: 0.182423]\n",
      "1377 [Loss: 0.166189]\n",
      "1378 [Loss: 0.181971]\n",
      "1379 [Loss: 0.180735]\n",
      "1380 [Loss: 0.181186]\n",
      "1381 [Loss: 0.177042]\n",
      "1382 [Loss: 0.174056]\n",
      "1383 [Loss: 0.172730]\n",
      "1384 [Loss: 0.183849]\n",
      "1385 [Loss: 0.182325]\n",
      "1386 [Loss: 0.170989]\n",
      "1387 [Loss: 0.196365]\n",
      "1388 [Loss: 0.179942]\n",
      "1389 [Loss: 0.177807]\n",
      "1390 [Loss: 0.179828]\n",
      "1391 [Loss: 0.180394]\n",
      "1392 [Loss: 0.176131]\n",
      "1393 [Loss: 0.171690]\n",
      "1394 [Loss: 0.180580]\n",
      "1395 [Loss: 0.163749]\n",
      "1396 [Loss: 0.182595]\n",
      "1397 [Loss: 0.165451]\n",
      "1398 [Loss: 0.174758]\n",
      "1399 [Loss: 0.176788]\n",
      "1400 [Loss: 0.167165]\n",
      "1401 [Loss: 0.179940]\n",
      "1402 [Loss: 0.178547]\n",
      "1403 [Loss: 0.176461]\n",
      "1404 [Loss: 0.188399]\n",
      "1405 [Loss: 0.176021]\n",
      "1406 [Loss: 0.178511]\n",
      "1407 [Loss: 0.173585]\n",
      "1408 [Loss: 0.187013]\n",
      "1409 [Loss: 0.172364]\n",
      "1410 [Loss: 0.167105]\n",
      "1411 [Loss: 0.170502]\n",
      "1412 [Loss: 0.174620]\n",
      "1413 [Loss: 0.174870]\n",
      "1414 [Loss: 0.160258]\n",
      "1415 [Loss: 0.170434]\n",
      "1416 [Loss: 0.175497]\n",
      "1417 [Loss: 0.158822]\n",
      "1418 [Loss: 0.168864]\n",
      "1419 [Loss: 0.172208]\n",
      "1420 [Loss: 0.161020]\n",
      "1421 [Loss: 0.171514]\n",
      "1422 [Loss: 0.178053]\n",
      "1423 [Loss: 0.178120]\n",
      "1424 [Loss: 0.174443]\n",
      "1425 [Loss: 0.169591]\n",
      "1426 [Loss: 0.178490]\n",
      "1427 [Loss: 0.165238]\n",
      "1428 [Loss: 0.187291]\n",
      "1429 [Loss: 0.177624]\n",
      "1430 [Loss: 0.175495]\n",
      "1431 [Loss: 0.188217]\n",
      "1432 [Loss: 0.178996]\n",
      "1433 [Loss: 0.175606]\n",
      "1434 [Loss: 0.185912]\n",
      "1435 [Loss: 0.167590]\n",
      "1436 [Loss: 0.172786]\n",
      "1437 [Loss: 0.177520]\n",
      "1438 [Loss: 0.182609]\n",
      "1439 [Loss: 0.175534]\n",
      "1440 [Loss: 0.177245]\n",
      "1441 [Loss: 0.178613]\n",
      "1442 [Loss: 0.191142]\n",
      "1443 [Loss: 0.167283]\n",
      "1444 [Loss: 0.170282]\n",
      "1445 [Loss: 0.175411]\n",
      "1446 [Loss: 0.160696]\n",
      "1447 [Loss: 0.179148]\n",
      "1448 [Loss: 0.165373]\n",
      "1449 [Loss: 0.169529]\n",
      "1450 [Loss: 0.170560]\n",
      "1451 [Loss: 0.181754]\n",
      "1452 [Loss: 0.164015]\n",
      "1453 [Loss: 0.171224]\n",
      "1454 [Loss: 0.178331]\n",
      "1455 [Loss: 0.186254]\n",
      "1456 [Loss: 0.178683]\n",
      "1457 [Loss: 0.163960]\n",
      "1458 [Loss: 0.174500]\n",
      "1459 [Loss: 0.166645]\n",
      "1460 [Loss: 0.163204]\n",
      "1461 [Loss: 0.162511]\n",
      "1462 [Loss: 0.168237]\n",
      "1463 [Loss: 0.167791]\n",
      "1464 [Loss: 0.170715]\n",
      "1465 [Loss: 0.167107]\n",
      "1466 [Loss: 0.174136]\n",
      "1467 [Loss: 0.172446]\n",
      "1468 [Loss: 0.166934]\n",
      "1469 [Loss: 0.172216]\n",
      "1470 [Loss: 0.184640]\n",
      "1471 [Loss: 0.170513]\n",
      "1472 [Loss: 0.173847]\n",
      "1473 [Loss: 0.172229]\n",
      "1474 [Loss: 0.167513]\n",
      "1475 [Loss: 0.179475]\n",
      "1476 [Loss: 0.183982]\n",
      "1477 [Loss: 0.167449]\n",
      "1478 [Loss: 0.174478]\n",
      "1479 [Loss: 0.164824]\n",
      "1480 [Loss: 0.181834]\n",
      "1481 [Loss: 0.163926]\n",
      "1482 [Loss: 0.182141]\n",
      "1483 [Loss: 0.170132]\n",
      "1484 [Loss: 0.187251]\n",
      "1485 [Loss: 0.157186]\n",
      "1486 [Loss: 0.172696]\n",
      "1487 [Loss: 0.179741]\n",
      "1488 [Loss: 0.174636]\n",
      "1489 [Loss: 0.165556]\n",
      "1490 [Loss: 0.172161]\n",
      "1491 [Loss: 0.171981]\n",
      "1492 [Loss: 0.170752]\n",
      "1493 [Loss: 0.178442]\n",
      "1494 [Loss: 0.172615]\n",
      "1495 [Loss: 0.168706]\n",
      "1496 [Loss: 0.179392]\n",
      "1497 [Loss: 0.174830]\n",
      "1498 [Loss: 0.170897]\n",
      "1499 [Loss: 0.177729]\n",
      "1500 [Loss: 0.165367]\n",
      "1501 [Loss: 0.176526]\n",
      "1502 [Loss: 0.175585]\n",
      "1503 [Loss: 0.168592]\n",
      "1504 [Loss: 0.169550]\n",
      "1505 [Loss: 0.163392]\n",
      "1506 [Loss: 0.160581]\n",
      "1507 [Loss: 0.170165]\n",
      "1508 [Loss: 0.166663]\n",
      "1509 [Loss: 0.168167]\n",
      "1510 [Loss: 0.166684]\n",
      "1511 [Loss: 0.169997]\n",
      "1512 [Loss: 0.166652]\n",
      "1513 [Loss: 0.176860]\n",
      "1514 [Loss: 0.171531]\n",
      "1515 [Loss: 0.168103]\n",
      "1516 [Loss: 0.173213]\n",
      "1517 [Loss: 0.179177]\n",
      "1518 [Loss: 0.175690]\n",
      "1519 [Loss: 0.172726]\n",
      "1520 [Loss: 0.189287]\n",
      "1521 [Loss: 0.172671]\n",
      "1522 [Loss: 0.172116]\n",
      "1523 [Loss: 0.167289]\n",
      "1524 [Loss: 0.172509]\n",
      "1525 [Loss: 0.165471]\n",
      "1526 [Loss: 0.171986]\n",
      "1527 [Loss: 0.174613]\n",
      "1528 [Loss: 0.171186]\n",
      "1529 [Loss: 0.172783]\n",
      "1530 [Loss: 0.166600]\n",
      "1531 [Loss: 0.157128]\n",
      "1532 [Loss: 0.178021]\n",
      "1533 [Loss: 0.175355]\n",
      "1534 [Loss: 0.174869]\n",
      "1535 [Loss: 0.168896]\n",
      "1536 [Loss: 0.188721]\n",
      "1537 [Loss: 0.179911]\n",
      "1538 [Loss: 0.177863]\n",
      "1539 [Loss: 0.164976]\n",
      "1540 [Loss: 0.165955]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1541 [Loss: 0.168566]\n",
      "1542 [Loss: 0.169010]\n",
      "1543 [Loss: 0.167425]\n",
      "1544 [Loss: 0.180312]\n",
      "1545 [Loss: 0.174673]\n",
      "1546 [Loss: 0.172777]\n",
      "1547 [Loss: 0.180687]\n",
      "1548 [Loss: 0.173536]\n",
      "1549 [Loss: 0.163129]\n",
      "1550 [Loss: 0.160323]\n",
      "1551 [Loss: 0.159418]\n",
      "1552 [Loss: 0.167370]\n",
      "1553 [Loss: 0.173675]\n",
      "1554 [Loss: 0.183261]\n",
      "1555 [Loss: 0.170948]\n",
      "1556 [Loss: 0.178313]\n",
      "1557 [Loss: 0.167504]\n",
      "1558 [Loss: 0.179332]\n",
      "1559 [Loss: 0.159042]\n",
      "1560 [Loss: 0.173266]\n",
      "1561 [Loss: 0.176640]\n",
      "1562 [Loss: 0.162989]\n",
      "1563 [Loss: 0.165603]\n",
      "1564 [Loss: 0.172604]\n",
      "1565 [Loss: 0.162457]\n",
      "1566 [Loss: 0.175404]\n",
      "1567 [Loss: 0.173421]\n",
      "1568 [Loss: 0.164698]\n",
      "1569 [Loss: 0.171952]\n",
      "1570 [Loss: 0.170189]\n",
      "1571 [Loss: 0.171139]\n",
      "1572 [Loss: 0.179516]\n",
      "1573 [Loss: 0.174092]\n",
      "1574 [Loss: 0.165823]\n",
      "1575 [Loss: 0.172031]\n",
      "1576 [Loss: 0.172290]\n",
      "1577 [Loss: 0.163894]\n",
      "1578 [Loss: 0.166445]\n",
      "1579 [Loss: 0.165918]\n",
      "1580 [Loss: 0.186503]\n",
      "1581 [Loss: 0.160690]\n",
      "1582 [Loss: 0.165971]\n",
      "1583 [Loss: 0.162293]\n",
      "1584 [Loss: 0.163771]\n",
      "1585 [Loss: 0.171005]\n",
      "1586 [Loss: 0.173776]\n",
      "1587 [Loss: 0.182031]\n",
      "1588 [Loss: 0.156648]\n",
      "1589 [Loss: 0.173341]\n",
      "1590 [Loss: 0.150942]\n",
      "1591 [Loss: 0.163512]\n",
      "1592 [Loss: 0.167816]\n",
      "1593 [Loss: 0.164259]\n",
      "1594 [Loss: 0.168885]\n",
      "1595 [Loss: 0.168471]\n",
      "1596 [Loss: 0.178228]\n",
      "1597 [Loss: 0.166280]\n",
      "1598 [Loss: 0.178121]\n",
      "1599 [Loss: 0.173254]\n",
      "1600 [Loss: 0.166614]\n",
      "1601 [Loss: 0.164130]\n",
      "1602 [Loss: 0.163122]\n",
      "1603 [Loss: 0.160999]\n",
      "1604 [Loss: 0.177300]\n",
      "1605 [Loss: 0.177794]\n",
      "1606 [Loss: 0.175464]\n",
      "1607 [Loss: 0.170066]\n",
      "1608 [Loss: 0.188392]\n",
      "1609 [Loss: 0.162873]\n",
      "1610 [Loss: 0.168050]\n",
      "1611 [Loss: 0.168173]\n",
      "1612 [Loss: 0.161109]\n",
      "1613 [Loss: 0.171651]\n",
      "1614 [Loss: 0.167361]\n",
      "1615 [Loss: 0.183530]\n",
      "1616 [Loss: 0.160799]\n",
      "1617 [Loss: 0.171933]\n",
      "1618 [Loss: 0.163673]\n",
      "1619 [Loss: 0.177103]\n",
      "1620 [Loss: 0.174713]\n",
      "1621 [Loss: 0.165378]\n",
      "1622 [Loss: 0.174497]\n",
      "1623 [Loss: 0.165073]\n",
      "1624 [Loss: 0.169421]\n",
      "1625 [Loss: 0.169455]\n",
      "1626 [Loss: 0.167819]\n",
      "1627 [Loss: 0.170355]\n",
      "1628 [Loss: 0.171235]\n",
      "1629 [Loss: 0.167284]\n",
      "1630 [Loss: 0.166572]\n",
      "1631 [Loss: 0.176374]\n",
      "1632 [Loss: 0.166796]\n",
      "1633 [Loss: 0.166928]\n",
      "1634 [Loss: 0.176063]\n",
      "1635 [Loss: 0.169382]\n",
      "1636 [Loss: 0.163823]\n",
      "1637 [Loss: 0.175792]\n",
      "1638 [Loss: 0.172400]\n",
      "1639 [Loss: 0.163407]\n",
      "1640 [Loss: 0.161948]\n",
      "1641 [Loss: 0.172222]\n",
      "1642 [Loss: 0.170679]\n",
      "1643 [Loss: 0.176651]\n",
      "1644 [Loss: 0.160897]\n",
      "1645 [Loss: 0.153886]\n",
      "1646 [Loss: 0.169542]\n",
      "1647 [Loss: 0.166224]\n",
      "1648 [Loss: 0.174677]\n",
      "1649 [Loss: 0.175932]\n",
      "1650 [Loss: 0.161838]\n",
      "1651 [Loss: 0.171895]\n",
      "1652 [Loss: 0.166340]\n",
      "1653 [Loss: 0.161660]\n",
      "1654 [Loss: 0.166951]\n",
      "1655 [Loss: 0.174808]\n",
      "1656 [Loss: 0.161772]\n",
      "1657 [Loss: 0.169539]\n",
      "1658 [Loss: 0.170518]\n",
      "1659 [Loss: 0.163054]\n",
      "1660 [Loss: 0.174434]\n",
      "1661 [Loss: 0.175069]\n",
      "1662 [Loss: 0.171811]\n",
      "1663 [Loss: 0.171767]\n",
      "1664 [Loss: 0.177835]\n",
      "1665 [Loss: 0.182820]\n",
      "1666 [Loss: 0.168411]\n",
      "1667 [Loss: 0.172218]\n",
      "1668 [Loss: 0.162204]\n",
      "1669 [Loss: 0.162649]\n",
      "1670 [Loss: 0.168980]\n",
      "1671 [Loss: 0.165731]\n",
      "1672 [Loss: 0.164413]\n",
      "1673 [Loss: 0.163760]\n",
      "1674 [Loss: 0.174624]\n",
      "1675 [Loss: 0.169157]\n",
      "1676 [Loss: 0.161864]\n",
      "1677 [Loss: 0.158360]\n",
      "1678 [Loss: 0.164724]\n",
      "1679 [Loss: 0.175070]\n",
      "1680 [Loss: 0.177611]\n",
      "1681 [Loss: 0.163254]\n",
      "1682 [Loss: 0.168789]\n",
      "1683 [Loss: 0.157385]\n",
      "1684 [Loss: 0.170672]\n",
      "1685 [Loss: 0.175537]\n",
      "1686 [Loss: 0.174373]\n",
      "1687 [Loss: 0.162073]\n",
      "1688 [Loss: 0.163968]\n",
      "1689 [Loss: 0.169666]\n",
      "1690 [Loss: 0.166591]\n",
      "1691 [Loss: 0.164590]\n",
      "1692 [Loss: 0.174164]\n",
      "1693 [Loss: 0.160536]\n",
      "1694 [Loss: 0.167464]\n",
      "1695 [Loss: 0.171211]\n",
      "1696 [Loss: 0.166317]\n",
      "1697 [Loss: 0.162447]\n",
      "1698 [Loss: 0.163143]\n",
      "1699 [Loss: 0.172610]\n",
      "1700 [Loss: 0.162187]\n",
      "1701 [Loss: 0.160928]\n",
      "1702 [Loss: 0.169277]\n",
      "1703 [Loss: 0.176023]\n",
      "1704 [Loss: 0.157698]\n",
      "1705 [Loss: 0.171301]\n",
      "1706 [Loss: 0.168819]\n",
      "1707 [Loss: 0.167908]\n",
      "1708 [Loss: 0.163527]\n",
      "1709 [Loss: 0.168203]\n",
      "1710 [Loss: 0.169250]\n",
      "1711 [Loss: 0.165593]\n",
      "1712 [Loss: 0.163713]\n",
      "1713 [Loss: 0.181081]\n",
      "1714 [Loss: 0.172980]\n",
      "1715 [Loss: 0.167614]\n",
      "1716 [Loss: 0.162867]\n",
      "1717 [Loss: 0.160650]\n",
      "1718 [Loss: 0.170391]\n",
      "1719 [Loss: 0.172987]\n",
      "1720 [Loss: 0.172757]\n",
      "1721 [Loss: 0.184549]\n",
      "1722 [Loss: 0.161170]\n",
      "1723 [Loss: 0.167620]\n",
      "1724 [Loss: 0.172642]\n",
      "1725 [Loss: 0.167221]\n",
      "1726 [Loss: 0.178262]\n",
      "1727 [Loss: 0.168447]\n",
      "1728 [Loss: 0.166389]\n",
      "1729 [Loss: 0.168998]\n",
      "1730 [Loss: 0.150450]\n",
      "1731 [Loss: 0.163116]\n",
      "1732 [Loss: 0.179435]\n",
      "1733 [Loss: 0.159546]\n",
      "1734 [Loss: 0.161605]\n",
      "1735 [Loss: 0.170171]\n",
      "1736 [Loss: 0.166185]\n",
      "1737 [Loss: 0.165892]\n",
      "1738 [Loss: 0.161876]\n",
      "1739 [Loss: 0.169211]\n",
      "1740 [Loss: 0.165022]\n",
      "1741 [Loss: 0.165529]\n",
      "1742 [Loss: 0.163735]\n",
      "1743 [Loss: 0.162351]\n",
      "1744 [Loss: 0.160305]\n",
      "1745 [Loss: 0.170575]\n",
      "1746 [Loss: 0.174800]\n",
      "1747 [Loss: 0.173974]\n",
      "1748 [Loss: 0.153912]\n",
      "1749 [Loss: 0.171814]\n",
      "1750 [Loss: 0.164411]\n",
      "1751 [Loss: 0.161346]\n",
      "1752 [Loss: 0.162581]\n",
      "1753 [Loss: 0.165443]\n",
      "1754 [Loss: 0.169140]\n",
      "1755 [Loss: 0.169665]\n",
      "1756 [Loss: 0.165191]\n",
      "1757 [Loss: 0.153858]\n",
      "1758 [Loss: 0.165470]\n",
      "1759 [Loss: 0.159125]\n",
      "1760 [Loss: 0.153805]\n",
      "1761 [Loss: 0.172252]\n",
      "1762 [Loss: 0.168463]\n",
      "1763 [Loss: 0.158585]\n",
      "1764 [Loss: 0.166186]\n",
      "1765 [Loss: 0.164521]\n",
      "1766 [Loss: 0.153868]\n",
      "1767 [Loss: 0.163466]\n",
      "1768 [Loss: 0.170784]\n",
      "1769 [Loss: 0.160355]\n",
      "1770 [Loss: 0.159827]\n",
      "1771 [Loss: 0.175092]\n",
      "1772 [Loss: 0.158591]\n",
      "1773 [Loss: 0.159008]\n",
      "1774 [Loss: 0.159135]\n",
      "1775 [Loss: 0.164503]\n",
      "1776 [Loss: 0.165535]\n",
      "1777 [Loss: 0.168708]\n",
      "1778 [Loss: 0.168600]\n",
      "1779 [Loss: 0.160674]\n",
      "1780 [Loss: 0.172606]\n",
      "1781 [Loss: 0.165339]\n",
      "1782 [Loss: 0.166485]\n",
      "1783 [Loss: 0.160391]\n",
      "1784 [Loss: 0.167866]\n",
      "1785 [Loss: 0.171927]\n",
      "1786 [Loss: 0.156392]\n",
      "1787 [Loss: 0.156808]\n",
      "1788 [Loss: 0.162316]\n",
      "1789 [Loss: 0.171622]\n",
      "1790 [Loss: 0.166867]\n",
      "1791 [Loss: 0.162453]\n",
      "1792 [Loss: 0.168303]\n",
      "1793 [Loss: 0.161583]\n",
      "1794 [Loss: 0.170447]\n",
      "1795 [Loss: 0.179755]\n",
      "1796 [Loss: 0.169387]\n",
      "1797 [Loss: 0.157739]\n",
      "1798 [Loss: 0.163830]\n",
      "1799 [Loss: 0.156508]\n",
      "1800 [Loss: 0.175944]\n",
      "1801 [Loss: 0.170134]\n",
      "1802 [Loss: 0.164282]\n",
      "1803 [Loss: 0.168863]\n",
      "1804 [Loss: 0.166565]\n",
      "1805 [Loss: 0.170181]\n",
      "1806 [Loss: 0.168408]\n",
      "1807 [Loss: 0.172101]\n",
      "1808 [Loss: 0.164598]\n",
      "1809 [Loss: 0.163977]\n",
      "1810 [Loss: 0.157697]\n",
      "1811 [Loss: 0.167056]\n",
      "1812 [Loss: 0.163069]\n",
      "1813 [Loss: 0.162237]\n",
      "1814 [Loss: 0.165855]\n",
      "1815 [Loss: 0.169612]\n",
      "1816 [Loss: 0.169179]\n",
      "1817 [Loss: 0.161460]\n",
      "1818 [Loss: 0.170319]\n",
      "1819 [Loss: 0.161050]\n",
      "1820 [Loss: 0.167290]\n",
      "1821 [Loss: 0.168588]\n",
      "1822 [Loss: 0.164015]\n",
      "1823 [Loss: 0.158836]\n",
      "1824 [Loss: 0.157493]\n",
      "1825 [Loss: 0.155805]\n",
      "1826 [Loss: 0.159953]\n",
      "1827 [Loss: 0.169820]\n",
      "1828 [Loss: 0.163873]\n",
      "1829 [Loss: 0.154768]\n",
      "1830 [Loss: 0.166507]\n",
      "1831 [Loss: 0.159022]\n",
      "1832 [Loss: 0.158238]\n",
      "1833 [Loss: 0.161922]\n",
      "1834 [Loss: 0.155837]\n",
      "1835 [Loss: 0.158819]\n",
      "1836 [Loss: 0.159054]\n",
      "1837 [Loss: 0.181274]\n",
      "1838 [Loss: 0.170941]\n",
      "1839 [Loss: 0.174828]\n",
      "1840 [Loss: 0.163323]\n",
      "1841 [Loss: 0.160244]\n",
      "1842 [Loss: 0.170087]\n",
      "1843 [Loss: 0.167032]\n",
      "1844 [Loss: 0.159764]\n",
      "1845 [Loss: 0.163056]\n",
      "1846 [Loss: 0.169537]\n",
      "1847 [Loss: 0.154174]\n",
      "1848 [Loss: 0.172887]\n",
      "1849 [Loss: 0.165493]\n",
      "1850 [Loss: 0.162864]\n",
      "1851 [Loss: 0.165921]\n",
      "1852 [Loss: 0.166688]\n",
      "1853 [Loss: 0.155232]\n",
      "1854 [Loss: 0.166211]\n",
      "1855 [Loss: 0.156837]\n",
      "1856 [Loss: 0.168243]\n",
      "1857 [Loss: 0.163511]\n",
      "1858 [Loss: 0.158728]\n",
      "1859 [Loss: 0.153055]\n",
      "1860 [Loss: 0.164200]\n",
      "1861 [Loss: 0.161601]\n",
      "1862 [Loss: 0.171256]\n",
      "1863 [Loss: 0.172771]\n",
      "1864 [Loss: 0.173962]\n",
      "1865 [Loss: 0.167582]\n",
      "1866 [Loss: 0.160957]\n",
      "1867 [Loss: 0.164061]\n",
      "1868 [Loss: 0.158146]\n",
      "1869 [Loss: 0.169925]\n",
      "1870 [Loss: 0.164789]\n",
      "1871 [Loss: 0.149588]\n",
      "1872 [Loss: 0.166926]\n",
      "1873 [Loss: 0.162064]\n",
      "1874 [Loss: 0.164960]\n",
      "1875 [Loss: 0.155352]\n",
      "1876 [Loss: 0.162329]\n",
      "1877 [Loss: 0.170013]\n",
      "1878 [Loss: 0.169549]\n",
      "1879 [Loss: 0.166141]\n",
      "1880 [Loss: 0.160183]\n",
      "1881 [Loss: 0.170864]\n",
      "1882 [Loss: 0.165523]\n",
      "1883 [Loss: 0.164091]\n",
      "1884 [Loss: 0.170683]\n",
      "1885 [Loss: 0.160151]\n",
      "1886 [Loss: 0.171740]\n",
      "1887 [Loss: 0.163563]\n",
      "1888 [Loss: 0.167026]\n",
      "1889 [Loss: 0.158430]\n",
      "1890 [Loss: 0.157777]\n",
      "1891 [Loss: 0.166884]\n",
      "1892 [Loss: 0.167042]\n",
      "1893 [Loss: 0.157521]\n",
      "1894 [Loss: 0.162681]\n",
      "1895 [Loss: 0.160261]\n",
      "1896 [Loss: 0.155076]\n",
      "1897 [Loss: 0.163445]\n",
      "1898 [Loss: 0.162315]\n",
      "1899 [Loss: 0.159168]\n",
      "1900 [Loss: 0.159532]\n",
      "1901 [Loss: 0.177092]\n",
      "1902 [Loss: 0.161645]\n",
      "1903 [Loss: 0.152400]\n",
      "1904 [Loss: 0.162346]\n",
      "1905 [Loss: 0.163279]\n",
      "1906 [Loss: 0.154779]\n",
      "1907 [Loss: 0.163424]\n",
      "1908 [Loss: 0.168651]\n",
      "1909 [Loss: 0.170398]\n",
      "1910 [Loss: 0.158636]\n",
      "1911 [Loss: 0.161897]\n",
      "1912 [Loss: 0.150946]\n",
      "1913 [Loss: 0.163256]\n",
      "1914 [Loss: 0.157007]\n",
      "1915 [Loss: 0.165396]\n",
      "1916 [Loss: 0.162764]\n",
      "1917 [Loss: 0.170438]\n",
      "1918 [Loss: 0.160905]\n",
      "1919 [Loss: 0.158092]\n",
      "1920 [Loss: 0.163378]\n",
      "1921 [Loss: 0.158653]\n",
      "1922 [Loss: 0.157764]\n",
      "1923 [Loss: 0.166670]\n",
      "1924 [Loss: 0.154035]\n",
      "1925 [Loss: 0.156673]\n",
      "1926 [Loss: 0.156400]\n",
      "1927 [Loss: 0.159584]\n",
      "1928 [Loss: 0.158718]\n",
      "1929 [Loss: 0.169503]\n",
      "1930 [Loss: 0.164513]\n",
      "1931 [Loss: 0.165733]\n",
      "1932 [Loss: 0.172958]\n",
      "1933 [Loss: 0.157048]\n",
      "1934 [Loss: 0.167704]\n",
      "1935 [Loss: 0.152478]\n",
      "1936 [Loss: 0.166432]\n",
      "1937 [Loss: 0.164552]\n",
      "1938 [Loss: 0.167955]\n",
      "1939 [Loss: 0.162734]\n",
      "1940 [Loss: 0.162284]\n",
      "1941 [Loss: 0.163385]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1942 [Loss: 0.167729]\n",
      "1943 [Loss: 0.165381]\n",
      "1944 [Loss: 0.149132]\n",
      "1945 [Loss: 0.164848]\n",
      "1946 [Loss: 0.163927]\n",
      "1947 [Loss: 0.148033]\n",
      "1948 [Loss: 0.152400]\n",
      "1949 [Loss: 0.160729]\n",
      "1950 [Loss: 0.154588]\n",
      "1951 [Loss: 0.158926]\n",
      "1952 [Loss: 0.157919]\n",
      "1953 [Loss: 0.160599]\n",
      "1954 [Loss: 0.154189]\n",
      "1955 [Loss: 0.159585]\n",
      "1956 [Loss: 0.165421]\n",
      "1957 [Loss: 0.158617]\n",
      "1958 [Loss: 0.156360]\n",
      "1959 [Loss: 0.154087]\n",
      "1960 [Loss: 0.161693]\n",
      "1961 [Loss: 0.159524]\n",
      "1962 [Loss: 0.155744]\n",
      "1963 [Loss: 0.146629]\n",
      "1964 [Loss: 0.163419]\n",
      "1965 [Loss: 0.156670]\n",
      "1966 [Loss: 0.163501]\n",
      "1967 [Loss: 0.164984]\n",
      "1968 [Loss: 0.164749]\n",
      "1969 [Loss: 0.160131]\n",
      "1970 [Loss: 0.162505]\n",
      "1971 [Loss: 0.157121]\n",
      "1972 [Loss: 0.162267]\n",
      "1973 [Loss: 0.152106]\n",
      "1974 [Loss: 0.153094]\n",
      "1975 [Loss: 0.154133]\n",
      "1976 [Loss: 0.155732]\n",
      "1977 [Loss: 0.159777]\n",
      "1978 [Loss: 0.151239]\n",
      "1979 [Loss: 0.165308]\n",
      "1980 [Loss: 0.165150]\n",
      "1981 [Loss: 0.171197]\n",
      "1982 [Loss: 0.149411]\n",
      "1983 [Loss: 0.161772]\n",
      "1984 [Loss: 0.155327]\n",
      "1985 [Loss: 0.167657]\n",
      "1986 [Loss: 0.161760]\n",
      "1987 [Loss: 0.165184]\n",
      "1988 [Loss: 0.152435]\n",
      "1989 [Loss: 0.157320]\n",
      "1990 [Loss: 0.157529]\n",
      "1991 [Loss: 0.165540]\n",
      "1992 [Loss: 0.167970]\n",
      "1993 [Loss: 0.154526]\n",
      "1994 [Loss: 0.161782]\n",
      "1995 [Loss: 0.152313]\n",
      "1996 [Loss: 0.151893]\n",
      "1997 [Loss: 0.164008]\n",
      "1998 [Loss: 0.162570]\n",
      "1999 [Loss: 0.154844]\n",
      "2000 [Loss: 0.165936]\n",
      "2001 [Loss: 0.159625]\n",
      "2002 [Loss: 0.166333]\n",
      "2003 [Loss: 0.156324]\n",
      "2004 [Loss: 0.158532]\n",
      "2005 [Loss: 0.164623]\n",
      "2006 [Loss: 0.159474]\n",
      "2007 [Loss: 0.165405]\n",
      "2008 [Loss: 0.162549]\n",
      "2009 [Loss: 0.156794]\n",
      "2010 [Loss: 0.162813]\n",
      "2011 [Loss: 0.147371]\n",
      "2012 [Loss: 0.156677]\n",
      "2013 [Loss: 0.159941]\n",
      "2014 [Loss: 0.161554]\n",
      "2015 [Loss: 0.157864]\n",
      "2016 [Loss: 0.164279]\n",
      "2017 [Loss: 0.174141]\n",
      "2018 [Loss: 0.145968]\n",
      "2019 [Loss: 0.157963]\n",
      "2020 [Loss: 0.158600]\n",
      "2021 [Loss: 0.162539]\n",
      "2022 [Loss: 0.170863]\n",
      "2023 [Loss: 0.165850]\n",
      "2024 [Loss: 0.152135]\n",
      "2025 [Loss: 0.162424]\n",
      "2026 [Loss: 0.156781]\n",
      "2027 [Loss: 0.156616]\n",
      "2028 [Loss: 0.154721]\n",
      "2029 [Loss: 0.162560]\n",
      "2030 [Loss: 0.164042]\n",
      "2031 [Loss: 0.147841]\n",
      "2032 [Loss: 0.156147]\n",
      "2033 [Loss: 0.167417]\n",
      "2034 [Loss: 0.164899]\n",
      "2035 [Loss: 0.152342]\n",
      "2036 [Loss: 0.167046]\n",
      "2037 [Loss: 0.151167]\n",
      "2038 [Loss: 0.163339]\n",
      "2039 [Loss: 0.154404]\n",
      "2040 [Loss: 0.157245]\n",
      "2041 [Loss: 0.169581]\n",
      "2042 [Loss: 0.169950]\n",
      "2043 [Loss: 0.153784]\n",
      "2044 [Loss: 0.151057]\n",
      "2045 [Loss: 0.158351]\n",
      "2046 [Loss: 0.174341]\n",
      "2047 [Loss: 0.168995]\n",
      "2048 [Loss: 0.159648]\n",
      "2049 [Loss: 0.158566]\n",
      "2050 [Loss: 0.163484]\n",
      "2051 [Loss: 0.158794]\n",
      "2052 [Loss: 0.159057]\n",
      "2053 [Loss: 0.157937]\n",
      "2054 [Loss: 0.152610]\n",
      "2055 [Loss: 0.160049]\n",
      "2056 [Loss: 0.155199]\n",
      "2057 [Loss: 0.148314]\n",
      "2058 [Loss: 0.155098]\n",
      "2059 [Loss: 0.161731]\n",
      "2060 [Loss: 0.161803]\n",
      "2061 [Loss: 0.152824]\n",
      "2062 [Loss: 0.162564]\n",
      "2063 [Loss: 0.154357]\n",
      "2064 [Loss: 0.154976]\n",
      "2065 [Loss: 0.153028]\n",
      "2066 [Loss: 0.164641]\n",
      "2067 [Loss: 0.158492]\n",
      "2068 [Loss: 0.156983]\n",
      "2069 [Loss: 0.159210]\n",
      "2070 [Loss: 0.155773]\n",
      "2071 [Loss: 0.152914]\n",
      "2072 [Loss: 0.150782]\n",
      "2073 [Loss: 0.161105]\n",
      "2074 [Loss: 0.152199]\n",
      "2075 [Loss: 0.166793]\n",
      "2076 [Loss: 0.153496]\n",
      "2077 [Loss: 0.156313]\n",
      "2078 [Loss: 0.160608]\n",
      "2079 [Loss: 0.160146]\n",
      "2080 [Loss: 0.153096]\n",
      "2081 [Loss: 0.154048]\n",
      "2082 [Loss: 0.151520]\n",
      "2083 [Loss: 0.148333]\n",
      "2084 [Loss: 0.149493]\n",
      "2085 [Loss: 0.156340]\n",
      "2086 [Loss: 0.149519]\n",
      "2087 [Loss: 0.165499]\n",
      "2088 [Loss: 0.144771]\n",
      "2089 [Loss: 0.160495]\n",
      "2090 [Loss: 0.162522]\n",
      "2091 [Loss: 0.164998]\n",
      "2092 [Loss: 0.163438]\n",
      "2093 [Loss: 0.172861]\n",
      "2094 [Loss: 0.163452]\n",
      "2095 [Loss: 0.157309]\n",
      "2096 [Loss: 0.152776]\n",
      "2097 [Loss: 0.153260]\n",
      "2098 [Loss: 0.148789]\n",
      "2099 [Loss: 0.158072]\n",
      "2100 [Loss: 0.154667]\n",
      "2101 [Loss: 0.162259]\n",
      "2102 [Loss: 0.148554]\n",
      "2103 [Loss: 0.160700]\n",
      "2104 [Loss: 0.149763]\n",
      "2105 [Loss: 0.145686]\n",
      "2106 [Loss: 0.158608]\n",
      "2107 [Loss: 0.162804]\n",
      "2108 [Loss: 0.156354]\n",
      "2109 [Loss: 0.157544]\n",
      "2110 [Loss: 0.158283]\n",
      "2111 [Loss: 0.162713]\n",
      "2112 [Loss: 0.156353]\n",
      "2113 [Loss: 0.148660]\n",
      "2114 [Loss: 0.156921]\n",
      "2115 [Loss: 0.158269]\n",
      "2116 [Loss: 0.164873]\n",
      "2117 [Loss: 0.143289]\n",
      "2118 [Loss: 0.147410]\n",
      "2119 [Loss: 0.161510]\n",
      "2120 [Loss: 0.154711]\n",
      "2121 [Loss: 0.145179]\n",
      "2122 [Loss: 0.165355]\n",
      "2123 [Loss: 0.166104]\n",
      "2124 [Loss: 0.162811]\n",
      "2125 [Loss: 0.158377]\n",
      "2126 [Loss: 0.157403]\n",
      "2127 [Loss: 0.151964]\n",
      "2128 [Loss: 0.159888]\n",
      "2129 [Loss: 0.159550]\n",
      "2130 [Loss: 0.149346]\n",
      "2131 [Loss: 0.155706]\n",
      "2132 [Loss: 0.160288]\n",
      "2133 [Loss: 0.161241]\n",
      "2134 [Loss: 0.156809]\n",
      "2135 [Loss: 0.161939]\n",
      "2136 [Loss: 0.155216]\n",
      "2137 [Loss: 0.165951]\n",
      "2138 [Loss: 0.155808]\n",
      "2139 [Loss: 0.153044]\n",
      "2140 [Loss: 0.157980]\n",
      "2141 [Loss: 0.156752]\n",
      "2142 [Loss: 0.144802]\n",
      "2143 [Loss: 0.163132]\n",
      "2144 [Loss: 0.159109]\n",
      "2145 [Loss: 0.153722]\n",
      "2146 [Loss: 0.158616]\n",
      "2147 [Loss: 0.154307]\n",
      "2148 [Loss: 0.155705]\n",
      "2149 [Loss: 0.142571]\n",
      "2150 [Loss: 0.149680]\n",
      "2151 [Loss: 0.161726]\n",
      "2152 [Loss: 0.145704]\n",
      "2153 [Loss: 0.147580]\n",
      "2154 [Loss: 0.160498]\n",
      "2155 [Loss: 0.145321]\n",
      "2156 [Loss: 0.162346]\n",
      "2157 [Loss: 0.157826]\n",
      "2158 [Loss: 0.157411]\n",
      "2159 [Loss: 0.154163]\n",
      "2160 [Loss: 0.159563]\n",
      "2161 [Loss: 0.167525]\n",
      "2162 [Loss: 0.157652]\n",
      "2163 [Loss: 0.163274]\n",
      "2164 [Loss: 0.151682]\n",
      "2165 [Loss: 0.158591]\n",
      "2166 [Loss: 0.145105]\n",
      "2167 [Loss: 0.161537]\n",
      "2168 [Loss: 0.162937]\n",
      "2169 [Loss: 0.155470]\n",
      "2170 [Loss: 0.171428]\n",
      "2171 [Loss: 0.148801]\n",
      "2172 [Loss: 0.158036]\n",
      "2173 [Loss: 0.150538]\n",
      "2174 [Loss: 0.164680]\n",
      "2175 [Loss: 0.160675]\n",
      "2176 [Loss: 0.160096]\n",
      "2177 [Loss: 0.150465]\n",
      "2178 [Loss: 0.160578]\n",
      "2179 [Loss: 0.152285]\n",
      "2180 [Loss: 0.157582]\n",
      "2181 [Loss: 0.162227]\n",
      "2182 [Loss: 0.153307]\n",
      "2183 [Loss: 0.148842]\n",
      "2184 [Loss: 0.155628]\n",
      "2185 [Loss: 0.164946]\n",
      "2186 [Loss: 0.153231]\n",
      "2187 [Loss: 0.164558]\n",
      "2188 [Loss: 0.154993]\n",
      "2189 [Loss: 0.158432]\n",
      "2190 [Loss: 0.144408]\n",
      "2191 [Loss: 0.136949]\n",
      "2192 [Loss: 0.160325]\n",
      "2193 [Loss: 0.166697]\n",
      "2194 [Loss: 0.163388]\n",
      "2195 [Loss: 0.155698]\n",
      "2196 [Loss: 0.158992]\n",
      "2197 [Loss: 0.167063]\n",
      "2198 [Loss: 0.143874]\n",
      "2199 [Loss: 0.152473]\n",
      "2200 [Loss: 0.148829]\n",
      "2201 [Loss: 0.156638]\n",
      "2202 [Loss: 0.155396]\n",
      "2203 [Loss: 0.151709]\n",
      "2204 [Loss: 0.155213]\n",
      "2205 [Loss: 0.155056]\n",
      "2206 [Loss: 0.146456]\n",
      "2207 [Loss: 0.162066]\n",
      "2208 [Loss: 0.149184]\n",
      "2209 [Loss: 0.145765]\n",
      "2210 [Loss: 0.156075]\n",
      "2211 [Loss: 0.155228]\n",
      "2212 [Loss: 0.161323]\n",
      "2213 [Loss: 0.152301]\n",
      "2214 [Loss: 0.156784]\n",
      "2215 [Loss: 0.152483]\n",
      "2216 [Loss: 0.141323]\n",
      "2217 [Loss: 0.157625]\n",
      "2218 [Loss: 0.148510]\n",
      "2219 [Loss: 0.162110]\n",
      "2220 [Loss: 0.159807]\n",
      "2221 [Loss: 0.150021]\n",
      "2222 [Loss: 0.169349]\n",
      "2223 [Loss: 0.150450]\n",
      "2224 [Loss: 0.149711]\n",
      "2225 [Loss: 0.153228]\n",
      "2226 [Loss: 0.152536]\n",
      "2227 [Loss: 0.152849]\n",
      "2228 [Loss: 0.149393]\n",
      "2229 [Loss: 0.158502]\n",
      "2230 [Loss: 0.159526]\n",
      "2231 [Loss: 0.159571]\n",
      "2232 [Loss: 0.153640]\n",
      "2233 [Loss: 0.157146]\n",
      "2234 [Loss: 0.146724]\n",
      "2235 [Loss: 0.169798]\n",
      "2236 [Loss: 0.150268]\n",
      "2237 [Loss: 0.152384]\n",
      "2238 [Loss: 0.152540]\n",
      "2239 [Loss: 0.157294]\n",
      "2240 [Loss: 0.150167]\n",
      "2241 [Loss: 0.149200]\n",
      "2242 [Loss: 0.149536]\n",
      "2243 [Loss: 0.152625]\n",
      "2244 [Loss: 0.160300]\n",
      "2245 [Loss: 0.153954]\n",
      "2246 [Loss: 0.155966]\n",
      "2247 [Loss: 0.146965]\n",
      "2248 [Loss: 0.156028]\n",
      "2249 [Loss: 0.153289]\n",
      "2250 [Loss: 0.151886]\n",
      "2251 [Loss: 0.148415]\n",
      "2252 [Loss: 0.164768]\n",
      "2253 [Loss: 0.147814]\n",
      "2254 [Loss: 0.149161]\n",
      "2255 [Loss: 0.163057]\n",
      "2256 [Loss: 0.146202]\n",
      "2257 [Loss: 0.153730]\n",
      "2258 [Loss: 0.160150]\n",
      "2259 [Loss: 0.154548]\n",
      "2260 [Loss: 0.142435]\n",
      "2261 [Loss: 0.151975]\n",
      "2262 [Loss: 0.152842]\n",
      "2263 [Loss: 0.150035]\n",
      "2264 [Loss: 0.159476]\n",
      "2265 [Loss: 0.167304]\n",
      "2266 [Loss: 0.159490]\n",
      "2267 [Loss: 0.153204]\n",
      "2268 [Loss: 0.158167]\n",
      "2269 [Loss: 0.159770]\n",
      "2270 [Loss: 0.162829]\n",
      "2271 [Loss: 0.146274]\n",
      "2272 [Loss: 0.156509]\n",
      "2273 [Loss: 0.155499]\n",
      "2274 [Loss: 0.151771]\n",
      "2275 [Loss: 0.153557]\n",
      "2276 [Loss: 0.151493]\n",
      "2277 [Loss: 0.148006]\n",
      "2278 [Loss: 0.146599]\n",
      "2279 [Loss: 0.154220]\n",
      "2280 [Loss: 0.143480]\n",
      "2281 [Loss: 0.159148]\n",
      "2282 [Loss: 0.147075]\n",
      "2283 [Loss: 0.155814]\n",
      "2284 [Loss: 0.158428]\n",
      "2285 [Loss: 0.151754]\n",
      "2286 [Loss: 0.146297]\n",
      "2287 [Loss: 0.153437]\n",
      "2288 [Loss: 0.144465]\n",
      "2289 [Loss: 0.156014]\n",
      "2290 [Loss: 0.154974]\n",
      "2291 [Loss: 0.150011]\n",
      "2292 [Loss: 0.145978]\n",
      "2293 [Loss: 0.155126]\n",
      "2294 [Loss: 0.139457]\n",
      "2295 [Loss: 0.153217]\n",
      "2296 [Loss: 0.147027]\n",
      "2297 [Loss: 0.153873]\n",
      "2298 [Loss: 0.144300]\n",
      "2299 [Loss: 0.155366]\n",
      "2300 [Loss: 0.154593]\n",
      "2301 [Loss: 0.156182]\n",
      "2302 [Loss: 0.147454]\n",
      "2303 [Loss: 0.149283]\n",
      "2304 [Loss: 0.153814]\n",
      "2305 [Loss: 0.147153]\n",
      "2306 [Loss: 0.162403]\n",
      "2307 [Loss: 0.155307]\n",
      "2308 [Loss: 0.150425]\n",
      "2309 [Loss: 0.146629]\n",
      "2310 [Loss: 0.143518]\n",
      "2311 [Loss: 0.147471]\n",
      "2312 [Loss: 0.160185]\n",
      "2313 [Loss: 0.149102]\n",
      "2314 [Loss: 0.156934]\n",
      "2315 [Loss: 0.154575]\n",
      "2316 [Loss: 0.151425]\n",
      "2317 [Loss: 0.159948]\n",
      "2318 [Loss: 0.154408]\n",
      "2319 [Loss: 0.158302]\n",
      "2320 [Loss: 0.159082]\n",
      "2321 [Loss: 0.158269]\n",
      "2322 [Loss: 0.159439]\n",
      "2323 [Loss: 0.143678]\n",
      "2324 [Loss: 0.142234]\n",
      "2325 [Loss: 0.149888]\n",
      "2326 [Loss: 0.151354]\n",
      "2327 [Loss: 0.143832]\n",
      "2328 [Loss: 0.136288]\n",
      "2329 [Loss: 0.151854]\n",
      "2330 [Loss: 0.146989]\n",
      "2331 [Loss: 0.155203]\n",
      "2332 [Loss: 0.155274]\n",
      "2333 [Loss: 0.147011]\n",
      "2334 [Loss: 0.153820]\n",
      "2335 [Loss: 0.141447]\n",
      "2336 [Loss: 0.154130]\n",
      "2337 [Loss: 0.153287]\n",
      "2338 [Loss: 0.140328]\n",
      "2339 [Loss: 0.157236]\n",
      "2340 [Loss: 0.143152]\n",
      "2341 [Loss: 0.159643]\n",
      "2342 [Loss: 0.155360]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2343 [Loss: 0.159868]\n",
      "2344 [Loss: 0.153638]\n",
      "2345 [Loss: 0.153967]\n",
      "2346 [Loss: 0.149871]\n",
      "2347 [Loss: 0.153198]\n",
      "2348 [Loss: 0.154362]\n",
      "2349 [Loss: 0.150939]\n",
      "2350 [Loss: 0.149409]\n",
      "2351 [Loss: 0.155136]\n",
      "2352 [Loss: 0.157148]\n",
      "2353 [Loss: 0.150869]\n",
      "2354 [Loss: 0.157111]\n",
      "2355 [Loss: 0.149421]\n",
      "2356 [Loss: 0.149009]\n",
      "2357 [Loss: 0.155085]\n",
      "2358 [Loss: 0.157025]\n",
      "2359 [Loss: 0.158245]\n",
      "2360 [Loss: 0.148835]\n",
      "2361 [Loss: 0.158596]\n",
      "2362 [Loss: 0.138181]\n",
      "2363 [Loss: 0.145623]\n",
      "2364 [Loss: 0.154436]\n",
      "2365 [Loss: 0.147188]\n",
      "2366 [Loss: 0.148554]\n",
      "2367 [Loss: 0.139479]\n",
      "2368 [Loss: 0.153590]\n",
      "2369 [Loss: 0.152694]\n",
      "2370 [Loss: 0.148356]\n",
      "2371 [Loss: 0.143663]\n",
      "2372 [Loss: 0.151785]\n",
      "2373 [Loss: 0.144735]\n",
      "2374 [Loss: 0.143827]\n",
      "2375 [Loss: 0.150814]\n",
      "2376 [Loss: 0.149479]\n",
      "2377 [Loss: 0.144856]\n",
      "2378 [Loss: 0.153599]\n",
      "2379 [Loss: 0.141987]\n",
      "2380 [Loss: 0.154035]\n",
      "2381 [Loss: 0.148499]\n",
      "2382 [Loss: 0.160274]\n",
      "2383 [Loss: 0.147784]\n",
      "2384 [Loss: 0.151472]\n",
      "2385 [Loss: 0.152283]\n",
      "2386 [Loss: 0.143638]\n",
      "2387 [Loss: 0.154530]\n",
      "2388 [Loss: 0.148880]\n",
      "2389 [Loss: 0.145166]\n",
      "2390 [Loss: 0.141001]\n",
      "2391 [Loss: 0.147461]\n",
      "2392 [Loss: 0.161500]\n",
      "2393 [Loss: 0.144345]\n",
      "2394 [Loss: 0.137114]\n",
      "2395 [Loss: 0.148633]\n",
      "2396 [Loss: 0.146179]\n",
      "2397 [Loss: 0.150341]\n",
      "2398 [Loss: 0.147492]\n",
      "2399 [Loss: 0.147745]\n",
      "2400 [Loss: 0.153606]\n",
      "2401 [Loss: 0.149054]\n",
      "2402 [Loss: 0.155074]\n",
      "2403 [Loss: 0.164535]\n",
      "2404 [Loss: 0.157229]\n",
      "2405 [Loss: 0.146655]\n",
      "2406 [Loss: 0.158392]\n",
      "2407 [Loss: 0.148606]\n",
      "2408 [Loss: 0.152076]\n",
      "2409 [Loss: 0.148556]\n",
      "2410 [Loss: 0.153228]\n",
      "2411 [Loss: 0.151916]\n",
      "2412 [Loss: 0.161970]\n",
      "2413 [Loss: 0.146417]\n",
      "2414 [Loss: 0.152833]\n",
      "2415 [Loss: 0.151359]\n",
      "2416 [Loss: 0.148484]\n",
      "2417 [Loss: 0.148272]\n",
      "2418 [Loss: 0.152724]\n",
      "2419 [Loss: 0.153838]\n",
      "2420 [Loss: 0.140290]\n",
      "2421 [Loss: 0.149944]\n",
      "2422 [Loss: 0.144221]\n",
      "2423 [Loss: 0.144093]\n",
      "2424 [Loss: 0.152441]\n",
      "2425 [Loss: 0.136008]\n",
      "2426 [Loss: 0.143489]\n",
      "2427 [Loss: 0.146924]\n",
      "2428 [Loss: 0.147230]\n",
      "2429 [Loss: 0.154876]\n",
      "2430 [Loss: 0.147463]\n",
      "2431 [Loss: 0.151995]\n",
      "2432 [Loss: 0.149127]\n",
      "2433 [Loss: 0.147505]\n",
      "2434 [Loss: 0.153944]\n",
      "2435 [Loss: 0.144378]\n",
      "2436 [Loss: 0.151451]\n",
      "2437 [Loss: 0.152792]\n",
      "2438 [Loss: 0.146513]\n",
      "2439 [Loss: 0.150988]\n",
      "2440 [Loss: 0.151129]\n",
      "2441 [Loss: 0.146751]\n",
      "2442 [Loss: 0.151882]\n",
      "2443 [Loss: 0.153041]\n",
      "2444 [Loss: 0.150096]\n",
      "2445 [Loss: 0.143945]\n",
      "2446 [Loss: 0.158990]\n",
      "2447 [Loss: 0.148974]\n",
      "2448 [Loss: 0.148022]\n",
      "2449 [Loss: 0.142419]\n",
      "2450 [Loss: 0.147098]\n",
      "2451 [Loss: 0.159941]\n",
      "2452 [Loss: 0.155301]\n",
      "2453 [Loss: 0.153474]\n",
      "2454 [Loss: 0.143957]\n",
      "2455 [Loss: 0.148286]\n",
      "2456 [Loss: 0.150576]\n",
      "2457 [Loss: 0.153562]\n",
      "2458 [Loss: 0.150994]\n",
      "2459 [Loss: 0.162919]\n",
      "2460 [Loss: 0.157632]\n",
      "2461 [Loss: 0.155332]\n",
      "2462 [Loss: 0.144824]\n",
      "2463 [Loss: 0.141700]\n",
      "2464 [Loss: 0.145832]\n",
      "2465 [Loss: 0.153756]\n",
      "2466 [Loss: 0.145524]\n",
      "2467 [Loss: 0.147496]\n",
      "2468 [Loss: 0.148550]\n",
      "2469 [Loss: 0.149992]\n",
      "2470 [Loss: 0.157986]\n",
      "2471 [Loss: 0.150005]\n",
      "2472 [Loss: 0.147067]\n",
      "2473 [Loss: 0.155104]\n",
      "2474 [Loss: 0.151735]\n",
      "2475 [Loss: 0.146817]\n",
      "2476 [Loss: 0.147205]\n",
      "2477 [Loss: 0.152503]\n",
      "2478 [Loss: 0.153107]\n",
      "2479 [Loss: 0.141248]\n",
      "2480 [Loss: 0.149680]\n",
      "2481 [Loss: 0.152762]\n",
      "2482 [Loss: 0.148125]\n",
      "2483 [Loss: 0.156630]\n",
      "2484 [Loss: 0.145512]\n",
      "2485 [Loss: 0.170990]\n",
      "2486 [Loss: 0.146960]\n",
      "2487 [Loss: 0.143391]\n",
      "2488 [Loss: 0.147916]\n",
      "2489 [Loss: 0.160219]\n",
      "2490 [Loss: 0.138030]\n",
      "2491 [Loss: 0.152890]\n",
      "2492 [Loss: 0.147646]\n",
      "2493 [Loss: 0.144212]\n",
      "2494 [Loss: 0.151472]\n",
      "2495 [Loss: 0.151937]\n",
      "2496 [Loss: 0.143072]\n",
      "2497 [Loss: 0.144997]\n",
      "2498 [Loss: 0.152303]\n",
      "2499 [Loss: 0.160560]\n",
      "2500 [Loss: 0.148940]\n",
      "2501 [Loss: 0.137455]\n",
      "2502 [Loss: 0.153211]\n",
      "2503 [Loss: 0.145220]\n",
      "2504 [Loss: 0.148456]\n",
      "2505 [Loss: 0.152770]\n",
      "2506 [Loss: 0.153288]\n",
      "2507 [Loss: 0.149674]\n",
      "2508 [Loss: 0.139602]\n",
      "2509 [Loss: 0.150866]\n",
      "2510 [Loss: 0.148922]\n",
      "2511 [Loss: 0.148422]\n",
      "2512 [Loss: 0.149293]\n",
      "2513 [Loss: 0.150725]\n",
      "2514 [Loss: 0.145808]\n",
      "2515 [Loss: 0.157442]\n",
      "2516 [Loss: 0.144196]\n",
      "2517 [Loss: 0.149328]\n",
      "2518 [Loss: 0.153190]\n",
      "2519 [Loss: 0.146307]\n",
      "2520 [Loss: 0.156750]\n",
      "2521 [Loss: 0.142975]\n",
      "2522 [Loss: 0.143908]\n",
      "2523 [Loss: 0.149000]\n",
      "2524 [Loss: 0.135163]\n",
      "2525 [Loss: 0.149709]\n",
      "2526 [Loss: 0.146997]\n",
      "2527 [Loss: 0.151974]\n",
      "2528 [Loss: 0.146966]\n",
      "2529 [Loss: 0.149424]\n",
      "2530 [Loss: 0.147889]\n",
      "2531 [Loss: 0.156410]\n",
      "2532 [Loss: 0.155145]\n",
      "2533 [Loss: 0.147922]\n",
      "2534 [Loss: 0.149511]\n",
      "2535 [Loss: 0.160535]\n",
      "2536 [Loss: 0.149019]\n",
      "2537 [Loss: 0.147392]\n",
      "2538 [Loss: 0.148246]\n",
      "2539 [Loss: 0.158343]\n",
      "2540 [Loss: 0.162172]\n",
      "2541 [Loss: 0.149356]\n",
      "2542 [Loss: 0.149556]\n",
      "2543 [Loss: 0.145815]\n",
      "2544 [Loss: 0.147003]\n",
      "2545 [Loss: 0.151777]\n",
      "2546 [Loss: 0.151027]\n",
      "2547 [Loss: 0.148430]\n",
      "2548 [Loss: 0.156984]\n",
      "2549 [Loss: 0.149988]\n",
      "2550 [Loss: 0.157044]\n",
      "2551 [Loss: 0.157950]\n",
      "2552 [Loss: 0.161589]\n",
      "2553 [Loss: 0.147704]\n",
      "2554 [Loss: 0.137932]\n",
      "2555 [Loss: 0.151665]\n",
      "2556 [Loss: 0.155364]\n",
      "2557 [Loss: 0.153488]\n",
      "2558 [Loss: 0.139381]\n",
      "2559 [Loss: 0.140791]\n",
      "2560 [Loss: 0.136247]\n",
      "2561 [Loss: 0.139524]\n",
      "2562 [Loss: 0.143739]\n",
      "2563 [Loss: 0.154966]\n",
      "2564 [Loss: 0.145079]\n",
      "2565 [Loss: 0.136034]\n",
      "2566 [Loss: 0.156276]\n",
      "2567 [Loss: 0.151465]\n",
      "2568 [Loss: 0.147878]\n",
      "2569 [Loss: 0.151856]\n",
      "2570 [Loss: 0.135911]\n",
      "2571 [Loss: 0.148317]\n",
      "2572 [Loss: 0.153825]\n",
      "2573 [Loss: 0.152434]\n",
      "2574 [Loss: 0.150695]\n",
      "2575 [Loss: 0.143476]\n",
      "2576 [Loss: 0.138437]\n",
      "2577 [Loss: 0.152948]\n",
      "2578 [Loss: 0.150040]\n",
      "2579 [Loss: 0.151361]\n",
      "2580 [Loss: 0.142262]\n",
      "2581 [Loss: 0.146351]\n",
      "2582 [Loss: 0.148444]\n",
      "2583 [Loss: 0.136038]\n",
      "2584 [Loss: 0.145807]\n",
      "2585 [Loss: 0.142410]\n",
      "2586 [Loss: 0.144758]\n",
      "2587 [Loss: 0.151390]\n",
      "2588 [Loss: 0.148387]\n",
      "2589 [Loss: 0.144943]\n",
      "2590 [Loss: 0.143367]\n",
      "2591 [Loss: 0.139265]\n",
      "2592 [Loss: 0.144389]\n",
      "2593 [Loss: 0.144519]\n",
      "2594 [Loss: 0.150246]\n",
      "2595 [Loss: 0.153582]\n",
      "2596 [Loss: 0.145941]\n",
      "2597 [Loss: 0.146748]\n",
      "2598 [Loss: 0.139823]\n",
      "2599 [Loss: 0.153144]\n",
      "2600 [Loss: 0.151450]\n",
      "2601 [Loss: 0.137353]\n",
      "2602 [Loss: 0.148888]\n",
      "2603 [Loss: 0.146986]\n",
      "2604 [Loss: 0.150629]\n",
      "2605 [Loss: 0.142663]\n",
      "2606 [Loss: 0.148881]\n",
      "2607 [Loss: 0.148797]\n",
      "2608 [Loss: 0.146708]\n",
      "2609 [Loss: 0.151542]\n",
      "2610 [Loss: 0.160155]\n",
      "2611 [Loss: 0.138490]\n",
      "2612 [Loss: 0.143941]\n",
      "2613 [Loss: 0.147829]\n",
      "2614 [Loss: 0.137688]\n",
      "2615 [Loss: 0.148190]\n",
      "2616 [Loss: 0.145068]\n",
      "2617 [Loss: 0.157140]\n",
      "2618 [Loss: 0.150163]\n",
      "2619 [Loss: 0.147557]\n",
      "2620 [Loss: 0.140270]\n",
      "2621 [Loss: 0.148556]\n",
      "2622 [Loss: 0.154018]\n",
      "2623 [Loss: 0.140973]\n",
      "2624 [Loss: 0.143750]\n",
      "2625 [Loss: 0.151458]\n",
      "2626 [Loss: 0.137671]\n",
      "2627 [Loss: 0.154722]\n",
      "2628 [Loss: 0.146118]\n",
      "2629 [Loss: 0.142872]\n",
      "2630 [Loss: 0.151975]\n",
      "2631 [Loss: 0.144307]\n",
      "2632 [Loss: 0.148776]\n",
      "2633 [Loss: 0.151138]\n",
      "2634 [Loss: 0.151215]\n",
      "2635 [Loss: 0.138300]\n",
      "2636 [Loss: 0.139299]\n",
      "2637 [Loss: 0.140892]\n",
      "2638 [Loss: 0.150220]\n",
      "2639 [Loss: 0.145246]\n",
      "2640 [Loss: 0.150763]\n",
      "2641 [Loss: 0.133564]\n",
      "2642 [Loss: 0.140507]\n",
      "2643 [Loss: 0.144233]\n",
      "2644 [Loss: 0.144919]\n",
      "2645 [Loss: 0.149146]\n",
      "2646 [Loss: 0.137077]\n",
      "2647 [Loss: 0.154661]\n",
      "2648 [Loss: 0.140237]\n",
      "2649 [Loss: 0.142586]\n",
      "2650 [Loss: 0.136759]\n",
      "2651 [Loss: 0.143558]\n",
      "2652 [Loss: 0.142830]\n",
      "2653 [Loss: 0.147088]\n",
      "2654 [Loss: 0.144043]\n",
      "2655 [Loss: 0.139938]\n",
      "2656 [Loss: 0.145071]\n",
      "2657 [Loss: 0.145270]\n",
      "2658 [Loss: 0.145745]\n",
      "2659 [Loss: 0.151136]\n",
      "2660 [Loss: 0.132691]\n",
      "2661 [Loss: 0.142110]\n",
      "2662 [Loss: 0.147129]\n",
      "2663 [Loss: 0.155505]\n",
      "2664 [Loss: 0.149901]\n",
      "2665 [Loss: 0.138085]\n",
      "2666 [Loss: 0.145756]\n",
      "2667 [Loss: 0.147710]\n",
      "2668 [Loss: 0.143397]\n",
      "2669 [Loss: 0.141989]\n",
      "2670 [Loss: 0.147977]\n",
      "2671 [Loss: 0.146192]\n",
      "2672 [Loss: 0.153172]\n",
      "2673 [Loss: 0.152024]\n",
      "2674 [Loss: 0.146749]\n",
      "2675 [Loss: 0.133611]\n",
      "2676 [Loss: 0.141444]\n",
      "2677 [Loss: 0.146351]\n",
      "2678 [Loss: 0.148321]\n",
      "2679 [Loss: 0.140032]\n",
      "2680 [Loss: 0.133165]\n",
      "2681 [Loss: 0.146841]\n",
      "2682 [Loss: 0.150360]\n",
      "2683 [Loss: 0.145284]\n",
      "2684 [Loss: 0.147016]\n",
      "2685 [Loss: 0.159527]\n",
      "2686 [Loss: 0.152517]\n",
      "2687 [Loss: 0.141329]\n",
      "2688 [Loss: 0.138408]\n",
      "2689 [Loss: 0.140856]\n",
      "2690 [Loss: 0.145591]\n",
      "2691 [Loss: 0.141856]\n",
      "2692 [Loss: 0.140933]\n",
      "2693 [Loss: 0.152963]\n",
      "2694 [Loss: 0.135465]\n",
      "2695 [Loss: 0.142881]\n",
      "2696 [Loss: 0.137913]\n",
      "2697 [Loss: 0.142655]\n",
      "2698 [Loss: 0.151008]\n",
      "2699 [Loss: 0.142137]\n",
      "2700 [Loss: 0.155141]\n",
      "2701 [Loss: 0.145746]\n",
      "2702 [Loss: 0.143055]\n",
      "2703 [Loss: 0.136656]\n",
      "2704 [Loss: 0.135648]\n",
      "2705 [Loss: 0.147317]\n",
      "2706 [Loss: 0.140151]\n",
      "2707 [Loss: 0.148078]\n",
      "2708 [Loss: 0.146525]\n",
      "2709 [Loss: 0.148195]\n",
      "2710 [Loss: 0.144824]\n",
      "2711 [Loss: 0.135204]\n",
      "2712 [Loss: 0.150572]\n",
      "2713 [Loss: 0.147574]\n",
      "2714 [Loss: 0.141731]\n",
      "2715 [Loss: 0.139525]\n",
      "2716 [Loss: 0.151273]\n",
      "2717 [Loss: 0.131978]\n",
      "2718 [Loss: 0.151426]\n",
      "2719 [Loss: 0.140068]\n",
      "2720 [Loss: 0.150411]\n",
      "2721 [Loss: 0.144106]\n",
      "2722 [Loss: 0.145397]\n",
      "2723 [Loss: 0.145617]\n",
      "2724 [Loss: 0.144019]\n",
      "2725 [Loss: 0.127003]\n",
      "2726 [Loss: 0.144849]\n",
      "2727 [Loss: 0.135500]\n",
      "2728 [Loss: 0.144508]\n",
      "2729 [Loss: 0.144937]\n",
      "2730 [Loss: 0.153934]\n",
      "2731 [Loss: 0.139717]\n",
      "2732 [Loss: 0.150392]\n",
      "2733 [Loss: 0.149281]\n",
      "2734 [Loss: 0.149789]\n",
      "2735 [Loss: 0.137056]\n",
      "2736 [Loss: 0.154826]\n",
      "2737 [Loss: 0.144480]\n",
      "2738 [Loss: 0.140734]\n",
      "2739 [Loss: 0.145808]\n",
      "2740 [Loss: 0.145962]\n",
      "2741 [Loss: 0.131573]\n",
      "2742 [Loss: 0.138791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2743 [Loss: 0.140711]\n",
      "2744 [Loss: 0.141311]\n",
      "2745 [Loss: 0.153969]\n",
      "2746 [Loss: 0.139787]\n",
      "2747 [Loss: 0.132511]\n",
      "2748 [Loss: 0.153133]\n",
      "2749 [Loss: 0.143954]\n",
      "2750 [Loss: 0.144061]\n",
      "2751 [Loss: 0.141365]\n",
      "2752 [Loss: 0.148566]\n",
      "2753 [Loss: 0.135025]\n",
      "2754 [Loss: 0.142633]\n",
      "2755 [Loss: 0.140167]\n",
      "2756 [Loss: 0.140877]\n",
      "2757 [Loss: 0.132301]\n",
      "2758 [Loss: 0.148288]\n",
      "2759 [Loss: 0.148104]\n",
      "2760 [Loss: 0.131774]\n",
      "2761 [Loss: 0.139471]\n",
      "2762 [Loss: 0.140824]\n",
      "2763 [Loss: 0.152416]\n",
      "2764 [Loss: 0.148147]\n",
      "2765 [Loss: 0.144155]\n",
      "2766 [Loss: 0.142010]\n",
      "2767 [Loss: 0.139409]\n",
      "2768 [Loss: 0.159975]\n",
      "2769 [Loss: 0.142954]\n",
      "2770 [Loss: 0.133022]\n",
      "2771 [Loss: 0.143296]\n",
      "2772 [Loss: 0.136229]\n",
      "2773 [Loss: 0.149527]\n",
      "2774 [Loss: 0.138776]\n",
      "2775 [Loss: 0.140583]\n",
      "2776 [Loss: 0.151406]\n",
      "2777 [Loss: 0.146321]\n",
      "2778 [Loss: 0.144760]\n",
      "2779 [Loss: 0.142915]\n",
      "2780 [Loss: 0.146311]\n",
      "2781 [Loss: 0.140649]\n",
      "2782 [Loss: 0.133404]\n",
      "2783 [Loss: 0.141735]\n",
      "2784 [Loss: 0.141397]\n",
      "2785 [Loss: 0.138245]\n",
      "2786 [Loss: 0.141939]\n",
      "2787 [Loss: 0.148154]\n",
      "2788 [Loss: 0.142644]\n",
      "2789 [Loss: 0.153669]\n",
      "2790 [Loss: 0.146506]\n",
      "2791 [Loss: 0.142148]\n",
      "2792 [Loss: 0.142423]\n",
      "2793 [Loss: 0.144099]\n",
      "2794 [Loss: 0.127320]\n",
      "2795 [Loss: 0.141989]\n",
      "2796 [Loss: 0.149567]\n",
      "2797 [Loss: 0.135307]\n",
      "2798 [Loss: 0.149400]\n",
      "2799 [Loss: 0.139621]\n",
      "2800 [Loss: 0.142166]\n",
      "2801 [Loss: 0.143749]\n",
      "2802 [Loss: 0.150620]\n",
      "2803 [Loss: 0.128972]\n",
      "2804 [Loss: 0.152740]\n",
      "2805 [Loss: 0.148805]\n",
      "2806 [Loss: 0.136267]\n",
      "2807 [Loss: 0.150068]\n",
      "2808 [Loss: 0.142525]\n",
      "2809 [Loss: 0.144686]\n",
      "2810 [Loss: 0.148183]\n",
      "2811 [Loss: 0.145610]\n",
      "2812 [Loss: 0.138790]\n",
      "2813 [Loss: 0.142360]\n",
      "2814 [Loss: 0.144466]\n",
      "2815 [Loss: 0.136614]\n",
      "2816 [Loss: 0.136195]\n",
      "2817 [Loss: 0.152113]\n",
      "2818 [Loss: 0.141891]\n",
      "2819 [Loss: 0.143916]\n",
      "2820 [Loss: 0.144504]\n",
      "2821 [Loss: 0.139560]\n",
      "2822 [Loss: 0.141121]\n",
      "2823 [Loss: 0.141274]\n",
      "2824 [Loss: 0.136107]\n",
      "2825 [Loss: 0.145576]\n",
      "2826 [Loss: 0.131780]\n",
      "2827 [Loss: 0.137873]\n",
      "2828 [Loss: 0.149079]\n",
      "2829 [Loss: 0.142750]\n",
      "2830 [Loss: 0.146056]\n",
      "2831 [Loss: 0.133987]\n",
      "2832 [Loss: 0.140858]\n",
      "2833 [Loss: 0.149826]\n",
      "2834 [Loss: 0.151556]\n",
      "2835 [Loss: 0.138752]\n",
      "2836 [Loss: 0.138822]\n",
      "2837 [Loss: 0.147835]\n",
      "2838 [Loss: 0.142369]\n",
      "2839 [Loss: 0.142618]\n",
      "2840 [Loss: 0.147927]\n",
      "2841 [Loss: 0.148642]\n",
      "2842 [Loss: 0.138266]\n",
      "2843 [Loss: 0.139240]\n",
      "2844 [Loss: 0.147492]\n",
      "2845 [Loss: 0.151778]\n",
      "2846 [Loss: 0.134836]\n",
      "2847 [Loss: 0.144451]\n",
      "2848 [Loss: 0.143483]\n",
      "2849 [Loss: 0.145742]\n",
      "2850 [Loss: 0.136648]\n",
      "2851 [Loss: 0.136304]\n",
      "2852 [Loss: 0.132933]\n",
      "2853 [Loss: 0.138583]\n",
      "2854 [Loss: 0.151816]\n",
      "2855 [Loss: 0.146142]\n",
      "2856 [Loss: 0.138433]\n",
      "2857 [Loss: 0.142206]\n",
      "2858 [Loss: 0.141934]\n",
      "2859 [Loss: 0.140191]\n",
      "2860 [Loss: 0.141091]\n",
      "2861 [Loss: 0.137859]\n",
      "2862 [Loss: 0.142964]\n",
      "2863 [Loss: 0.145360]\n",
      "2864 [Loss: 0.147736]\n",
      "2865 [Loss: 0.145040]\n",
      "2866 [Loss: 0.135751]\n",
      "2867 [Loss: 0.145554]\n",
      "2868 [Loss: 0.141172]\n",
      "2869 [Loss: 0.140766]\n",
      "2870 [Loss: 0.145104]\n",
      "2871 [Loss: 0.137773]\n",
      "2872 [Loss: 0.142160]\n",
      "2873 [Loss: 0.142218]\n",
      "2874 [Loss: 0.139085]\n",
      "2875 [Loss: 0.138760]\n",
      "2876 [Loss: 0.138579]\n",
      "2877 [Loss: 0.136284]\n",
      "2878 [Loss: 0.145101]\n",
      "2879 [Loss: 0.138677]\n",
      "2880 [Loss: 0.134218]\n",
      "2881 [Loss: 0.145783]\n",
      "2882 [Loss: 0.133620]\n",
      "2883 [Loss: 0.132426]\n",
      "2884 [Loss: 0.138054]\n",
      "2885 [Loss: 0.148940]\n",
      "2886 [Loss: 0.130247]\n",
      "2887 [Loss: 0.154959]\n",
      "2888 [Loss: 0.145133]\n",
      "2889 [Loss: 0.133483]\n",
      "2890 [Loss: 0.142050]\n",
      "2891 [Loss: 0.136820]\n",
      "2892 [Loss: 0.134463]\n",
      "2893 [Loss: 0.148498]\n",
      "2894 [Loss: 0.143470]\n",
      "2895 [Loss: 0.133370]\n",
      "2896 [Loss: 0.146272]\n",
      "2897 [Loss: 0.145706]\n",
      "2898 [Loss: 0.142877]\n",
      "2899 [Loss: 0.145390]\n",
      "2900 [Loss: 0.143473]\n",
      "2901 [Loss: 0.139355]\n",
      "2902 [Loss: 0.135600]\n",
      "2903 [Loss: 0.151288]\n",
      "2904 [Loss: 0.145190]\n",
      "2905 [Loss: 0.138519]\n",
      "2906 [Loss: 0.137842]\n",
      "2907 [Loss: 0.137423]\n",
      "2908 [Loss: 0.138058]\n",
      "2909 [Loss: 0.147515]\n",
      "2910 [Loss: 0.139457]\n",
      "2911 [Loss: 0.146374]\n",
      "2912 [Loss: 0.148338]\n",
      "2913 [Loss: 0.133220]\n",
      "2914 [Loss: 0.131273]\n",
      "2915 [Loss: 0.138620]\n",
      "2916 [Loss: 0.148168]\n",
      "2917 [Loss: 0.138283]\n",
      "2918 [Loss: 0.151113]\n",
      "2919 [Loss: 0.136724]\n",
      "2920 [Loss: 0.142133]\n",
      "2921 [Loss: 0.149829]\n",
      "2922 [Loss: 0.141191]\n",
      "2923 [Loss: 0.144892]\n",
      "2924 [Loss: 0.142387]\n",
      "2925 [Loss: 0.143107]\n",
      "2926 [Loss: 0.141241]\n",
      "2927 [Loss: 0.141783]\n",
      "2928 [Loss: 0.134177]\n",
      "2929 [Loss: 0.135784]\n",
      "2930 [Loss: 0.148792]\n",
      "2931 [Loss: 0.138880]\n",
      "2932 [Loss: 0.139493]\n",
      "2933 [Loss: 0.135447]\n",
      "2934 [Loss: 0.136042]\n",
      "2935 [Loss: 0.146100]\n",
      "2936 [Loss: 0.132615]\n",
      "2937 [Loss: 0.145686]\n",
      "2938 [Loss: 0.144820]\n",
      "2939 [Loss: 0.134720]\n",
      "2940 [Loss: 0.148956]\n",
      "2941 [Loss: 0.130667]\n",
      "2942 [Loss: 0.138024]\n",
      "2943 [Loss: 0.134621]\n",
      "2944 [Loss: 0.138771]\n",
      "2945 [Loss: 0.139104]\n",
      "2946 [Loss: 0.146377]\n",
      "2947 [Loss: 0.138036]\n",
      "2948 [Loss: 0.137297]\n",
      "2949 [Loss: 0.145038]\n",
      "2950 [Loss: 0.140354]\n",
      "2951 [Loss: 0.137483]\n",
      "2952 [Loss: 0.142570]\n",
      "2953 [Loss: 0.139578]\n",
      "2954 [Loss: 0.143758]\n",
      "2955 [Loss: 0.136722]\n",
      "2956 [Loss: 0.138396]\n",
      "2957 [Loss: 0.150032]\n",
      "2958 [Loss: 0.140951]\n",
      "2959 [Loss: 0.131932]\n",
      "2960 [Loss: 0.134850]\n",
      "2961 [Loss: 0.141971]\n",
      "2962 [Loss: 0.145485]\n",
      "2963 [Loss: 0.128779]\n",
      "2964 [Loss: 0.138410]\n",
      "2965 [Loss: 0.143540]\n",
      "2966 [Loss: 0.134387]\n",
      "2967 [Loss: 0.140836]\n",
      "2968 [Loss: 0.142320]\n",
      "2969 [Loss: 0.139505]\n",
      "2970 [Loss: 0.139529]\n",
      "2971 [Loss: 0.139162]\n",
      "2972 [Loss: 0.134750]\n",
      "2973 [Loss: 0.130729]\n",
      "2974 [Loss: 0.141117]\n",
      "2975 [Loss: 0.141909]\n",
      "2976 [Loss: 0.133878]\n",
      "2977 [Loss: 0.138150]\n",
      "2978 [Loss: 0.142025]\n",
      "2979 [Loss: 0.144623]\n",
      "2980 [Loss: 0.132694]\n",
      "2981 [Loss: 0.139351]\n",
      "2982 [Loss: 0.143552]\n",
      "2983 [Loss: 0.135375]\n",
      "2984 [Loss: 0.144957]\n",
      "2985 [Loss: 0.143505]\n",
      "2986 [Loss: 0.131305]\n",
      "2987 [Loss: 0.145543]\n",
      "2988 [Loss: 0.131742]\n",
      "2989 [Loss: 0.140110]\n",
      "2990 [Loss: 0.137532]\n",
      "2991 [Loss: 0.142205]\n",
      "2992 [Loss: 0.129953]\n",
      "2993 [Loss: 0.145079]\n",
      "2994 [Loss: 0.135842]\n",
      "2995 [Loss: 0.145456]\n",
      "2996 [Loss: 0.144503]\n",
      "2997 [Loss: 0.139296]\n",
      "2998 [Loss: 0.137477]\n",
      "2999 [Loss: 0.140398]\n",
      "3000 [Loss: 0.136105]\n",
      "3001 [Loss: 0.138336]\n",
      "3002 [Loss: 0.134529]\n",
      "3003 [Loss: 0.139350]\n",
      "3004 [Loss: 0.157734]\n",
      "3005 [Loss: 0.132657]\n",
      "3006 [Loss: 0.134424]\n",
      "3007 [Loss: 0.145625]\n",
      "3008 [Loss: 0.134462]\n",
      "3009 [Loss: 0.152125]\n",
      "3010 [Loss: 0.143388]\n",
      "3011 [Loss: 0.129654]\n",
      "3012 [Loss: 0.134596]\n",
      "3013 [Loss: 0.134658]\n",
      "3014 [Loss: 0.138939]\n",
      "3015 [Loss: 0.138860]\n",
      "3016 [Loss: 0.135880]\n",
      "3017 [Loss: 0.136600]\n",
      "3018 [Loss: 0.152716]\n",
      "3019 [Loss: 0.147992]\n",
      "3020 [Loss: 0.143289]\n",
      "3021 [Loss: 0.141329]\n",
      "3022 [Loss: 0.138959]\n",
      "3023 [Loss: 0.146031]\n",
      "3024 [Loss: 0.137513]\n",
      "3025 [Loss: 0.144435]\n",
      "3026 [Loss: 0.136013]\n",
      "3027 [Loss: 0.140542]\n",
      "3028 [Loss: 0.140917]\n",
      "3029 [Loss: 0.130672]\n",
      "3030 [Loss: 0.143766]\n",
      "3031 [Loss: 0.139669]\n",
      "3032 [Loss: 0.138937]\n",
      "3033 [Loss: 0.133378]\n",
      "3034 [Loss: 0.129387]\n",
      "3035 [Loss: 0.145563]\n",
      "3036 [Loss: 0.130116]\n",
      "3037 [Loss: 0.141861]\n",
      "3038 [Loss: 0.143339]\n",
      "3039 [Loss: 0.131419]\n",
      "3040 [Loss: 0.132590]\n",
      "3041 [Loss: 0.137614]\n",
      "3042 [Loss: 0.133446]\n",
      "3043 [Loss: 0.138090]\n",
      "3044 [Loss: 0.144399]\n",
      "3045 [Loss: 0.150252]\n",
      "3046 [Loss: 0.140301]\n",
      "3047 [Loss: 0.147738]\n",
      "3048 [Loss: 0.134409]\n",
      "3049 [Loss: 0.139164]\n",
      "3050 [Loss: 0.143695]\n",
      "3051 [Loss: 0.138341]\n",
      "3052 [Loss: 0.140991]\n",
      "3053 [Loss: 0.137557]\n",
      "3054 [Loss: 0.131125]\n",
      "3055 [Loss: 0.134533]\n",
      "3056 [Loss: 0.145701]\n",
      "3057 [Loss: 0.137783]\n",
      "3058 [Loss: 0.153282]\n",
      "3059 [Loss: 0.141897]\n",
      "3060 [Loss: 0.135103]\n",
      "3061 [Loss: 0.143451]\n",
      "3062 [Loss: 0.137218]\n",
      "3063 [Loss: 0.138193]\n",
      "3064 [Loss: 0.137602]\n",
      "3065 [Loss: 0.137935]\n",
      "3066 [Loss: 0.137120]\n",
      "3067 [Loss: 0.135948]\n",
      "3068 [Loss: 0.137920]\n",
      "3069 [Loss: 0.147517]\n",
      "3070 [Loss: 0.137924]\n",
      "3071 [Loss: 0.128435]\n",
      "3072 [Loss: 0.138646]\n",
      "3073 [Loss: 0.147392]\n",
      "3074 [Loss: 0.134285]\n",
      "3075 [Loss: 0.138622]\n",
      "3076 [Loss: 0.137901]\n",
      "3077 [Loss: 0.145020]\n",
      "3078 [Loss: 0.138060]\n",
      "3079 [Loss: 0.142582]\n",
      "3080 [Loss: 0.144168]\n",
      "3081 [Loss: 0.137972]\n",
      "3082 [Loss: 0.132870]\n",
      "3083 [Loss: 0.138748]\n",
      "3084 [Loss: 0.137237]\n",
      "3085 [Loss: 0.144237]\n",
      "3086 [Loss: 0.140541]\n",
      "3087 [Loss: 0.136588]\n",
      "3088 [Loss: 0.136853]\n",
      "3089 [Loss: 0.133231]\n",
      "3090 [Loss: 0.140723]\n",
      "3091 [Loss: 0.140166]\n",
      "3092 [Loss: 0.140272]\n",
      "3093 [Loss: 0.136763]\n",
      "3094 [Loss: 0.138810]\n",
      "3095 [Loss: 0.131777]\n",
      "3096 [Loss: 0.138081]\n",
      "3097 [Loss: 0.125495]\n",
      "3098 [Loss: 0.137717]\n",
      "3099 [Loss: 0.128142]\n",
      "3100 [Loss: 0.139563]\n",
      "3101 [Loss: 0.148887]\n",
      "3102 [Loss: 0.137830]\n",
      "3103 [Loss: 0.141059]\n",
      "3104 [Loss: 0.140581]\n",
      "3105 [Loss: 0.136334]\n",
      "3106 [Loss: 0.140805]\n",
      "3107 [Loss: 0.140948]\n",
      "3108 [Loss: 0.138658]\n",
      "3109 [Loss: 0.133913]\n",
      "3110 [Loss: 0.140242]\n",
      "3111 [Loss: 0.143662]\n",
      "3112 [Loss: 0.140517]\n",
      "3113 [Loss: 0.132252]\n",
      "3114 [Loss: 0.136783]\n",
      "3115 [Loss: 0.134742]\n",
      "3116 [Loss: 0.132354]\n",
      "3117 [Loss: 0.144634]\n",
      "3118 [Loss: 0.147997]\n",
      "3119 [Loss: 0.134507]\n",
      "3120 [Loss: 0.135982]\n",
      "3121 [Loss: 0.133106]\n",
      "3122 [Loss: 0.130912]\n",
      "3123 [Loss: 0.135689]\n",
      "3124 [Loss: 0.131634]\n",
      "3125 [Loss: 0.135203]\n",
      "3126 [Loss: 0.132922]\n",
      "3127 [Loss: 0.135903]\n",
      "3128 [Loss: 0.138645]\n",
      "3129 [Loss: 0.137217]\n",
      "3130 [Loss: 0.133479]\n",
      "3131 [Loss: 0.137247]\n",
      "3132 [Loss: 0.136914]\n",
      "3133 [Loss: 0.145570]\n",
      "3134 [Loss: 0.138808]\n",
      "3135 [Loss: 0.136702]\n",
      "3136 [Loss: 0.128555]\n",
      "3137 [Loss: 0.135495]\n",
      "3138 [Loss: 0.132582]\n",
      "3139 [Loss: 0.146007]\n",
      "3140 [Loss: 0.132400]\n",
      "3141 [Loss: 0.133714]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3142 [Loss: 0.139654]\n",
      "3143 [Loss: 0.143608]\n",
      "3144 [Loss: 0.132861]\n",
      "3145 [Loss: 0.133577]\n",
      "3146 [Loss: 0.123210]\n",
      "3147 [Loss: 0.148114]\n",
      "3148 [Loss: 0.143030]\n",
      "3149 [Loss: 0.137059]\n",
      "3150 [Loss: 0.127083]\n",
      "3151 [Loss: 0.127317]\n",
      "3152 [Loss: 0.131194]\n",
      "3153 [Loss: 0.138633]\n",
      "3154 [Loss: 0.138848]\n",
      "3155 [Loss: 0.137262]\n",
      "3156 [Loss: 0.143251]\n",
      "3157 [Loss: 0.137106]\n",
      "3158 [Loss: 0.126377]\n",
      "3159 [Loss: 0.135307]\n",
      "3160 [Loss: 0.145456]\n",
      "3161 [Loss: 0.130705]\n",
      "3162 [Loss: 0.148727]\n",
      "3163 [Loss: 0.127821]\n",
      "3164 [Loss: 0.140833]\n",
      "3165 [Loss: 0.140707]\n",
      "3166 [Loss: 0.134975]\n",
      "3167 [Loss: 0.140092]\n",
      "3168 [Loss: 0.130350]\n",
      "3169 [Loss: 0.133097]\n",
      "3170 [Loss: 0.130993]\n",
      "3171 [Loss: 0.142936]\n",
      "3172 [Loss: 0.135573]\n",
      "3173 [Loss: 0.145692]\n",
      "3174 [Loss: 0.140402]\n",
      "3175 [Loss: 0.134165]\n",
      "3176 [Loss: 0.138172]\n",
      "3177 [Loss: 0.141602]\n",
      "3178 [Loss: 0.137867]\n",
      "3179 [Loss: 0.136121]\n",
      "3180 [Loss: 0.136296]\n",
      "3181 [Loss: 0.133289]\n",
      "3182 [Loss: 0.136248]\n",
      "3183 [Loss: 0.129453]\n",
      "3184 [Loss: 0.143680]\n",
      "3185 [Loss: 0.143019]\n",
      "3186 [Loss: 0.132938]\n",
      "3187 [Loss: 0.139100]\n",
      "3188 [Loss: 0.132715]\n",
      "3189 [Loss: 0.141991]\n",
      "3190 [Loss: 0.147121]\n",
      "3191 [Loss: 0.139891]\n",
      "3192 [Loss: 0.135039]\n",
      "3193 [Loss: 0.151298]\n",
      "3194 [Loss: 0.136311]\n",
      "3195 [Loss: 0.141732]\n",
      "3196 [Loss: 0.134202]\n",
      "3197 [Loss: 0.132973]\n",
      "3198 [Loss: 0.145505]\n",
      "3199 [Loss: 0.132580]\n",
      "3200 [Loss: 0.137006]\n",
      "3201 [Loss: 0.121353]\n",
      "3202 [Loss: 0.142634]\n",
      "3203 [Loss: 0.133291]\n",
      "3204 [Loss: 0.128417]\n",
      "3205 [Loss: 0.134517]\n",
      "3206 [Loss: 0.134655]\n",
      "3207 [Loss: 0.133005]\n",
      "3208 [Loss: 0.141978]\n",
      "3209 [Loss: 0.134347]\n",
      "3210 [Loss: 0.136059]\n",
      "3211 [Loss: 0.138701]\n",
      "3212 [Loss: 0.136230]\n",
      "3213 [Loss: 0.141660]\n",
      "3214 [Loss: 0.136082]\n",
      "3215 [Loss: 0.136967]\n",
      "3216 [Loss: 0.137462]\n",
      "3217 [Loss: 0.136200]\n",
      "3218 [Loss: 0.136700]\n",
      "3219 [Loss: 0.130767]\n",
      "3220 [Loss: 0.138911]\n",
      "3221 [Loss: 0.135462]\n",
      "3222 [Loss: 0.138701]\n",
      "3223 [Loss: 0.136747]\n",
      "3224 [Loss: 0.125177]\n",
      "3225 [Loss: 0.136972]\n",
      "3226 [Loss: 0.132683]\n",
      "3227 [Loss: 0.144145]\n",
      "3228 [Loss: 0.129038]\n",
      "3229 [Loss: 0.136502]\n",
      "3230 [Loss: 0.140927]\n",
      "3231 [Loss: 0.144648]\n",
      "3232 [Loss: 0.131351]\n",
      "3233 [Loss: 0.134430]\n",
      "3234 [Loss: 0.138949]\n",
      "3235 [Loss: 0.141522]\n",
      "3236 [Loss: 0.137446]\n",
      "3237 [Loss: 0.145669]\n",
      "3238 [Loss: 0.132250]\n",
      "3239 [Loss: 0.133006]\n",
      "3240 [Loss: 0.131781]\n",
      "3241 [Loss: 0.122149]\n",
      "3242 [Loss: 0.146092]\n",
      "3243 [Loss: 0.134667]\n",
      "3244 [Loss: 0.134442]\n",
      "3245 [Loss: 0.141310]\n",
      "3246 [Loss: 0.125993]\n",
      "3247 [Loss: 0.131014]\n",
      "3248 [Loss: 0.144686]\n",
      "3249 [Loss: 0.133581]\n",
      "3250 [Loss: 0.136604]\n",
      "3251 [Loss: 0.136695]\n",
      "3252 [Loss: 0.146713]\n",
      "3253 [Loss: 0.140656]\n",
      "3254 [Loss: 0.130573]\n",
      "3255 [Loss: 0.136220]\n",
      "3256 [Loss: 0.137982]\n",
      "3257 [Loss: 0.131816]\n",
      "3258 [Loss: 0.141077]\n",
      "3259 [Loss: 0.135371]\n",
      "3260 [Loss: 0.138526]\n",
      "3261 [Loss: 0.132930]\n",
      "3262 [Loss: 0.141898]\n",
      "3263 [Loss: 0.132955]\n",
      "3264 [Loss: 0.140237]\n",
      "3265 [Loss: 0.135976]\n",
      "3266 [Loss: 0.133059]\n",
      "3267 [Loss: 0.131383]\n",
      "3268 [Loss: 0.130579]\n",
      "3269 [Loss: 0.142027]\n",
      "3270 [Loss: 0.133971]\n",
      "3271 [Loss: 0.134296]\n",
      "3272 [Loss: 0.132588]\n",
      "3273 [Loss: 0.139687]\n",
      "3274 [Loss: 0.138876]\n",
      "3275 [Loss: 0.133333]\n",
      "3276 [Loss: 0.131471]\n",
      "3277 [Loss: 0.125423]\n",
      "3278 [Loss: 0.131526]\n",
      "3279 [Loss: 0.134683]\n",
      "3280 [Loss: 0.130653]\n",
      "3281 [Loss: 0.131854]\n",
      "3282 [Loss: 0.132747]\n",
      "3283 [Loss: 0.137600]\n",
      "3284 [Loss: 0.131900]\n",
      "3285 [Loss: 0.132207]\n",
      "3286 [Loss: 0.131922]\n",
      "3287 [Loss: 0.137029]\n",
      "3288 [Loss: 0.128921]\n",
      "3289 [Loss: 0.142411]\n",
      "3290 [Loss: 0.134288]\n",
      "3291 [Loss: 0.131096]\n",
      "3292 [Loss: 0.123903]\n",
      "3293 [Loss: 0.133620]\n",
      "3294 [Loss: 0.134236]\n",
      "3295 [Loss: 0.136183]\n",
      "3296 [Loss: 0.142844]\n",
      "3297 [Loss: 0.134578]\n",
      "3298 [Loss: 0.147222]\n",
      "3299 [Loss: 0.140764]\n",
      "3300 [Loss: 0.125923]\n",
      "3301 [Loss: 0.135470]\n",
      "3302 [Loss: 0.132358]\n",
      "3303 [Loss: 0.129713]\n",
      "3304 [Loss: 0.131622]\n",
      "3305 [Loss: 0.133756]\n",
      "3306 [Loss: 0.131822]\n",
      "3307 [Loss: 0.125420]\n",
      "3308 [Loss: 0.130854]\n",
      "3309 [Loss: 0.145375]\n",
      "3310 [Loss: 0.135116]\n",
      "3311 [Loss: 0.130647]\n",
      "3312 [Loss: 0.139749]\n",
      "3313 [Loss: 0.133822]\n",
      "3314 [Loss: 0.136420]\n",
      "3315 [Loss: 0.136689]\n",
      "3316 [Loss: 0.140505]\n",
      "3317 [Loss: 0.134273]\n",
      "3318 [Loss: 0.130963]\n",
      "3319 [Loss: 0.124219]\n",
      "3320 [Loss: 0.131133]\n",
      "3321 [Loss: 0.133220]\n",
      "3322 [Loss: 0.128775]\n",
      "3323 [Loss: 0.139676]\n",
      "3324 [Loss: 0.130135]\n",
      "3325 [Loss: 0.130457]\n",
      "3326 [Loss: 0.134380]\n",
      "3327 [Loss: 0.133983]\n",
      "3328 [Loss: 0.142805]\n",
      "3329 [Loss: 0.138766]\n",
      "3330 [Loss: 0.139652]\n",
      "3331 [Loss: 0.138205]\n",
      "3332 [Loss: 0.140244]\n",
      "3333 [Loss: 0.145674]\n",
      "3334 [Loss: 0.131017]\n",
      "3335 [Loss: 0.128690]\n",
      "3336 [Loss: 0.131003]\n",
      "3337 [Loss: 0.125132]\n",
      "3338 [Loss: 0.135206]\n",
      "3339 [Loss: 0.140564]\n",
      "3340 [Loss: 0.138691]\n",
      "3341 [Loss: 0.146265]\n",
      "3342 [Loss: 0.139771]\n",
      "3343 [Loss: 0.135278]\n",
      "3344 [Loss: 0.137205]\n",
      "3345 [Loss: 0.137709]\n",
      "3346 [Loss: 0.134909]\n",
      "3347 [Loss: 0.140125]\n",
      "3348 [Loss: 0.133790]\n",
      "3349 [Loss: 0.127311]\n",
      "3350 [Loss: 0.135403]\n",
      "3351 [Loss: 0.134276]\n",
      "3352 [Loss: 0.140630]\n",
      "3353 [Loss: 0.137044]\n",
      "3354 [Loss: 0.133997]\n",
      "3355 [Loss: 0.133669]\n",
      "3356 [Loss: 0.139064]\n",
      "3357 [Loss: 0.127675]\n",
      "3358 [Loss: 0.132060]\n",
      "3359 [Loss: 0.126011]\n",
      "3360 [Loss: 0.139823]\n",
      "3361 [Loss: 0.122490]\n",
      "3362 [Loss: 0.141031]\n",
      "3363 [Loss: 0.145571]\n",
      "3364 [Loss: 0.123741]\n",
      "3365 [Loss: 0.134883]\n",
      "3366 [Loss: 0.133428]\n",
      "3367 [Loss: 0.138483]\n",
      "3368 [Loss: 0.131476]\n",
      "3369 [Loss: 0.128437]\n",
      "3370 [Loss: 0.134620]\n",
      "3371 [Loss: 0.125425]\n",
      "3372 [Loss: 0.132450]\n",
      "3373 [Loss: 0.133511]\n",
      "3374 [Loss: 0.131179]\n",
      "3375 [Loss: 0.129472]\n",
      "3376 [Loss: 0.142551]\n",
      "3377 [Loss: 0.131388]\n",
      "3378 [Loss: 0.132763]\n",
      "3379 [Loss: 0.134869]\n",
      "3380 [Loss: 0.126135]\n",
      "3381 [Loss: 0.129346]\n",
      "3382 [Loss: 0.124489]\n",
      "3383 [Loss: 0.137536]\n",
      "3384 [Loss: 0.130747]\n",
      "3385 [Loss: 0.130656]\n",
      "3386 [Loss: 0.137460]\n",
      "3387 [Loss: 0.129951]\n",
      "3388 [Loss: 0.137463]\n",
      "3389 [Loss: 0.132123]\n",
      "3390 [Loss: 0.128218]\n",
      "3391 [Loss: 0.136228]\n",
      "3392 [Loss: 0.134597]\n",
      "3393 [Loss: 0.124922]\n",
      "3394 [Loss: 0.128562]\n",
      "3395 [Loss: 0.128921]\n",
      "3396 [Loss: 0.142399]\n",
      "3397 [Loss: 0.129320]\n",
      "3398 [Loss: 0.129807]\n",
      "3399 [Loss: 0.127497]\n",
      "3400 [Loss: 0.130087]\n",
      "3401 [Loss: 0.130820]\n",
      "3402 [Loss: 0.134554]\n",
      "3403 [Loss: 0.132130]\n",
      "3404 [Loss: 0.131961]\n",
      "3405 [Loss: 0.137810]\n",
      "3406 [Loss: 0.140537]\n",
      "3407 [Loss: 0.136878]\n",
      "3408 [Loss: 0.131464]\n",
      "3409 [Loss: 0.137919]\n",
      "3410 [Loss: 0.138885]\n",
      "3411 [Loss: 0.131936]\n",
      "3412 [Loss: 0.136692]\n",
      "3413 [Loss: 0.134708]\n",
      "3414 [Loss: 0.128733]\n",
      "3415 [Loss: 0.139057]\n",
      "3416 [Loss: 0.127567]\n",
      "3417 [Loss: 0.122415]\n",
      "3418 [Loss: 0.137613]\n",
      "3419 [Loss: 0.133697]\n",
      "3420 [Loss: 0.134133]\n",
      "3421 [Loss: 0.125506]\n",
      "3422 [Loss: 0.132704]\n",
      "3423 [Loss: 0.138801]\n",
      "3424 [Loss: 0.145597]\n",
      "3425 [Loss: 0.126170]\n",
      "3426 [Loss: 0.135241]\n",
      "3427 [Loss: 0.134453]\n",
      "3428 [Loss: 0.135135]\n",
      "3429 [Loss: 0.127684]\n",
      "3430 [Loss: 0.129989]\n",
      "3431 [Loss: 0.130829]\n",
      "3432 [Loss: 0.140745]\n",
      "3433 [Loss: 0.133484]\n",
      "3434 [Loss: 0.125193]\n",
      "3435 [Loss: 0.129377]\n",
      "3436 [Loss: 0.138505]\n",
      "3437 [Loss: 0.135065]\n",
      "3438 [Loss: 0.134323]\n",
      "3439 [Loss: 0.139580]\n",
      "3440 [Loss: 0.129889]\n",
      "3441 [Loss: 0.132904]\n",
      "3442 [Loss: 0.124311]\n",
      "3443 [Loss: 0.137700]\n",
      "3444 [Loss: 0.133085]\n",
      "3445 [Loss: 0.130989]\n",
      "3446 [Loss: 0.131838]\n",
      "3447 [Loss: 0.139602]\n",
      "3448 [Loss: 0.135695]\n",
      "3449 [Loss: 0.126787]\n",
      "3450 [Loss: 0.133730]\n",
      "3451 [Loss: 0.128905]\n",
      "3452 [Loss: 0.132638]\n",
      "3453 [Loss: 0.135592]\n",
      "3454 [Loss: 0.126935]\n",
      "3455 [Loss: 0.137169]\n",
      "3456 [Loss: 0.126372]\n",
      "3457 [Loss: 0.134566]\n",
      "3458 [Loss: 0.125437]\n",
      "3459 [Loss: 0.137014]\n",
      "3460 [Loss: 0.133211]\n",
      "3461 [Loss: 0.138482]\n",
      "3462 [Loss: 0.129126]\n",
      "3463 [Loss: 0.130109]\n",
      "3464 [Loss: 0.133248]\n",
      "3465 [Loss: 0.139839]\n",
      "3466 [Loss: 0.128976]\n",
      "3467 [Loss: 0.114896]\n",
      "3468 [Loss: 0.141272]\n",
      "3469 [Loss: 0.135810]\n",
      "3470 [Loss: 0.138445]\n",
      "3471 [Loss: 0.136649]\n",
      "3472 [Loss: 0.128624]\n",
      "3473 [Loss: 0.135211]\n",
      "3474 [Loss: 0.116678]\n",
      "3475 [Loss: 0.129028]\n",
      "3476 [Loss: 0.134510]\n",
      "3477 [Loss: 0.129803]\n",
      "3478 [Loss: 0.128279]\n",
      "3479 [Loss: 0.142679]\n",
      "3480 [Loss: 0.135155]\n",
      "3481 [Loss: 0.131628]\n",
      "3482 [Loss: 0.137482]\n",
      "3483 [Loss: 0.132192]\n",
      "3484 [Loss: 0.139704]\n",
      "3485 [Loss: 0.132667]\n",
      "3486 [Loss: 0.132898]\n",
      "3487 [Loss: 0.130293]\n",
      "3488 [Loss: 0.134923]\n",
      "3489 [Loss: 0.129508]\n",
      "3490 [Loss: 0.127718]\n",
      "3491 [Loss: 0.130993]\n",
      "3492 [Loss: 0.141891]\n",
      "3493 [Loss: 0.151376]\n",
      "3494 [Loss: 0.128602]\n",
      "3495 [Loss: 0.134076]\n",
      "3496 [Loss: 0.132678]\n",
      "3497 [Loss: 0.126689]\n",
      "3498 [Loss: 0.124724]\n",
      "3499 [Loss: 0.136517]\n",
      "3500 [Loss: 0.120236]\n",
      "3501 [Loss: 0.128545]\n",
      "3502 [Loss: 0.134851]\n",
      "3503 [Loss: 0.128221]\n",
      "3504 [Loss: 0.140621]\n",
      "3505 [Loss: 0.138886]\n",
      "3506 [Loss: 0.136789]\n",
      "3507 [Loss: 0.123798]\n",
      "3508 [Loss: 0.142074]\n",
      "3509 [Loss: 0.134539]\n",
      "3510 [Loss: 0.133962]\n",
      "3511 [Loss: 0.125102]\n",
      "3512 [Loss: 0.124093]\n",
      "3513 [Loss: 0.128015]\n",
      "3514 [Loss: 0.129531]\n",
      "3515 [Loss: 0.135134]\n",
      "3516 [Loss: 0.134654]\n",
      "3517 [Loss: 0.130525]\n",
      "3518 [Loss: 0.133023]\n",
      "3519 [Loss: 0.133668]\n",
      "3520 [Loss: 0.143069]\n",
      "3521 [Loss: 0.126923]\n",
      "3522 [Loss: 0.134072]\n",
      "3523 [Loss: 0.134603]\n",
      "3524 [Loss: 0.128104]\n",
      "3525 [Loss: 0.131587]\n",
      "3526 [Loss: 0.132786]\n",
      "3527 [Loss: 0.130347]\n",
      "3528 [Loss: 0.136136]\n",
      "3529 [Loss: 0.137894]\n",
      "3530 [Loss: 0.136213]\n",
      "3531 [Loss: 0.126179]\n",
      "3532 [Loss: 0.130289]\n",
      "3533 [Loss: 0.127853]\n",
      "3534 [Loss: 0.139904]\n",
      "3535 [Loss: 0.135320]\n",
      "3536 [Loss: 0.130985]\n",
      "3537 [Loss: 0.121723]\n",
      "3538 [Loss: 0.134154]\n",
      "3539 [Loss: 0.135810]\n",
      "3540 [Loss: 0.130063]\n",
      "3541 [Loss: 0.132410]\n",
      "3542 [Loss: 0.137598]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3543 [Loss: 0.135089]\n",
      "3544 [Loss: 0.135452]\n",
      "3545 [Loss: 0.124377]\n",
      "3546 [Loss: 0.136897]\n",
      "3547 [Loss: 0.128129]\n",
      "3548 [Loss: 0.129441]\n",
      "3549 [Loss: 0.125019]\n",
      "3550 [Loss: 0.132835]\n",
      "3551 [Loss: 0.125922]\n",
      "3552 [Loss: 0.136588]\n",
      "3553 [Loss: 0.128759]\n",
      "3554 [Loss: 0.136483]\n",
      "3555 [Loss: 0.133784]\n",
      "3556 [Loss: 0.134354]\n",
      "3557 [Loss: 0.141939]\n",
      "3558 [Loss: 0.127007]\n",
      "3559 [Loss: 0.124400]\n",
      "3560 [Loss: 0.127458]\n",
      "3561 [Loss: 0.121182]\n",
      "3562 [Loss: 0.129544]\n",
      "3563 [Loss: 0.136584]\n",
      "3564 [Loss: 0.137312]\n",
      "3565 [Loss: 0.130298]\n",
      "3566 [Loss: 0.133994]\n",
      "3567 [Loss: 0.138996]\n",
      "3568 [Loss: 0.136256]\n",
      "3569 [Loss: 0.126514]\n",
      "3570 [Loss: 0.133126]\n",
      "3571 [Loss: 0.129212]\n",
      "3572 [Loss: 0.133915]\n",
      "3573 [Loss: 0.128755]\n",
      "3574 [Loss: 0.132792]\n",
      "3575 [Loss: 0.145350]\n",
      "3576 [Loss: 0.134563]\n",
      "3577 [Loss: 0.128257]\n",
      "3578 [Loss: 0.134376]\n",
      "3579 [Loss: 0.134315]\n",
      "3580 [Loss: 0.133897]\n",
      "3581 [Loss: 0.125401]\n",
      "3582 [Loss: 0.120536]\n",
      "3583 [Loss: 0.131999]\n",
      "3584 [Loss: 0.131586]\n",
      "3585 [Loss: 0.129886]\n",
      "3586 [Loss: 0.131176]\n",
      "3587 [Loss: 0.127900]\n",
      "3588 [Loss: 0.137356]\n",
      "3589 [Loss: 0.119957]\n",
      "3590 [Loss: 0.125319]\n",
      "3591 [Loss: 0.138435]\n",
      "3592 [Loss: 0.131434]\n",
      "3593 [Loss: 0.127267]\n",
      "3594 [Loss: 0.135104]\n",
      "3595 [Loss: 0.131881]\n",
      "3596 [Loss: 0.132690]\n",
      "3597 [Loss: 0.131072]\n",
      "3598 [Loss: 0.126317]\n",
      "3599 [Loss: 0.132729]\n",
      "3600 [Loss: 0.128083]\n",
      "3601 [Loss: 0.137309]\n",
      "3602 [Loss: 0.134784]\n",
      "3603 [Loss: 0.128992]\n",
      "3604 [Loss: 0.138916]\n",
      "3605 [Loss: 0.132961]\n",
      "3606 [Loss: 0.127692]\n",
      "3607 [Loss: 0.130254]\n",
      "3608 [Loss: 0.127557]\n",
      "3609 [Loss: 0.124368]\n",
      "3610 [Loss: 0.130922]\n",
      "3611 [Loss: 0.135086]\n",
      "3612 [Loss: 0.138735]\n",
      "3613 [Loss: 0.129712]\n",
      "3614 [Loss: 0.120691]\n",
      "3615 [Loss: 0.136649]\n",
      "3616 [Loss: 0.137385]\n",
      "3617 [Loss: 0.137437]\n",
      "3618 [Loss: 0.136733]\n",
      "3619 [Loss: 0.132493]\n",
      "3620 [Loss: 0.127407]\n",
      "3621 [Loss: 0.133880]\n",
      "3622 [Loss: 0.118438]\n",
      "3623 [Loss: 0.138279]\n",
      "3624 [Loss: 0.127935]\n",
      "3625 [Loss: 0.142860]\n",
      "3626 [Loss: 0.135147]\n",
      "3627 [Loss: 0.146754]\n",
      "3628 [Loss: 0.132354]\n",
      "3629 [Loss: 0.132252]\n",
      "3630 [Loss: 0.133992]\n",
      "3631 [Loss: 0.135096]\n",
      "3632 [Loss: 0.134530]\n",
      "3633 [Loss: 0.134332]\n",
      "3634 [Loss: 0.131474]\n",
      "3635 [Loss: 0.127450]\n",
      "3636 [Loss: 0.130017]\n",
      "3637 [Loss: 0.122445]\n",
      "3638 [Loss: 0.129179]\n",
      "3639 [Loss: 0.132740]\n",
      "3640 [Loss: 0.124968]\n",
      "3641 [Loss: 0.136939]\n",
      "3642 [Loss: 0.133578]\n",
      "3643 [Loss: 0.131739]\n",
      "3644 [Loss: 0.130902]\n",
      "3645 [Loss: 0.126505]\n",
      "3646 [Loss: 0.123237]\n",
      "3647 [Loss: 0.127517]\n",
      "3648 [Loss: 0.128635]\n",
      "3649 [Loss: 0.129457]\n",
      "3650 [Loss: 0.128265]\n",
      "3651 [Loss: 0.125255]\n",
      "3652 [Loss: 0.133343]\n",
      "3653 [Loss: 0.134526]\n",
      "3654 [Loss: 0.122742]\n",
      "3655 [Loss: 0.129667]\n",
      "3656 [Loss: 0.137157]\n",
      "3657 [Loss: 0.132365]\n",
      "3658 [Loss: 0.129690]\n",
      "3659 [Loss: 0.136438]\n",
      "3660 [Loss: 0.142191]\n",
      "3661 [Loss: 0.129609]\n",
      "3662 [Loss: 0.132639]\n",
      "3663 [Loss: 0.131472]\n",
      "3664 [Loss: 0.125670]\n",
      "3665 [Loss: 0.128764]\n",
      "3666 [Loss: 0.124583]\n",
      "3667 [Loss: 0.131974]\n",
      "3668 [Loss: 0.131396]\n",
      "3669 [Loss: 0.137373]\n",
      "3670 [Loss: 0.126183]\n",
      "3671 [Loss: 0.124901]\n",
      "3672 [Loss: 0.126721]\n",
      "3673 [Loss: 0.130542]\n",
      "3674 [Loss: 0.125177]\n",
      "3675 [Loss: 0.134112]\n",
      "3676 [Loss: 0.129133]\n",
      "3677 [Loss: 0.132072]\n",
      "3678 [Loss: 0.124739]\n",
      "3679 [Loss: 0.125101]\n",
      "3680 [Loss: 0.129475]\n",
      "3681 [Loss: 0.137386]\n",
      "3682 [Loss: 0.130705]\n",
      "3683 [Loss: 0.133827]\n",
      "3684 [Loss: 0.126338]\n",
      "3685 [Loss: 0.133186]\n",
      "3686 [Loss: 0.129709]\n",
      "3687 [Loss: 0.130381]\n",
      "3688 [Loss: 0.124266]\n",
      "3689 [Loss: 0.132449]\n",
      "3690 [Loss: 0.131459]\n",
      "3691 [Loss: 0.133075]\n",
      "3692 [Loss: 0.128887]\n",
      "3693 [Loss: 0.128104]\n",
      "3694 [Loss: 0.129864]\n",
      "3695 [Loss: 0.143632]\n",
      "3696 [Loss: 0.135308]\n",
      "3697 [Loss: 0.124287]\n",
      "3698 [Loss: 0.133948]\n",
      "3699 [Loss: 0.129783]\n",
      "3700 [Loss: 0.130144]\n",
      "3701 [Loss: 0.133211]\n",
      "3702 [Loss: 0.135307]\n",
      "3703 [Loss: 0.122327]\n",
      "3704 [Loss: 0.129697]\n",
      "3705 [Loss: 0.126357]\n",
      "3706 [Loss: 0.134105]\n",
      "3707 [Loss: 0.130776]\n",
      "3708 [Loss: 0.129800]\n",
      "3709 [Loss: 0.124186]\n",
      "3710 [Loss: 0.125086]\n",
      "3711 [Loss: 0.135592]\n",
      "3712 [Loss: 0.124700]\n",
      "3713 [Loss: 0.133933]\n",
      "3714 [Loss: 0.124385]\n",
      "3715 [Loss: 0.129492]\n",
      "3716 [Loss: 0.126112]\n",
      "3717 [Loss: 0.132398]\n",
      "3718 [Loss: 0.124490]\n",
      "3719 [Loss: 0.126248]\n",
      "3720 [Loss: 0.122333]\n",
      "3721 [Loss: 0.127843]\n",
      "3722 [Loss: 0.134420]\n",
      "3723 [Loss: 0.128299]\n",
      "3724 [Loss: 0.127956]\n",
      "3725 [Loss: 0.129534]\n",
      "3726 [Loss: 0.129375]\n",
      "3727 [Loss: 0.136809]\n",
      "3728 [Loss: 0.124529]\n",
      "3729 [Loss: 0.130900]\n",
      "3730 [Loss: 0.129944]\n",
      "3731 [Loss: 0.124013]\n",
      "3732 [Loss: 0.131465]\n",
      "3733 [Loss: 0.122052]\n",
      "3734 [Loss: 0.123174]\n",
      "3735 [Loss: 0.125786]\n",
      "3736 [Loss: 0.129579]\n",
      "3737 [Loss: 0.134172]\n",
      "3738 [Loss: 0.127402]\n",
      "3739 [Loss: 0.124827]\n",
      "3740 [Loss: 0.126701]\n",
      "3741 [Loss: 0.125697]\n",
      "3742 [Loss: 0.138589]\n",
      "3743 [Loss: 0.132606]\n",
      "3744 [Loss: 0.130320]\n",
      "3745 [Loss: 0.136621]\n",
      "3746 [Loss: 0.123243]\n",
      "3747 [Loss: 0.133529]\n",
      "3748 [Loss: 0.134292]\n",
      "3749 [Loss: 0.134195]\n",
      "3750 [Loss: 0.131665]\n",
      "3751 [Loss: 0.125029]\n",
      "3752 [Loss: 0.135709]\n",
      "3753 [Loss: 0.133890]\n",
      "3754 [Loss: 0.124004]\n",
      "3755 [Loss: 0.124027]\n",
      "3756 [Loss: 0.126425]\n",
      "3757 [Loss: 0.129527]\n",
      "3758 [Loss: 0.132601]\n",
      "3759 [Loss: 0.128690]\n",
      "3760 [Loss: 0.127149]\n",
      "3761 [Loss: 0.124004]\n",
      "3762 [Loss: 0.135637]\n",
      "3763 [Loss: 0.124042]\n",
      "3764 [Loss: 0.127174]\n",
      "3765 [Loss: 0.136639]\n",
      "3766 [Loss: 0.125090]\n",
      "3767 [Loss: 0.135807]\n",
      "3768 [Loss: 0.126011]\n",
      "3769 [Loss: 0.139157]\n",
      "3770 [Loss: 0.137543]\n",
      "3771 [Loss: 0.134084]\n",
      "3772 [Loss: 0.131416]\n",
      "3773 [Loss: 0.133150]\n",
      "3774 [Loss: 0.131805]\n",
      "3775 [Loss: 0.128824]\n",
      "3776 [Loss: 0.126708]\n",
      "3777 [Loss: 0.124661]\n",
      "3778 [Loss: 0.125788]\n",
      "3779 [Loss: 0.129257]\n",
      "3780 [Loss: 0.126424]\n",
      "3781 [Loss: 0.125028]\n",
      "3782 [Loss: 0.126565]\n",
      "3783 [Loss: 0.123889]\n",
      "3784 [Loss: 0.116831]\n",
      "3785 [Loss: 0.124304]\n",
      "3786 [Loss: 0.127922]\n",
      "3787 [Loss: 0.131154]\n",
      "3788 [Loss: 0.127993]\n",
      "3789 [Loss: 0.122446]\n",
      "3790 [Loss: 0.131244]\n",
      "3791 [Loss: 0.132393]\n",
      "3792 [Loss: 0.131490]\n",
      "3793 [Loss: 0.133835]\n",
      "3794 [Loss: 0.133501]\n",
      "3795 [Loss: 0.132231]\n",
      "3796 [Loss: 0.123793]\n",
      "3797 [Loss: 0.126577]\n",
      "3798 [Loss: 0.123092]\n",
      "3799 [Loss: 0.131035]\n",
      "3800 [Loss: 0.137939]\n",
      "3801 [Loss: 0.130159]\n",
      "3802 [Loss: 0.134961]\n",
      "3803 [Loss: 0.128122]\n",
      "3804 [Loss: 0.134120]\n",
      "3805 [Loss: 0.126173]\n",
      "3806 [Loss: 0.129693]\n",
      "3807 [Loss: 0.125618]\n",
      "3808 [Loss: 0.134926]\n",
      "3809 [Loss: 0.130951]\n",
      "3810 [Loss: 0.131386]\n",
      "3811 [Loss: 0.122667]\n",
      "3812 [Loss: 0.120269]\n",
      "3813 [Loss: 0.131595]\n",
      "3814 [Loss: 0.131853]\n",
      "3815 [Loss: 0.128679]\n",
      "3816 [Loss: 0.133133]\n",
      "3817 [Loss: 0.123627]\n",
      "3818 [Loss: 0.128825]\n",
      "3819 [Loss: 0.130242]\n",
      "3820 [Loss: 0.135781]\n",
      "3821 [Loss: 0.133832]\n",
      "3822 [Loss: 0.130203]\n",
      "3823 [Loss: 0.124068]\n",
      "3824 [Loss: 0.123740]\n",
      "3825 [Loss: 0.124705]\n",
      "3826 [Loss: 0.136690]\n",
      "3827 [Loss: 0.129447]\n",
      "3828 [Loss: 0.121711]\n",
      "3829 [Loss: 0.128101]\n",
      "3830 [Loss: 0.125627]\n",
      "3831 [Loss: 0.130094]\n",
      "3832 [Loss: 0.128056]\n",
      "3833 [Loss: 0.127174]\n",
      "3834 [Loss: 0.119740]\n",
      "3835 [Loss: 0.125490]\n",
      "3836 [Loss: 0.128745]\n",
      "3837 [Loss: 0.125957]\n",
      "3838 [Loss: 0.131341]\n",
      "3839 [Loss: 0.132003]\n",
      "3840 [Loss: 0.131506]\n",
      "3841 [Loss: 0.127696]\n",
      "3842 [Loss: 0.125028]\n",
      "3843 [Loss: 0.131950]\n",
      "3844 [Loss: 0.124834]\n",
      "3845 [Loss: 0.127677]\n",
      "3846 [Loss: 0.130800]\n",
      "3847 [Loss: 0.124779]\n",
      "3848 [Loss: 0.123601]\n",
      "3849 [Loss: 0.129343]\n",
      "3850 [Loss: 0.123676]\n",
      "3851 [Loss: 0.131690]\n",
      "3852 [Loss: 0.133192]\n",
      "3853 [Loss: 0.132591]\n",
      "3854 [Loss: 0.128064]\n",
      "3855 [Loss: 0.123372]\n",
      "3856 [Loss: 0.135155]\n",
      "3857 [Loss: 0.126536]\n",
      "3858 [Loss: 0.121686]\n",
      "3859 [Loss: 0.123681]\n",
      "3860 [Loss: 0.130990]\n",
      "3861 [Loss: 0.135296]\n",
      "3862 [Loss: 0.131908]\n",
      "3863 [Loss: 0.125526]\n",
      "3864 [Loss: 0.120632]\n",
      "3865 [Loss: 0.132359]\n",
      "3866 [Loss: 0.129861]\n",
      "3867 [Loss: 0.123914]\n",
      "3868 [Loss: 0.125507]\n",
      "3869 [Loss: 0.130616]\n",
      "3870 [Loss: 0.131585]\n",
      "3871 [Loss: 0.127080]\n",
      "3872 [Loss: 0.120176]\n",
      "3873 [Loss: 0.129924]\n",
      "3874 [Loss: 0.130333]\n",
      "3875 [Loss: 0.130648]\n",
      "3876 [Loss: 0.123296]\n",
      "3877 [Loss: 0.130763]\n",
      "3878 [Loss: 0.130116]\n",
      "3879 [Loss: 0.130674]\n",
      "3880 [Loss: 0.127996]\n",
      "3881 [Loss: 0.118217]\n",
      "3882 [Loss: 0.124100]\n",
      "3883 [Loss: 0.121454]\n",
      "3884 [Loss: 0.117556]\n",
      "3885 [Loss: 0.127211]\n",
      "3886 [Loss: 0.128415]\n",
      "3887 [Loss: 0.118940]\n",
      "3888 [Loss: 0.119323]\n",
      "3889 [Loss: 0.124846]\n",
      "3890 [Loss: 0.129080]\n",
      "3891 [Loss: 0.125306]\n",
      "3892 [Loss: 0.134750]\n",
      "3893 [Loss: 0.134030]\n",
      "3894 [Loss: 0.126375]\n",
      "3895 [Loss: 0.127064]\n",
      "3896 [Loss: 0.117082]\n",
      "3897 [Loss: 0.133457]\n",
      "3898 [Loss: 0.123723]\n",
      "3899 [Loss: 0.125075]\n",
      "3900 [Loss: 0.126229]\n",
      "3901 [Loss: 0.134157]\n",
      "3902 [Loss: 0.122206]\n",
      "3903 [Loss: 0.135721]\n",
      "3904 [Loss: 0.129257]\n",
      "3905 [Loss: 0.136800]\n",
      "3906 [Loss: 0.134164]\n",
      "3907 [Loss: 0.130467]\n",
      "3908 [Loss: 0.125889]\n",
      "3909 [Loss: 0.128282]\n",
      "3910 [Loss: 0.124677]\n",
      "3911 [Loss: 0.131575]\n",
      "3912 [Loss: 0.133014]\n",
      "3913 [Loss: 0.121475]\n",
      "3914 [Loss: 0.126771]\n",
      "3915 [Loss: 0.127831]\n",
      "3916 [Loss: 0.123294]\n",
      "3917 [Loss: 0.129968]\n",
      "3918 [Loss: 0.121988]\n",
      "3919 [Loss: 0.123125]\n",
      "3920 [Loss: 0.126980]\n",
      "3921 [Loss: 0.125638]\n",
      "3922 [Loss: 0.121862]\n",
      "3923 [Loss: 0.127911]\n",
      "3924 [Loss: 0.127929]\n",
      "3925 [Loss: 0.127353]\n",
      "3926 [Loss: 0.132966]\n",
      "3927 [Loss: 0.130937]\n",
      "3928 [Loss: 0.125126]\n",
      "3929 [Loss: 0.115060]\n",
      "3930 [Loss: 0.124948]\n",
      "3931 [Loss: 0.125639]\n",
      "3932 [Loss: 0.129878]\n",
      "3933 [Loss: 0.123125]\n",
      "3934 [Loss: 0.126679]\n",
      "3935 [Loss: 0.125100]\n",
      "3936 [Loss: 0.116797]\n",
      "3937 [Loss: 0.127418]\n",
      "3938 [Loss: 0.127798]\n",
      "3939 [Loss: 0.127961]\n",
      "3940 [Loss: 0.128455]\n",
      "3941 [Loss: 0.128489]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3942 [Loss: 0.123005]\n",
      "3943 [Loss: 0.132326]\n",
      "3944 [Loss: 0.125334]\n",
      "3945 [Loss: 0.126757]\n",
      "3946 [Loss: 0.121525]\n",
      "3947 [Loss: 0.123608]\n",
      "3948 [Loss: 0.120434]\n",
      "3949 [Loss: 0.125766]\n",
      "3950 [Loss: 0.123376]\n",
      "3951 [Loss: 0.126823]\n",
      "3952 [Loss: 0.124800]\n",
      "3953 [Loss: 0.128330]\n",
      "3954 [Loss: 0.126240]\n",
      "3955 [Loss: 0.126370]\n",
      "3956 [Loss: 0.125545]\n",
      "3957 [Loss: 0.131129]\n",
      "3958 [Loss: 0.128683]\n",
      "3959 [Loss: 0.119728]\n",
      "3960 [Loss: 0.126637]\n",
      "3961 [Loss: 0.122972]\n",
      "3962 [Loss: 0.129982]\n",
      "3963 [Loss: 0.130997]\n",
      "3964 [Loss: 0.119801]\n",
      "3965 [Loss: 0.119945]\n",
      "3966 [Loss: 0.131152]\n",
      "3967 [Loss: 0.123248]\n",
      "3968 [Loss: 0.130316]\n",
      "3969 [Loss: 0.133755]\n",
      "3970 [Loss: 0.130432]\n",
      "3971 [Loss: 0.134203]\n",
      "3972 [Loss: 0.119818]\n",
      "3973 [Loss: 0.134447]\n",
      "3974 [Loss: 0.131943]\n",
      "3975 [Loss: 0.123959]\n",
      "3976 [Loss: 0.124515]\n",
      "3977 [Loss: 0.129908]\n",
      "3978 [Loss: 0.124552]\n",
      "3979 [Loss: 0.134849]\n",
      "3980 [Loss: 0.125867]\n",
      "3981 [Loss: 0.125361]\n",
      "3982 [Loss: 0.125960]\n",
      "3983 [Loss: 0.122361]\n",
      "3984 [Loss: 0.128221]\n",
      "3985 [Loss: 0.131726]\n",
      "3986 [Loss: 0.132263]\n",
      "3987 [Loss: 0.118138]\n",
      "3988 [Loss: 0.123999]\n",
      "3989 [Loss: 0.129167]\n",
      "3990 [Loss: 0.125625]\n",
      "3991 [Loss: 0.132213]\n",
      "3992 [Loss: 0.122040]\n",
      "3993 [Loss: 0.129006]\n",
      "3994 [Loss: 0.127705]\n",
      "3995 [Loss: 0.126206]\n",
      "3996 [Loss: 0.119132]\n",
      "3997 [Loss: 0.127767]\n",
      "3998 [Loss: 0.119816]\n",
      "3999 [Loss: 0.125553]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "\t#create the output image directory\n",
    "\tif (os.path.isdir('images')==0):\n",
    "\t\tos.mkdir('images')\n",
    "\n",
    "\t#choose dataset\n",
    "\tdataset_name = 'mnist'#\n",
    "\n",
    "\t#create AE model\n",
    "\tarchitecture = 'mlp'#'convolutional'#\n",
    "\tae_mlp = autoencoder(dataset_name,architecture)#,\n",
    "\n",
    "\tae_mlp.train(epochs=ae_mlp.epochs, batch_size=64, sample_interval=100)\n",
    "\tplt.plot(ae_mlp.error_list[30:])\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        (None, 28, 28, 1)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 14, 14, 8)         80        \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)   (None, 14, 14, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 7, 7, 4)           292       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)   (None, 7, 7, 4)           0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 100)               19700     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 196)               19796     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)   (None, 196)               0         \n",
      "_________________________________________________________________\n",
      "reshape_18 (Reshape)         (None, 7, 7, 4)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_17 (Conv2DT (None, 14, 14, 4)         148       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)   (None, 14, 14, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_18 (Conv2DT (None, 28, 28, 1)         37        \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 40,053\n",
      "Trainable params: 40,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "0 [Loss: 0.691376]\n",
      "1 [Loss: 0.690508]\n",
      "2 [Loss: 0.689662]\n",
      "3 [Loss: 0.688710]\n",
      "4 [Loss: 0.687930]\n",
      "5 [Loss: 0.686854]\n",
      "6 [Loss: 0.686032]\n",
      "7 [Loss: 0.684956]\n",
      "8 [Loss: 0.684003]\n",
      "9 [Loss: 0.683042]\n",
      "10 [Loss: 0.681873]\n",
      "11 [Loss: 0.680392]\n",
      "12 [Loss: 0.679173]\n",
      "13 [Loss: 0.677882]\n",
      "14 [Loss: 0.676563]\n",
      "15 [Loss: 0.674753]\n",
      "16 [Loss: 0.673063]\n",
      "17 [Loss: 0.671176]\n",
      "18 [Loss: 0.668939]\n",
      "19 [Loss: 0.666658]\n",
      "20 [Loss: 0.664381]\n",
      "21 [Loss: 0.661734]\n",
      "22 [Loss: 0.658379]\n",
      "23 [Loss: 0.654626]\n",
      "24 [Loss: 0.649934]\n",
      "25 [Loss: 0.644662]\n",
      "26 [Loss: 0.638160]\n",
      "27 [Loss: 0.630402]\n",
      "28 [Loss: 0.621035]\n",
      "29 [Loss: 0.607527]\n",
      "30 [Loss: 0.589526]\n",
      "31 [Loss: 0.570574]\n",
      "32 [Loss: 0.542616]\n",
      "33 [Loss: 0.513888]\n",
      "34 [Loss: 0.483053]\n",
      "35 [Loss: 0.456218]\n",
      "36 [Loss: 0.418890]\n",
      "37 [Loss: 0.409986]\n",
      "38 [Loss: 0.388192]\n",
      "39 [Loss: 0.363436]\n",
      "40 [Loss: 0.344513]\n",
      "41 [Loss: 0.342187]\n",
      "42 [Loss: 0.350198]\n",
      "43 [Loss: 0.334766]\n",
      "44 [Loss: 0.346416]\n",
      "45 [Loss: 0.314272]\n",
      "46 [Loss: 0.310574]\n",
      "47 [Loss: 0.301060]\n",
      "48 [Loss: 0.297314]\n",
      "49 [Loss: 0.306444]\n",
      "50 [Loss: 0.313156]\n",
      "51 [Loss: 0.313376]\n",
      "52 [Loss: 0.297808]\n",
      "53 [Loss: 0.285858]\n",
      "54 [Loss: 0.295025]\n",
      "55 [Loss: 0.298987]\n",
      "56 [Loss: 0.278670]\n",
      "57 [Loss: 0.298091]\n",
      "58 [Loss: 0.272301]\n",
      "59 [Loss: 0.286951]\n",
      "60 [Loss: 0.283406]\n",
      "61 [Loss: 0.282157]\n",
      "62 [Loss: 0.289099]\n",
      "63 [Loss: 0.276196]\n",
      "64 [Loss: 0.270386]\n",
      "65 [Loss: 0.279965]\n",
      "66 [Loss: 0.287325]\n",
      "67 [Loss: 0.269478]\n",
      "68 [Loss: 0.298335]\n",
      "69 [Loss: 0.280032]\n",
      "70 [Loss: 0.278455]\n",
      "71 [Loss: 0.277770]\n",
      "72 [Loss: 0.275070]\n",
      "73 [Loss: 0.275631]\n",
      "74 [Loss: 0.276420]\n",
      "75 [Loss: 0.268510]\n",
      "76 [Loss: 0.256510]\n",
      "77 [Loss: 0.270435]\n",
      "78 [Loss: 0.256765]\n",
      "79 [Loss: 0.255553]\n",
      "80 [Loss: 0.259581]\n",
      "81 [Loss: 0.271436]\n",
      "82 [Loss: 0.265019]\n",
      "83 [Loss: 0.260002]\n",
      "84 [Loss: 0.259652]\n",
      "85 [Loss: 0.252391]\n",
      "86 [Loss: 0.256512]\n",
      "87 [Loss: 0.253337]\n",
      "88 [Loss: 0.263152]\n",
      "89 [Loss: 0.262709]\n",
      "90 [Loss: 0.256488]\n",
      "91 [Loss: 0.254870]\n",
      "92 [Loss: 0.258604]\n",
      "93 [Loss: 0.254118]\n",
      "94 [Loss: 0.252920]\n",
      "95 [Loss: 0.260220]\n",
      "96 [Loss: 0.256755]\n",
      "97 [Loss: 0.242096]\n",
      "98 [Loss: 0.257233]\n",
      "99 [Loss: 0.251572]\n",
      "100 [Loss: 0.253116]\n",
      "101 [Loss: 0.247938]\n",
      "102 [Loss: 0.254825]\n",
      "103 [Loss: 0.256014]\n",
      "104 [Loss: 0.246993]\n",
      "105 [Loss: 0.240366]\n",
      "106 [Loss: 0.245419]\n",
      "107 [Loss: 0.249586]\n",
      "108 [Loss: 0.246566]\n",
      "109 [Loss: 0.235776]\n",
      "110 [Loss: 0.245720]\n",
      "111 [Loss: 0.235800]\n",
      "112 [Loss: 0.241622]\n",
      "113 [Loss: 0.240755]\n",
      "114 [Loss: 0.238608]\n",
      "115 [Loss: 0.246165]\n",
      "116 [Loss: 0.237405]\n",
      "117 [Loss: 0.251303]\n",
      "118 [Loss: 0.236274]\n",
      "119 [Loss: 0.237888]\n",
      "120 [Loss: 0.235224]\n",
      "121 [Loss: 0.229075]\n",
      "122 [Loss: 0.241255]\n",
      "123 [Loss: 0.222369]\n",
      "124 [Loss: 0.240412]\n",
      "125 [Loss: 0.237946]\n",
      "126 [Loss: 0.229975]\n",
      "127 [Loss: 0.237588]\n",
      "128 [Loss: 0.238886]\n",
      "129 [Loss: 0.228832]\n",
      "130 [Loss: 0.244721]\n",
      "131 [Loss: 0.237496]\n",
      "132 [Loss: 0.235321]\n",
      "133 [Loss: 0.242955]\n",
      "134 [Loss: 0.222101]\n",
      "135 [Loss: 0.233259]\n",
      "136 [Loss: 0.227074]\n",
      "137 [Loss: 0.220539]\n",
      "138 [Loss: 0.236046]\n",
      "139 [Loss: 0.230954]\n",
      "140 [Loss: 0.234809]\n",
      "141 [Loss: 0.220306]\n",
      "142 [Loss: 0.226630]\n",
      "143 [Loss: 0.222769]\n",
      "144 [Loss: 0.211025]\n",
      "145 [Loss: 0.228544]\n",
      "146 [Loss: 0.225848]\n",
      "147 [Loss: 0.220891]\n",
      "148 [Loss: 0.225677]\n",
      "149 [Loss: 0.229416]\n",
      "150 [Loss: 0.214921]\n",
      "151 [Loss: 0.230893]\n",
      "152 [Loss: 0.215641]\n",
      "153 [Loss: 0.244110]\n",
      "154 [Loss: 0.231026]\n",
      "155 [Loss: 0.229572]\n",
      "156 [Loss: 0.226876]\n",
      "157 [Loss: 0.216822]\n",
      "158 [Loss: 0.217244]\n",
      "159 [Loss: 0.213123]\n",
      "160 [Loss: 0.219522]\n",
      "161 [Loss: 0.226342]\n",
      "162 [Loss: 0.217713]\n",
      "163 [Loss: 0.211413]\n",
      "164 [Loss: 0.216125]\n",
      "165 [Loss: 0.220384]\n",
      "166 [Loss: 0.214443]\n",
      "167 [Loss: 0.212131]\n",
      "168 [Loss: 0.212053]\n",
      "169 [Loss: 0.209453]\n",
      "170 [Loss: 0.209529]\n",
      "171 [Loss: 0.210055]\n",
      "172 [Loss: 0.209669]\n",
      "173 [Loss: 0.206043]\n",
      "174 [Loss: 0.212648]\n",
      "175 [Loss: 0.209314]\n",
      "176 [Loss: 0.213014]\n",
      "177 [Loss: 0.205140]\n",
      "178 [Loss: 0.209306]\n",
      "179 [Loss: 0.207328]\n",
      "180 [Loss: 0.198007]\n",
      "181 [Loss: 0.210039]\n",
      "182 [Loss: 0.210136]\n",
      "183 [Loss: 0.199005]\n",
      "184 [Loss: 0.198289]\n",
      "185 [Loss: 0.210558]\n",
      "186 [Loss: 0.211300]\n",
      "187 [Loss: 0.209521]\n",
      "188 [Loss: 0.202981]\n",
      "189 [Loss: 0.197310]\n",
      "190 [Loss: 0.198983]\n",
      "191 [Loss: 0.195344]\n",
      "192 [Loss: 0.193078]\n",
      "193 [Loss: 0.200239]\n",
      "194 [Loss: 0.197747]\n",
      "195 [Loss: 0.196588]\n",
      "196 [Loss: 0.201698]\n",
      "197 [Loss: 0.192585]\n",
      "198 [Loss: 0.192878]\n",
      "199 [Loss: 0.192537]\n",
      "200 [Loss: 0.190590]\n",
      "201 [Loss: 0.197903]\n",
      "202 [Loss: 0.197845]\n",
      "203 [Loss: 0.202431]\n",
      "204 [Loss: 0.200308]\n",
      "205 [Loss: 0.195330]\n",
      "206 [Loss: 0.180306]\n",
      "207 [Loss: 0.194651]\n",
      "208 [Loss: 0.191581]\n",
      "209 [Loss: 0.189630]\n",
      "210 [Loss: 0.197103]\n",
      "211 [Loss: 0.192705]\n",
      "212 [Loss: 0.201892]\n",
      "213 [Loss: 0.186543]\n",
      "214 [Loss: 0.198121]\n",
      "215 [Loss: 0.183588]\n",
      "216 [Loss: 0.204427]\n",
      "217 [Loss: 0.197769]\n",
      "218 [Loss: 0.184327]\n",
      "219 [Loss: 0.188442]\n",
      "220 [Loss: 0.176715]\n",
      "221 [Loss: 0.189732]\n",
      "222 [Loss: 0.187317]\n",
      "223 [Loss: 0.187653]\n",
      "224 [Loss: 0.187170]\n",
      "225 [Loss: 0.193208]\n",
      "226 [Loss: 0.177105]\n",
      "227 [Loss: 0.199059]\n",
      "228 [Loss: 0.186407]\n",
      "229 [Loss: 0.188232]\n",
      "230 [Loss: 0.185385]\n",
      "231 [Loss: 0.182489]\n",
      "232 [Loss: 0.176798]\n",
      "233 [Loss: 0.179431]\n",
      "234 [Loss: 0.191185]\n",
      "235 [Loss: 0.177474]\n",
      "236 [Loss: 0.183721]\n",
      "237 [Loss: 0.187863]\n",
      "238 [Loss: 0.175868]\n",
      "239 [Loss: 0.168765]\n",
      "240 [Loss: 0.183944]\n",
      "241 [Loss: 0.168484]\n",
      "242 [Loss: 0.178361]\n",
      "243 [Loss: 0.187899]\n",
      "244 [Loss: 0.178863]\n",
      "245 [Loss: 0.190309]\n",
      "246 [Loss: 0.172447]\n",
      "247 [Loss: 0.181164]\n",
      "248 [Loss: 0.177366]\n",
      "249 [Loss: 0.182474]\n",
      "250 [Loss: 0.185264]\n",
      "251 [Loss: 0.179511]\n",
      "252 [Loss: 0.176502]\n",
      "253 [Loss: 0.178374]\n",
      "254 [Loss: 0.165425]\n",
      "255 [Loss: 0.183115]\n",
      "256 [Loss: 0.170126]\n",
      "257 [Loss: 0.177002]\n",
      "258 [Loss: 0.182396]\n",
      "259 [Loss: 0.175144]\n",
      "260 [Loss: 0.172229]\n",
      "261 [Loss: 0.186592]\n",
      "262 [Loss: 0.180912]\n",
      "263 [Loss: 0.178322]\n",
      "264 [Loss: 0.184054]\n",
      "265 [Loss: 0.178152]\n",
      "266 [Loss: 0.179627]\n",
      "267 [Loss: 0.170793]\n",
      "268 [Loss: 0.173627]\n",
      "269 [Loss: 0.161973]\n",
      "270 [Loss: 0.171181]\n",
      "271 [Loss: 0.179425]\n",
      "272 [Loss: 0.180895]\n",
      "273 [Loss: 0.172991]\n",
      "274 [Loss: 0.179036]\n",
      "275 [Loss: 0.191690]\n",
      "276 [Loss: 0.196671]\n",
      "277 [Loss: 0.188406]\n",
      "278 [Loss: 0.176039]\n",
      "279 [Loss: 0.187337]\n",
      "280 [Loss: 0.171927]\n",
      "281 [Loss: 0.177523]\n",
      "282 [Loss: 0.167146]\n",
      "283 [Loss: 0.173296]\n",
      "284 [Loss: 0.168102]\n",
      "285 [Loss: 0.162819]\n",
      "286 [Loss: 0.163565]\n",
      "287 [Loss: 0.169158]\n",
      "288 [Loss: 0.160041]\n",
      "289 [Loss: 0.163200]\n",
      "290 [Loss: 0.170374]\n",
      "291 [Loss: 0.174080]\n",
      "292 [Loss: 0.166480]\n",
      "293 [Loss: 0.168878]\n",
      "294 [Loss: 0.174520]\n",
      "295 [Loss: 0.159096]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 [Loss: 0.175762]\n",
      "297 [Loss: 0.175761]\n",
      "298 [Loss: 0.177841]\n",
      "299 [Loss: 0.172588]\n",
      "300 [Loss: 0.172218]\n",
      "301 [Loss: 0.151857]\n",
      "302 [Loss: 0.171654]\n",
      "303 [Loss: 0.167670]\n",
      "304 [Loss: 0.177306]\n",
      "305 [Loss: 0.166491]\n",
      "306 [Loss: 0.164377]\n",
      "307 [Loss: 0.173612]\n",
      "308 [Loss: 0.187555]\n",
      "309 [Loss: 0.175176]\n",
      "310 [Loss: 0.164301]\n",
      "311 [Loss: 0.169884]\n",
      "312 [Loss: 0.158645]\n",
      "313 [Loss: 0.173720]\n",
      "314 [Loss: 0.163740]\n",
      "315 [Loss: 0.161269]\n",
      "316 [Loss: 0.153046]\n",
      "317 [Loss: 0.164683]\n",
      "318 [Loss: 0.152851]\n",
      "319 [Loss: 0.160881]\n",
      "320 [Loss: 0.169923]\n",
      "321 [Loss: 0.166568]\n",
      "322 [Loss: 0.162644]\n",
      "323 [Loss: 0.170061]\n",
      "324 [Loss: 0.169789]\n",
      "325 [Loss: 0.163346]\n",
      "326 [Loss: 0.156496]\n",
      "327 [Loss: 0.161077]\n",
      "328 [Loss: 0.163365]\n",
      "329 [Loss: 0.174164]\n",
      "330 [Loss: 0.158823]\n",
      "331 [Loss: 0.163772]\n",
      "332 [Loss: 0.163603]\n",
      "333 [Loss: 0.152751]\n",
      "334 [Loss: 0.157010]\n",
      "335 [Loss: 0.164553]\n",
      "336 [Loss: 0.164762]\n",
      "337 [Loss: 0.164827]\n",
      "338 [Loss: 0.171668]\n",
      "339 [Loss: 0.156742]\n",
      "340 [Loss: 0.162813]\n",
      "341 [Loss: 0.163983]\n",
      "342 [Loss: 0.161460]\n",
      "343 [Loss: 0.157116]\n",
      "344 [Loss: 0.155688]\n",
      "345 [Loss: 0.166451]\n",
      "346 [Loss: 0.160643]\n",
      "347 [Loss: 0.157074]\n",
      "348 [Loss: 0.161156]\n",
      "349 [Loss: 0.162255]\n",
      "350 [Loss: 0.149065]\n",
      "351 [Loss: 0.153397]\n",
      "352 [Loss: 0.153489]\n",
      "353 [Loss: 0.153031]\n",
      "354 [Loss: 0.156653]\n",
      "355 [Loss: 0.151882]\n",
      "356 [Loss: 0.151656]\n",
      "357 [Loss: 0.155237]\n",
      "358 [Loss: 0.155169]\n",
      "359 [Loss: 0.154482]\n",
      "360 [Loss: 0.148975]\n",
      "361 [Loss: 0.162527]\n",
      "362 [Loss: 0.158880]\n",
      "363 [Loss: 0.171473]\n",
      "364 [Loss: 0.164399]\n",
      "365 [Loss: 0.169947]\n",
      "366 [Loss: 0.162013]\n",
      "367 [Loss: 0.168273]\n",
      "368 [Loss: 0.166606]\n",
      "369 [Loss: 0.162534]\n",
      "370 [Loss: 0.153156]\n",
      "371 [Loss: 0.157136]\n",
      "372 [Loss: 0.152254]\n",
      "373 [Loss: 0.155729]\n",
      "374 [Loss: 0.154976]\n",
      "375 [Loss: 0.144080]\n",
      "376 [Loss: 0.149832]\n",
      "377 [Loss: 0.146861]\n",
      "378 [Loss: 0.144930]\n",
      "379 [Loss: 0.151400]\n",
      "380 [Loss: 0.140737]\n",
      "381 [Loss: 0.143765]\n",
      "382 [Loss: 0.158314]\n",
      "383 [Loss: 0.161032]\n",
      "384 [Loss: 0.155532]\n",
      "385 [Loss: 0.144526]\n",
      "386 [Loss: 0.150491]\n",
      "387 [Loss: 0.153290]\n",
      "388 [Loss: 0.153323]\n",
      "389 [Loss: 0.157527]\n",
      "390 [Loss: 0.148859]\n",
      "391 [Loss: 0.142597]\n",
      "392 [Loss: 0.141187]\n",
      "393 [Loss: 0.157893]\n",
      "394 [Loss: 0.152341]\n",
      "395 [Loss: 0.147161]\n",
      "396 [Loss: 0.146343]\n",
      "397 [Loss: 0.167936]\n",
      "398 [Loss: 0.167438]\n",
      "399 [Loss: 0.149897]\n",
      "400 [Loss: 0.158357]\n",
      "401 [Loss: 0.156222]\n",
      "402 [Loss: 0.159811]\n",
      "403 [Loss: 0.152752]\n",
      "404 [Loss: 0.163237]\n",
      "405 [Loss: 0.149406]\n",
      "406 [Loss: 0.154039]\n",
      "407 [Loss: 0.153189]\n",
      "408 [Loss: 0.155285]\n",
      "409 [Loss: 0.143765]\n",
      "410 [Loss: 0.157562]\n",
      "411 [Loss: 0.145144]\n",
      "412 [Loss: 0.144837]\n",
      "413 [Loss: 0.152875]\n",
      "414 [Loss: 0.153593]\n",
      "415 [Loss: 0.161009]\n",
      "416 [Loss: 0.161647]\n",
      "417 [Loss: 0.154261]\n",
      "418 [Loss: 0.156067]\n",
      "419 [Loss: 0.142948]\n",
      "420 [Loss: 0.156654]\n",
      "421 [Loss: 0.145542]\n",
      "422 [Loss: 0.142465]\n",
      "423 [Loss: 0.147441]\n",
      "424 [Loss: 0.149609]\n",
      "425 [Loss: 0.155944]\n",
      "426 [Loss: 0.146219]\n",
      "427 [Loss: 0.151185]\n",
      "428 [Loss: 0.144332]\n",
      "429 [Loss: 0.142943]\n",
      "430 [Loss: 0.145626]\n",
      "431 [Loss: 0.149009]\n",
      "432 [Loss: 0.160285]\n",
      "433 [Loss: 0.144532]\n",
      "434 [Loss: 0.149393]\n",
      "435 [Loss: 0.147511]\n",
      "436 [Loss: 0.148675]\n",
      "437 [Loss: 0.148011]\n",
      "438 [Loss: 0.154026]\n",
      "439 [Loss: 0.142739]\n",
      "440 [Loss: 0.144260]\n",
      "441 [Loss: 0.146753]\n",
      "442 [Loss: 0.145735]\n",
      "443 [Loss: 0.141798]\n",
      "444 [Loss: 0.142171]\n",
      "445 [Loss: 0.147756]\n",
      "446 [Loss: 0.148091]\n",
      "447 [Loss: 0.134997]\n",
      "448 [Loss: 0.138761]\n",
      "449 [Loss: 0.150521]\n",
      "450 [Loss: 0.145476]\n",
      "451 [Loss: 0.135597]\n",
      "452 [Loss: 0.143134]\n",
      "453 [Loss: 0.137943]\n",
      "454 [Loss: 0.150445]\n",
      "455 [Loss: 0.148992]\n",
      "456 [Loss: 0.150227]\n",
      "457 [Loss: 0.165042]\n",
      "458 [Loss: 0.156718]\n",
      "459 [Loss: 0.152773]\n",
      "460 [Loss: 0.146853]\n",
      "461 [Loss: 0.150628]\n",
      "462 [Loss: 0.142244]\n",
      "463 [Loss: 0.143141]\n",
      "464 [Loss: 0.137322]\n",
      "465 [Loss: 0.140066]\n",
      "466 [Loss: 0.140823]\n",
      "467 [Loss: 0.147528]\n",
      "468 [Loss: 0.140869]\n",
      "469 [Loss: 0.141569]\n",
      "470 [Loss: 0.139972]\n",
      "471 [Loss: 0.147353]\n",
      "472 [Loss: 0.143278]\n",
      "473 [Loss: 0.135167]\n",
      "474 [Loss: 0.153457]\n",
      "475 [Loss: 0.156783]\n",
      "476 [Loss: 0.144857]\n",
      "477 [Loss: 0.158417]\n",
      "478 [Loss: 0.142128]\n",
      "479 [Loss: 0.138008]\n",
      "480 [Loss: 0.153221]\n",
      "481 [Loss: 0.144493]\n",
      "482 [Loss: 0.153218]\n",
      "483 [Loss: 0.158501]\n",
      "484 [Loss: 0.143629]\n",
      "485 [Loss: 0.139617]\n",
      "486 [Loss: 0.148210]\n",
      "487 [Loss: 0.142767]\n",
      "488 [Loss: 0.143074]\n",
      "489 [Loss: 0.134163]\n",
      "490 [Loss: 0.145127]\n",
      "491 [Loss: 0.136603]\n",
      "492 [Loss: 0.146069]\n",
      "493 [Loss: 0.138003]\n",
      "494 [Loss: 0.150056]\n",
      "495 [Loss: 0.143072]\n",
      "496 [Loss: 0.145523]\n",
      "497 [Loss: 0.143379]\n",
      "498 [Loss: 0.146916]\n",
      "499 [Loss: 0.153492]\n",
      "500 [Loss: 0.145612]\n",
      "501 [Loss: 0.140414]\n",
      "502 [Loss: 0.143452]\n",
      "503 [Loss: 0.136867]\n",
      "504 [Loss: 0.142233]\n",
      "505 [Loss: 0.152260]\n",
      "506 [Loss: 0.149588]\n",
      "507 [Loss: 0.141049]\n",
      "508 [Loss: 0.133158]\n",
      "509 [Loss: 0.146602]\n",
      "510 [Loss: 0.139699]\n",
      "511 [Loss: 0.145215]\n",
      "512 [Loss: 0.145409]\n",
      "513 [Loss: 0.141209]\n",
      "514 [Loss: 0.135300]\n",
      "515 [Loss: 0.135771]\n",
      "516 [Loss: 0.126869]\n",
      "517 [Loss: 0.137498]\n",
      "518 [Loss: 0.144205]\n",
      "519 [Loss: 0.129373]\n",
      "520 [Loss: 0.139362]\n",
      "521 [Loss: 0.141197]\n",
      "522 [Loss: 0.131151]\n",
      "523 [Loss: 0.142177]\n",
      "524 [Loss: 0.134657]\n",
      "525 [Loss: 0.141347]\n",
      "526 [Loss: 0.146487]\n",
      "527 [Loss: 0.133261]\n",
      "528 [Loss: 0.140433]\n",
      "529 [Loss: 0.144161]\n",
      "530 [Loss: 0.143875]\n",
      "531 [Loss: 0.145113]\n",
      "532 [Loss: 0.163849]\n",
      "533 [Loss: 0.164772]\n",
      "534 [Loss: 0.140387]\n",
      "535 [Loss: 0.140810]\n",
      "536 [Loss: 0.143314]\n",
      "537 [Loss: 0.143254]\n",
      "538 [Loss: 0.139542]\n",
      "539 [Loss: 0.133155]\n",
      "540 [Loss: 0.135563]\n",
      "541 [Loss: 0.139145]\n",
      "542 [Loss: 0.138283]\n",
      "543 [Loss: 0.138443]\n",
      "544 [Loss: 0.138829]\n",
      "545 [Loss: 0.139814]\n",
      "546 [Loss: 0.144927]\n",
      "547 [Loss: 0.134405]\n",
      "548 [Loss: 0.135641]\n",
      "549 [Loss: 0.134625]\n",
      "550 [Loss: 0.145527]\n",
      "551 [Loss: 0.127574]\n",
      "552 [Loss: 0.129801]\n",
      "553 [Loss: 0.128686]\n",
      "554 [Loss: 0.143197]\n",
      "555 [Loss: 0.146833]\n",
      "556 [Loss: 0.150062]\n",
      "557 [Loss: 0.143046]\n",
      "558 [Loss: 0.141663]\n",
      "559 [Loss: 0.143965]\n",
      "560 [Loss: 0.131569]\n",
      "561 [Loss: 0.128429]\n",
      "562 [Loss: 0.134629]\n",
      "563 [Loss: 0.143204]\n",
      "564 [Loss: 0.149643]\n",
      "565 [Loss: 0.140282]\n",
      "566 [Loss: 0.133794]\n",
      "567 [Loss: 0.143044]\n",
      "568 [Loss: 0.128197]\n",
      "569 [Loss: 0.133103]\n",
      "570 [Loss: 0.131392]\n",
      "571 [Loss: 0.133482]\n",
      "572 [Loss: 0.130337]\n",
      "573 [Loss: 0.137549]\n",
      "574 [Loss: 0.125717]\n",
      "575 [Loss: 0.140220]\n",
      "576 [Loss: 0.136429]\n",
      "577 [Loss: 0.133613]\n",
      "578 [Loss: 0.133302]\n",
      "579 [Loss: 0.133593]\n",
      "580 [Loss: 0.126001]\n",
      "581 [Loss: 0.141542]\n",
      "582 [Loss: 0.131084]\n",
      "583 [Loss: 0.134785]\n",
      "584 [Loss: 0.140486]\n",
      "585 [Loss: 0.134448]\n",
      "586 [Loss: 0.138538]\n",
      "587 [Loss: 0.136630]\n",
      "588 [Loss: 0.139344]\n",
      "589 [Loss: 0.141252]\n",
      "590 [Loss: 0.137666]\n",
      "591 [Loss: 0.133924]\n",
      "592 [Loss: 0.130058]\n",
      "593 [Loss: 0.133761]\n",
      "594 [Loss: 0.139485]\n",
      "595 [Loss: 0.141187]\n",
      "596 [Loss: 0.129735]\n",
      "597 [Loss: 0.133293]\n",
      "598 [Loss: 0.138369]\n",
      "599 [Loss: 0.141375]\n",
      "600 [Loss: 0.136188]\n",
      "601 [Loss: 0.141500]\n",
      "602 [Loss: 0.133522]\n",
      "603 [Loss: 0.128795]\n",
      "604 [Loss: 0.134821]\n",
      "605 [Loss: 0.139082]\n",
      "606 [Loss: 0.132374]\n",
      "607 [Loss: 0.133665]\n",
      "608 [Loss: 0.130778]\n",
      "609 [Loss: 0.131951]\n",
      "610 [Loss: 0.139762]\n",
      "611 [Loss: 0.131651]\n",
      "612 [Loss: 0.140869]\n",
      "613 [Loss: 0.147507]\n",
      "614 [Loss: 0.133096]\n",
      "615 [Loss: 0.133831]\n",
      "616 [Loss: 0.132709]\n",
      "617 [Loss: 0.135508]\n",
      "618 [Loss: 0.129326]\n",
      "619 [Loss: 0.137744]\n",
      "620 [Loss: 0.134330]\n",
      "621 [Loss: 0.132512]\n",
      "622 [Loss: 0.134436]\n",
      "623 [Loss: 0.147209]\n",
      "624 [Loss: 0.138447]\n",
      "625 [Loss: 0.131799]\n",
      "626 [Loss: 0.135633]\n",
      "627 [Loss: 0.129665]\n",
      "628 [Loss: 0.132442]\n",
      "629 [Loss: 0.135063]\n",
      "630 [Loss: 0.134006]\n",
      "631 [Loss: 0.131289]\n",
      "632 [Loss: 0.136665]\n",
      "633 [Loss: 0.144612]\n",
      "634 [Loss: 0.134178]\n",
      "635 [Loss: 0.143428]\n",
      "636 [Loss: 0.139023]\n",
      "637 [Loss: 0.133854]\n",
      "638 [Loss: 0.129358]\n",
      "639 [Loss: 0.129157]\n",
      "640 [Loss: 0.129396]\n",
      "641 [Loss: 0.128158]\n",
      "642 [Loss: 0.143479]\n",
      "643 [Loss: 0.146796]\n",
      "644 [Loss: 0.124564]\n",
      "645 [Loss: 0.129278]\n",
      "646 [Loss: 0.133984]\n",
      "647 [Loss: 0.135686]\n",
      "648 [Loss: 0.132196]\n",
      "649 [Loss: 0.135740]\n",
      "650 [Loss: 0.139044]\n",
      "651 [Loss: 0.136053]\n",
      "652 [Loss: 0.137384]\n",
      "653 [Loss: 0.128072]\n",
      "654 [Loss: 0.131611]\n",
      "655 [Loss: 0.129056]\n",
      "656 [Loss: 0.135133]\n",
      "657 [Loss: 0.136003]\n",
      "658 [Loss: 0.140348]\n",
      "659 [Loss: 0.131318]\n",
      "660 [Loss: 0.130221]\n",
      "661 [Loss: 0.132719]\n",
      "662 [Loss: 0.135075]\n",
      "663 [Loss: 0.132569]\n",
      "664 [Loss: 0.127108]\n",
      "665 [Loss: 0.125163]\n",
      "666 [Loss: 0.128916]\n",
      "667 [Loss: 0.125368]\n",
      "668 [Loss: 0.132856]\n",
      "669 [Loss: 0.128351]\n",
      "670 [Loss: 0.132122]\n",
      "671 [Loss: 0.130847]\n",
      "672 [Loss: 0.135288]\n",
      "673 [Loss: 0.132358]\n",
      "674 [Loss: 0.129061]\n",
      "675 [Loss: 0.133700]\n",
      "676 [Loss: 0.120768]\n",
      "677 [Loss: 0.132668]\n",
      "678 [Loss: 0.127104]\n",
      "679 [Loss: 0.135613]\n",
      "680 [Loss: 0.127862]\n",
      "681 [Loss: 0.128520]\n",
      "682 [Loss: 0.129103]\n",
      "683 [Loss: 0.125790]\n",
      "684 [Loss: 0.127396]\n",
      "685 [Loss: 0.124736]\n",
      "686 [Loss: 0.126000]\n",
      "687 [Loss: 0.127135]\n",
      "688 [Loss: 0.126818]\n",
      "689 [Loss: 0.124529]\n",
      "690 [Loss: 0.126357]\n",
      "691 [Loss: 0.130019]\n",
      "692 [Loss: 0.133328]\n",
      "693 [Loss: 0.129218]\n",
      "694 [Loss: 0.125242]\n",
      "695 [Loss: 0.128689]\n",
      "696 [Loss: 0.133441]\n",
      "697 [Loss: 0.133766]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "698 [Loss: 0.125228]\n",
      "699 [Loss: 0.135419]\n",
      "700 [Loss: 0.131606]\n",
      "701 [Loss: 0.127913]\n",
      "702 [Loss: 0.129658]\n",
      "703 [Loss: 0.122537]\n",
      "704 [Loss: 0.128810]\n",
      "705 [Loss: 0.128045]\n",
      "706 [Loss: 0.130337]\n",
      "707 [Loss: 0.117871]\n",
      "708 [Loss: 0.129142]\n",
      "709 [Loss: 0.124870]\n",
      "710 [Loss: 0.133913]\n",
      "711 [Loss: 0.130943]\n",
      "712 [Loss: 0.132635]\n",
      "713 [Loss: 0.133623]\n",
      "714 [Loss: 0.142540]\n",
      "715 [Loss: 0.142778]\n",
      "716 [Loss: 0.130626]\n",
      "717 [Loss: 0.129616]\n",
      "718 [Loss: 0.128781]\n",
      "719 [Loss: 0.127701]\n",
      "720 [Loss: 0.138754]\n",
      "721 [Loss: 0.124324]\n",
      "722 [Loss: 0.128685]\n",
      "723 [Loss: 0.140187]\n",
      "724 [Loss: 0.135004]\n",
      "725 [Loss: 0.131925]\n",
      "726 [Loss: 0.127267]\n",
      "727 [Loss: 0.121337]\n",
      "728 [Loss: 0.125829]\n",
      "729 [Loss: 0.130429]\n",
      "730 [Loss: 0.140622]\n",
      "731 [Loss: 0.143546]\n",
      "732 [Loss: 0.128235]\n",
      "733 [Loss: 0.132917]\n",
      "734 [Loss: 0.123544]\n",
      "735 [Loss: 0.128873]\n",
      "736 [Loss: 0.127685]\n",
      "737 [Loss: 0.133632]\n",
      "738 [Loss: 0.122226]\n",
      "739 [Loss: 0.119898]\n",
      "740 [Loss: 0.127533]\n",
      "741 [Loss: 0.121075]\n",
      "742 [Loss: 0.124145]\n",
      "743 [Loss: 0.129904]\n",
      "744 [Loss: 0.126359]\n",
      "745 [Loss: 0.121607]\n",
      "746 [Loss: 0.131079]\n",
      "747 [Loss: 0.133017]\n",
      "748 [Loss: 0.124789]\n",
      "749 [Loss: 0.128536]\n",
      "750 [Loss: 0.122751]\n",
      "751 [Loss: 0.126067]\n",
      "752 [Loss: 0.122447]\n",
      "753 [Loss: 0.119018]\n",
      "754 [Loss: 0.123734]\n",
      "755 [Loss: 0.125179]\n",
      "756 [Loss: 0.120264]\n",
      "757 [Loss: 0.127177]\n",
      "758 [Loss: 0.125745]\n",
      "759 [Loss: 0.122687]\n",
      "760 [Loss: 0.127902]\n",
      "761 [Loss: 0.125488]\n",
      "762 [Loss: 0.130003]\n",
      "763 [Loss: 0.133770]\n",
      "764 [Loss: 0.132647]\n",
      "765 [Loss: 0.115914]\n",
      "766 [Loss: 0.122572]\n",
      "767 [Loss: 0.128999]\n",
      "768 [Loss: 0.127479]\n",
      "769 [Loss: 0.131025]\n",
      "770 [Loss: 0.131387]\n",
      "771 [Loss: 0.125210]\n",
      "772 [Loss: 0.123091]\n",
      "773 [Loss: 0.127955]\n",
      "774 [Loss: 0.131783]\n",
      "775 [Loss: 0.134467]\n",
      "776 [Loss: 0.121493]\n",
      "777 [Loss: 0.127005]\n",
      "778 [Loss: 0.128250]\n",
      "779 [Loss: 0.128582]\n",
      "780 [Loss: 0.134797]\n",
      "781 [Loss: 0.124857]\n",
      "782 [Loss: 0.126485]\n",
      "783 [Loss: 0.132964]\n",
      "784 [Loss: 0.128327]\n",
      "785 [Loss: 0.130087]\n",
      "786 [Loss: 0.127928]\n",
      "787 [Loss: 0.131763]\n",
      "788 [Loss: 0.119581]\n",
      "789 [Loss: 0.119823]\n",
      "790 [Loss: 0.131382]\n",
      "791 [Loss: 0.123991]\n",
      "792 [Loss: 0.125583]\n",
      "793 [Loss: 0.118236]\n",
      "794 [Loss: 0.114955]\n",
      "795 [Loss: 0.120620]\n",
      "796 [Loss: 0.128302]\n",
      "797 [Loss: 0.127100]\n",
      "798 [Loss: 0.123071]\n",
      "799 [Loss: 0.127256]\n",
      "800 [Loss: 0.124351]\n",
      "801 [Loss: 0.131904]\n",
      "802 [Loss: 0.126037]\n",
      "803 [Loss: 0.121680]\n",
      "804 [Loss: 0.123635]\n",
      "805 [Loss: 0.125371]\n",
      "806 [Loss: 0.127377]\n",
      "807 [Loss: 0.130112]\n",
      "808 [Loss: 0.136664]\n",
      "809 [Loss: 0.142196]\n",
      "810 [Loss: 0.122689]\n",
      "811 [Loss: 0.134482]\n",
      "812 [Loss: 0.124909]\n",
      "813 [Loss: 0.126419]\n",
      "814 [Loss: 0.117569]\n",
      "815 [Loss: 0.121414]\n",
      "816 [Loss: 0.121132]\n",
      "817 [Loss: 0.132461]\n",
      "818 [Loss: 0.115940]\n",
      "819 [Loss: 0.118872]\n",
      "820 [Loss: 0.114676]\n",
      "821 [Loss: 0.121901]\n",
      "822 [Loss: 0.124965]\n",
      "823 [Loss: 0.129402]\n",
      "824 [Loss: 0.125291]\n",
      "825 [Loss: 0.124454]\n",
      "826 [Loss: 0.123492]\n",
      "827 [Loss: 0.127251]\n",
      "828 [Loss: 0.115132]\n",
      "829 [Loss: 0.125452]\n",
      "830 [Loss: 0.122365]\n",
      "831 [Loss: 0.123130]\n",
      "832 [Loss: 0.124860]\n",
      "833 [Loss: 0.126818]\n",
      "834 [Loss: 0.122302]\n",
      "835 [Loss: 0.117858]\n",
      "836 [Loss: 0.118334]\n",
      "837 [Loss: 0.125641]\n",
      "838 [Loss: 0.132201]\n",
      "839 [Loss: 0.121237]\n",
      "840 [Loss: 0.120601]\n",
      "841 [Loss: 0.112880]\n",
      "842 [Loss: 0.123942]\n",
      "843 [Loss: 0.124524]\n",
      "844 [Loss: 0.127224]\n",
      "845 [Loss: 0.131777]\n",
      "846 [Loss: 0.119026]\n",
      "847 [Loss: 0.126139]\n",
      "848 [Loss: 0.120513]\n",
      "849 [Loss: 0.119806]\n",
      "850 [Loss: 0.121633]\n",
      "851 [Loss: 0.132526]\n",
      "852 [Loss: 0.129838]\n",
      "853 [Loss: 0.128214]\n",
      "854 [Loss: 0.124796]\n",
      "855 [Loss: 0.125063]\n",
      "856 [Loss: 0.120052]\n",
      "857 [Loss: 0.125776]\n",
      "858 [Loss: 0.116993]\n",
      "859 [Loss: 0.125135]\n",
      "860 [Loss: 0.121986]\n",
      "861 [Loss: 0.122721]\n",
      "862 [Loss: 0.114271]\n",
      "863 [Loss: 0.114553]\n",
      "864 [Loss: 0.121529]\n",
      "865 [Loss: 0.121585]\n",
      "866 [Loss: 0.127036]\n",
      "867 [Loss: 0.129976]\n",
      "868 [Loss: 0.117422]\n",
      "869 [Loss: 0.127488]\n",
      "870 [Loss: 0.124735]\n",
      "871 [Loss: 0.127180]\n",
      "872 [Loss: 0.124570]\n",
      "873 [Loss: 0.124464]\n",
      "874 [Loss: 0.124165]\n",
      "875 [Loss: 0.124277]\n",
      "876 [Loss: 0.116005]\n",
      "877 [Loss: 0.118305]\n",
      "878 [Loss: 0.118518]\n",
      "879 [Loss: 0.123162]\n",
      "880 [Loss: 0.118120]\n",
      "881 [Loss: 0.120240]\n",
      "882 [Loss: 0.119967]\n",
      "883 [Loss: 0.115252]\n",
      "884 [Loss: 0.116481]\n",
      "885 [Loss: 0.129845]\n",
      "886 [Loss: 0.128519]\n",
      "887 [Loss: 0.125774]\n",
      "888 [Loss: 0.124833]\n",
      "889 [Loss: 0.128340]\n",
      "890 [Loss: 0.119093]\n",
      "891 [Loss: 0.115157]\n",
      "892 [Loss: 0.122032]\n",
      "893 [Loss: 0.122332]\n",
      "894 [Loss: 0.124150]\n",
      "895 [Loss: 0.118743]\n",
      "896 [Loss: 0.128693]\n",
      "897 [Loss: 0.126461]\n",
      "898 [Loss: 0.121586]\n",
      "899 [Loss: 0.118767]\n",
      "900 [Loss: 0.123968]\n",
      "901 [Loss: 0.117163]\n",
      "902 [Loss: 0.120318]\n",
      "903 [Loss: 0.128732]\n",
      "904 [Loss: 0.127555]\n",
      "905 [Loss: 0.126743]\n",
      "906 [Loss: 0.118244]\n",
      "907 [Loss: 0.112027]\n",
      "908 [Loss: 0.122239]\n",
      "909 [Loss: 0.122995]\n",
      "910 [Loss: 0.127298]\n",
      "911 [Loss: 0.128627]\n",
      "912 [Loss: 0.118102]\n",
      "913 [Loss: 0.120855]\n",
      "914 [Loss: 0.116189]\n",
      "915 [Loss: 0.127143]\n",
      "916 [Loss: 0.126538]\n",
      "917 [Loss: 0.121460]\n",
      "918 [Loss: 0.117265]\n",
      "919 [Loss: 0.119490]\n",
      "920 [Loss: 0.118655]\n",
      "921 [Loss: 0.124336]\n",
      "922 [Loss: 0.125942]\n",
      "923 [Loss: 0.119108]\n",
      "924 [Loss: 0.120006]\n",
      "925 [Loss: 0.125228]\n",
      "926 [Loss: 0.128565]\n",
      "927 [Loss: 0.123885]\n",
      "928 [Loss: 0.118351]\n",
      "929 [Loss: 0.118870]\n",
      "930 [Loss: 0.120763]\n",
      "931 [Loss: 0.120987]\n",
      "932 [Loss: 0.121277]\n",
      "933 [Loss: 0.115219]\n",
      "934 [Loss: 0.113555]\n",
      "935 [Loss: 0.116928]\n",
      "936 [Loss: 0.117782]\n",
      "937 [Loss: 0.118444]\n",
      "938 [Loss: 0.123913]\n",
      "939 [Loss: 0.127237]\n",
      "940 [Loss: 0.125281]\n",
      "941 [Loss: 0.118480]\n",
      "942 [Loss: 0.118741]\n",
      "943 [Loss: 0.115910]\n",
      "944 [Loss: 0.115154]\n",
      "945 [Loss: 0.124120]\n",
      "946 [Loss: 0.133554]\n",
      "947 [Loss: 0.112161]\n",
      "948 [Loss: 0.119778]\n",
      "949 [Loss: 0.122266]\n",
      "950 [Loss: 0.112899]\n",
      "951 [Loss: 0.126088]\n",
      "952 [Loss: 0.119686]\n",
      "953 [Loss: 0.117960]\n",
      "954 [Loss: 0.114092]\n",
      "955 [Loss: 0.119610]\n",
      "956 [Loss: 0.118886]\n",
      "957 [Loss: 0.119688]\n",
      "958 [Loss: 0.116551]\n",
      "959 [Loss: 0.114806]\n",
      "960 [Loss: 0.120762]\n",
      "961 [Loss: 0.129797]\n",
      "962 [Loss: 0.121711]\n",
      "963 [Loss: 0.114661]\n",
      "964 [Loss: 0.115299]\n",
      "965 [Loss: 0.117409]\n",
      "966 [Loss: 0.125243]\n",
      "967 [Loss: 0.112258]\n",
      "968 [Loss: 0.117303]\n",
      "969 [Loss: 0.125336]\n",
      "970 [Loss: 0.124287]\n",
      "971 [Loss: 0.121124]\n",
      "972 [Loss: 0.121388]\n",
      "973 [Loss: 0.116069]\n",
      "974 [Loss: 0.116604]\n",
      "975 [Loss: 0.112564]\n",
      "976 [Loss: 0.118435]\n",
      "977 [Loss: 0.119217]\n",
      "978 [Loss: 0.115280]\n",
      "979 [Loss: 0.118526]\n",
      "980 [Loss: 0.117091]\n",
      "981 [Loss: 0.121407]\n",
      "982 [Loss: 0.114012]\n",
      "983 [Loss: 0.122134]\n",
      "984 [Loss: 0.112576]\n",
      "985 [Loss: 0.124984]\n",
      "986 [Loss: 0.127129]\n",
      "987 [Loss: 0.128110]\n",
      "988 [Loss: 0.122021]\n",
      "989 [Loss: 0.116993]\n",
      "990 [Loss: 0.117485]\n",
      "991 [Loss: 0.110666]\n",
      "992 [Loss: 0.120881]\n",
      "993 [Loss: 0.126936]\n",
      "994 [Loss: 0.123236]\n",
      "995 [Loss: 0.108416]\n",
      "996 [Loss: 0.120923]\n",
      "997 [Loss: 0.111445]\n",
      "998 [Loss: 0.122432]\n",
      "999 [Loss: 0.118087]\n",
      "1000 [Loss: 0.120814]\n",
      "1001 [Loss: 0.121379]\n",
      "1002 [Loss: 0.114784]\n",
      "1003 [Loss: 0.116832]\n",
      "1004 [Loss: 0.111860]\n",
      "1005 [Loss: 0.120931]\n",
      "1006 [Loss: 0.116695]\n",
      "1007 [Loss: 0.121744]\n",
      "1008 [Loss: 0.111026]\n",
      "1009 [Loss: 0.117238]\n",
      "1010 [Loss: 0.127130]\n",
      "1011 [Loss: 0.125207]\n",
      "1012 [Loss: 0.116049]\n",
      "1013 [Loss: 0.120905]\n",
      "1014 [Loss: 0.111608]\n",
      "1015 [Loss: 0.114308]\n",
      "1016 [Loss: 0.122356]\n",
      "1017 [Loss: 0.114039]\n",
      "1018 [Loss: 0.122727]\n",
      "1019 [Loss: 0.121328]\n",
      "1020 [Loss: 0.113591]\n",
      "1021 [Loss: 0.118095]\n",
      "1022 [Loss: 0.117862]\n",
      "1023 [Loss: 0.117515]\n",
      "1024 [Loss: 0.114602]\n",
      "1025 [Loss: 0.109531]\n",
      "1026 [Loss: 0.121576]\n",
      "1027 [Loss: 0.120983]\n",
      "1028 [Loss: 0.117565]\n",
      "1029 [Loss: 0.113837]\n",
      "1030 [Loss: 0.115119]\n",
      "1031 [Loss: 0.126729]\n",
      "1032 [Loss: 0.116771]\n",
      "1033 [Loss: 0.113077]\n",
      "1034 [Loss: 0.127285]\n",
      "1035 [Loss: 0.125990]\n",
      "1036 [Loss: 0.122574]\n",
      "1037 [Loss: 0.120440]\n",
      "1038 [Loss: 0.117445]\n",
      "1039 [Loss: 0.117916]\n",
      "1040 [Loss: 0.117141]\n",
      "1041 [Loss: 0.117123]\n",
      "1042 [Loss: 0.111979]\n",
      "1043 [Loss: 0.117717]\n",
      "1044 [Loss: 0.115906]\n",
      "1045 [Loss: 0.119146]\n",
      "1046 [Loss: 0.113464]\n",
      "1047 [Loss: 0.121885]\n",
      "1048 [Loss: 0.121246]\n",
      "1049 [Loss: 0.116373]\n",
      "1050 [Loss: 0.117106]\n",
      "1051 [Loss: 0.121440]\n",
      "1052 [Loss: 0.119634]\n",
      "1053 [Loss: 0.115693]\n",
      "1054 [Loss: 0.110858]\n",
      "1055 [Loss: 0.117389]\n",
      "1056 [Loss: 0.126111]\n",
      "1057 [Loss: 0.130161]\n",
      "1058 [Loss: 0.121374]\n",
      "1059 [Loss: 0.123252]\n",
      "1060 [Loss: 0.119722]\n",
      "1061 [Loss: 0.121241]\n",
      "1062 [Loss: 0.120930]\n",
      "1063 [Loss: 0.118446]\n",
      "1064 [Loss: 0.108273]\n",
      "1065 [Loss: 0.118772]\n",
      "1066 [Loss: 0.111736]\n",
      "1067 [Loss: 0.108748]\n",
      "1068 [Loss: 0.118657]\n",
      "1069 [Loss: 0.111767]\n",
      "1070 [Loss: 0.117395]\n",
      "1071 [Loss: 0.117273]\n",
      "1072 [Loss: 0.124264]\n",
      "1073 [Loss: 0.117212]\n",
      "1074 [Loss: 0.117155]\n",
      "1075 [Loss: 0.112205]\n",
      "1076 [Loss: 0.114389]\n",
      "1077 [Loss: 0.121087]\n",
      "1078 [Loss: 0.109579]\n",
      "1079 [Loss: 0.119957]\n",
      "1080 [Loss: 0.109767]\n",
      "1081 [Loss: 0.113091]\n",
      "1082 [Loss: 0.111316]\n",
      "1083 [Loss: 0.112741]\n",
      "1084 [Loss: 0.118786]\n",
      "1085 [Loss: 0.107545]\n",
      "1086 [Loss: 0.115066]\n",
      "1087 [Loss: 0.111020]\n",
      "1088 [Loss: 0.112887]\n",
      "1089 [Loss: 0.122338]\n",
      "1090 [Loss: 0.111022]\n",
      "1091 [Loss: 0.114587]\n",
      "1092 [Loss: 0.115683]\n",
      "1093 [Loss: 0.113491]\n",
      "1094 [Loss: 0.120023]\n",
      "1095 [Loss: 0.121962]\n",
      "1096 [Loss: 0.112743]\n",
      "1097 [Loss: 0.112360]\n",
      "1098 [Loss: 0.119351]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1099 [Loss: 0.113267]\n",
      "1100 [Loss: 0.121796]\n",
      "1101 [Loss: 0.112248]\n",
      "1102 [Loss: 0.112917]\n",
      "1103 [Loss: 0.130115]\n",
      "1104 [Loss: 0.116124]\n",
      "1105 [Loss: 0.118920]\n",
      "1106 [Loss: 0.114725]\n",
      "1107 [Loss: 0.109113]\n",
      "1108 [Loss: 0.116612]\n",
      "1109 [Loss: 0.117425]\n",
      "1110 [Loss: 0.126944]\n",
      "1111 [Loss: 0.124940]\n",
      "1112 [Loss: 0.110811]\n",
      "1113 [Loss: 0.118430]\n",
      "1114 [Loss: 0.114660]\n",
      "1115 [Loss: 0.120394]\n",
      "1116 [Loss: 0.126745]\n",
      "1117 [Loss: 0.120363]\n",
      "1118 [Loss: 0.115874]\n",
      "1119 [Loss: 0.115906]\n",
      "1120 [Loss: 0.117961]\n",
      "1121 [Loss: 0.111042]\n",
      "1122 [Loss: 0.113833]\n",
      "1123 [Loss: 0.114660]\n",
      "1124 [Loss: 0.114053]\n",
      "1125 [Loss: 0.121110]\n",
      "1126 [Loss: 0.111478]\n",
      "1127 [Loss: 0.119035]\n",
      "1128 [Loss: 0.118699]\n",
      "1129 [Loss: 0.118428]\n",
      "1130 [Loss: 0.113666]\n",
      "1131 [Loss: 0.113150]\n",
      "1132 [Loss: 0.116831]\n",
      "1133 [Loss: 0.115032]\n",
      "1134 [Loss: 0.120156]\n",
      "1135 [Loss: 0.116457]\n",
      "1136 [Loss: 0.121065]\n",
      "1137 [Loss: 0.117554]\n",
      "1138 [Loss: 0.116339]\n",
      "1139 [Loss: 0.120631]\n",
      "1140 [Loss: 0.115503]\n",
      "1141 [Loss: 0.108508]\n",
      "1142 [Loss: 0.111138]\n",
      "1143 [Loss: 0.116721]\n",
      "1144 [Loss: 0.124604]\n",
      "1145 [Loss: 0.114079]\n",
      "1146 [Loss: 0.111844]\n",
      "1147 [Loss: 0.117846]\n",
      "1148 [Loss: 0.119314]\n",
      "1149 [Loss: 0.118137]\n",
      "1150 [Loss: 0.118037]\n",
      "1151 [Loss: 0.113177]\n",
      "1152 [Loss: 0.121853]\n",
      "1153 [Loss: 0.124723]\n",
      "1154 [Loss: 0.113722]\n",
      "1155 [Loss: 0.115553]\n",
      "1156 [Loss: 0.117583]\n",
      "1157 [Loss: 0.111314]\n",
      "1158 [Loss: 0.114328]\n",
      "1159 [Loss: 0.111492]\n",
      "1160 [Loss: 0.117657]\n",
      "1161 [Loss: 0.115608]\n",
      "1162 [Loss: 0.105927]\n",
      "1163 [Loss: 0.121899]\n",
      "1164 [Loss: 0.110272]\n",
      "1165 [Loss: 0.114124]\n",
      "1166 [Loss: 0.110201]\n",
      "1167 [Loss: 0.111560]\n",
      "1168 [Loss: 0.115799]\n",
      "1169 [Loss: 0.117749]\n",
      "1170 [Loss: 0.111584]\n",
      "1171 [Loss: 0.111727]\n",
      "1172 [Loss: 0.111523]\n",
      "1173 [Loss: 0.109014]\n",
      "1174 [Loss: 0.114826]\n",
      "1175 [Loss: 0.119499]\n",
      "1176 [Loss: 0.118276]\n",
      "1177 [Loss: 0.109409]\n",
      "1178 [Loss: 0.110765]\n",
      "1179 [Loss: 0.114367]\n",
      "1180 [Loss: 0.108973]\n",
      "1181 [Loss: 0.112694]\n",
      "1182 [Loss: 0.111996]\n",
      "1183 [Loss: 0.120383]\n",
      "1184 [Loss: 0.104946]\n",
      "1185 [Loss: 0.116171]\n",
      "1186 [Loss: 0.109762]\n",
      "1187 [Loss: 0.114854]\n",
      "1188 [Loss: 0.126597]\n",
      "1189 [Loss: 0.118214]\n",
      "1190 [Loss: 0.113271]\n",
      "1191 [Loss: 0.116504]\n",
      "1192 [Loss: 0.113278]\n",
      "1193 [Loss: 0.112280]\n",
      "1194 [Loss: 0.108525]\n",
      "1195 [Loss: 0.116053]\n",
      "1196 [Loss: 0.118029]\n",
      "1197 [Loss: 0.115105]\n",
      "1198 [Loss: 0.113349]\n",
      "1199 [Loss: 0.108008]\n",
      "1200 [Loss: 0.110095]\n",
      "1201 [Loss: 0.113809]\n",
      "1202 [Loss: 0.112392]\n",
      "1203 [Loss: 0.112574]\n",
      "1204 [Loss: 0.120546]\n",
      "1205 [Loss: 0.128161]\n",
      "1206 [Loss: 0.115371]\n",
      "1207 [Loss: 0.118205]\n",
      "1208 [Loss: 0.111903]\n",
      "1209 [Loss: 0.104601]\n",
      "1210 [Loss: 0.112084]\n",
      "1211 [Loss: 0.111340]\n",
      "1212 [Loss: 0.114052]\n",
      "1213 [Loss: 0.112385]\n",
      "1214 [Loss: 0.113245]\n",
      "1215 [Loss: 0.117864]\n",
      "1216 [Loss: 0.106600]\n",
      "1217 [Loss: 0.118104]\n",
      "1218 [Loss: 0.112572]\n",
      "1219 [Loss: 0.109747]\n",
      "1220 [Loss: 0.112505]\n",
      "1221 [Loss: 0.120359]\n",
      "1222 [Loss: 0.117899]\n",
      "1223 [Loss: 0.118080]\n",
      "1224 [Loss: 0.110519]\n",
      "1225 [Loss: 0.111930]\n",
      "1226 [Loss: 0.114557]\n",
      "1227 [Loss: 0.117304]\n",
      "1228 [Loss: 0.113645]\n",
      "1229 [Loss: 0.110416]\n",
      "1230 [Loss: 0.115908]\n",
      "1231 [Loss: 0.107341]\n",
      "1232 [Loss: 0.106660]\n",
      "1233 [Loss: 0.118712]\n",
      "1234 [Loss: 0.109923]\n",
      "1235 [Loss: 0.114100]\n",
      "1236 [Loss: 0.119175]\n",
      "1237 [Loss: 0.103885]\n",
      "1238 [Loss: 0.113706]\n",
      "1239 [Loss: 0.121935]\n",
      "1240 [Loss: 0.125089]\n",
      "1241 [Loss: 0.108305]\n",
      "1242 [Loss: 0.111115]\n",
      "1243 [Loss: 0.112119]\n",
      "1244 [Loss: 0.116668]\n",
      "1245 [Loss: 0.111428]\n",
      "1246 [Loss: 0.118537]\n",
      "1247 [Loss: 0.108279]\n",
      "1248 [Loss: 0.111385]\n",
      "1249 [Loss: 0.113898]\n",
      "1250 [Loss: 0.114579]\n",
      "1251 [Loss: 0.105847]\n",
      "1252 [Loss: 0.111022]\n",
      "1253 [Loss: 0.101484]\n",
      "1254 [Loss: 0.108989]\n",
      "1255 [Loss: 0.113950]\n",
      "1256 [Loss: 0.112251]\n",
      "1257 [Loss: 0.113728]\n",
      "1258 [Loss: 0.108931]\n",
      "1259 [Loss: 0.112330]\n",
      "1260 [Loss: 0.117035]\n",
      "1261 [Loss: 0.130598]\n",
      "1262 [Loss: 0.114816]\n",
      "1263 [Loss: 0.113357]\n",
      "1264 [Loss: 0.110427]\n",
      "1265 [Loss: 0.113824]\n",
      "1266 [Loss: 0.109077]\n",
      "1267 [Loss: 0.113260]\n",
      "1268 [Loss: 0.107161]\n",
      "1269 [Loss: 0.115138]\n",
      "1270 [Loss: 0.113659]\n",
      "1271 [Loss: 0.116150]\n",
      "1272 [Loss: 0.105507]\n",
      "1273 [Loss: 0.112641]\n",
      "1274 [Loss: 0.118300]\n",
      "1275 [Loss: 0.125604]\n",
      "1276 [Loss: 0.118792]\n",
      "1277 [Loss: 0.123910]\n",
      "1278 [Loss: 0.118007]\n",
      "1279 [Loss: 0.111851]\n",
      "1280 [Loss: 0.114265]\n",
      "1281 [Loss: 0.113141]\n",
      "1282 [Loss: 0.114687]\n",
      "1283 [Loss: 0.108182]\n",
      "1284 [Loss: 0.110238]\n",
      "1285 [Loss: 0.111060]\n",
      "1286 [Loss: 0.111900]\n",
      "1287 [Loss: 0.104279]\n",
      "1288 [Loss: 0.109129]\n",
      "1289 [Loss: 0.110314]\n",
      "1290 [Loss: 0.120634]\n",
      "1291 [Loss: 0.115471]\n",
      "1292 [Loss: 0.114740]\n",
      "1293 [Loss: 0.104759]\n",
      "1294 [Loss: 0.116267]\n",
      "1295 [Loss: 0.104007]\n",
      "1296 [Loss: 0.109312]\n",
      "1297 [Loss: 0.107847]\n",
      "1298 [Loss: 0.113017]\n",
      "1299 [Loss: 0.114957]\n",
      "1300 [Loss: 0.111083]\n",
      "1301 [Loss: 0.103185]\n",
      "1302 [Loss: 0.114046]\n",
      "1303 [Loss: 0.114653]\n",
      "1304 [Loss: 0.109566]\n",
      "1305 [Loss: 0.107173]\n",
      "1306 [Loss: 0.105870]\n",
      "1307 [Loss: 0.110347]\n",
      "1308 [Loss: 0.123823]\n",
      "1309 [Loss: 0.121924]\n",
      "1310 [Loss: 0.116196]\n",
      "1311 [Loss: 0.109756]\n",
      "1312 [Loss: 0.120309]\n",
      "1313 [Loss: 0.110780]\n",
      "1314 [Loss: 0.107577]\n",
      "1315 [Loss: 0.111904]\n",
      "1316 [Loss: 0.116393]\n",
      "1317 [Loss: 0.108990]\n",
      "1318 [Loss: 0.108901]\n",
      "1319 [Loss: 0.108178]\n",
      "1320 [Loss: 0.102847]\n",
      "1321 [Loss: 0.108501]\n",
      "1322 [Loss: 0.113750]\n",
      "1323 [Loss: 0.112619]\n",
      "1324 [Loss: 0.111876]\n",
      "1325 [Loss: 0.108907]\n",
      "1326 [Loss: 0.109579]\n",
      "1327 [Loss: 0.101358]\n",
      "1328 [Loss: 0.100247]\n",
      "1329 [Loss: 0.110157]\n",
      "1330 [Loss: 0.107579]\n",
      "1331 [Loss: 0.112442]\n",
      "1332 [Loss: 0.112200]\n",
      "1333 [Loss: 0.111009]\n",
      "1334 [Loss: 0.108122]\n",
      "1335 [Loss: 0.108785]\n",
      "1336 [Loss: 0.108934]\n",
      "1337 [Loss: 0.112465]\n",
      "1338 [Loss: 0.104640]\n",
      "1339 [Loss: 0.110189]\n",
      "1340 [Loss: 0.103922]\n",
      "1341 [Loss: 0.110712]\n",
      "1342 [Loss: 0.116733]\n",
      "1343 [Loss: 0.118123]\n",
      "1344 [Loss: 0.114108]\n",
      "1345 [Loss: 0.112620]\n",
      "1346 [Loss: 0.110231]\n",
      "1347 [Loss: 0.110879]\n",
      "1348 [Loss: 0.110827]\n",
      "1349 [Loss: 0.114066]\n",
      "1350 [Loss: 0.114645]\n",
      "1351 [Loss: 0.113780]\n",
      "1352 [Loss: 0.108780]\n",
      "1353 [Loss: 0.107295]\n",
      "1354 [Loss: 0.107256]\n",
      "1355 [Loss: 0.112461]\n",
      "1356 [Loss: 0.107630]\n",
      "1357 [Loss: 0.107819]\n",
      "1358 [Loss: 0.102453]\n",
      "1359 [Loss: 0.107487]\n",
      "1360 [Loss: 0.110463]\n",
      "1361 [Loss: 0.108606]\n",
      "1362 [Loss: 0.106443]\n",
      "1363 [Loss: 0.117089]\n",
      "1364 [Loss: 0.107456]\n",
      "1365 [Loss: 0.102306]\n",
      "1366 [Loss: 0.111268]\n",
      "1367 [Loss: 0.114808]\n",
      "1368 [Loss: 0.106428]\n",
      "1369 [Loss: 0.114863]\n",
      "1370 [Loss: 0.106391]\n",
      "1371 [Loss: 0.110235]\n",
      "1372 [Loss: 0.106638]\n",
      "1373 [Loss: 0.112704]\n",
      "1374 [Loss: 0.116317]\n",
      "1375 [Loss: 0.111858]\n",
      "1376 [Loss: 0.107181]\n",
      "1377 [Loss: 0.102819]\n",
      "1378 [Loss: 0.114072]\n",
      "1379 [Loss: 0.116687]\n",
      "1380 [Loss: 0.111992]\n",
      "1381 [Loss: 0.115653]\n",
      "1382 [Loss: 0.106328]\n",
      "1383 [Loss: 0.109187]\n",
      "1384 [Loss: 0.111493]\n",
      "1385 [Loss: 0.115035]\n",
      "1386 [Loss: 0.109059]\n",
      "1387 [Loss: 0.107015]\n",
      "1388 [Loss: 0.109161]\n",
      "1389 [Loss: 0.112140]\n",
      "1390 [Loss: 0.107292]\n",
      "1391 [Loss: 0.109399]\n",
      "1392 [Loss: 0.111196]\n",
      "1393 [Loss: 0.108710]\n",
      "1394 [Loss: 0.111593]\n",
      "1395 [Loss: 0.104610]\n",
      "1396 [Loss: 0.108919]\n",
      "1397 [Loss: 0.106870]\n",
      "1398 [Loss: 0.110385]\n",
      "1399 [Loss: 0.105626]\n",
      "1400 [Loss: 0.108523]\n",
      "1401 [Loss: 0.114480]\n",
      "1402 [Loss: 0.114990]\n",
      "1403 [Loss: 0.111067]\n",
      "1404 [Loss: 0.109873]\n",
      "1405 [Loss: 0.107047]\n",
      "1406 [Loss: 0.099508]\n",
      "1407 [Loss: 0.108290]\n",
      "1408 [Loss: 0.107730]\n",
      "1409 [Loss: 0.113084]\n",
      "1410 [Loss: 0.106649]\n",
      "1411 [Loss: 0.121577]\n",
      "1412 [Loss: 0.118431]\n",
      "1413 [Loss: 0.101673]\n",
      "1414 [Loss: 0.101392]\n",
      "1415 [Loss: 0.102647]\n",
      "1416 [Loss: 0.112657]\n",
      "1417 [Loss: 0.109987]\n",
      "1418 [Loss: 0.113665]\n",
      "1419 [Loss: 0.107964]\n",
      "1420 [Loss: 0.100323]\n",
      "1421 [Loss: 0.106549]\n",
      "1422 [Loss: 0.107682]\n",
      "1423 [Loss: 0.117496]\n",
      "1424 [Loss: 0.112830]\n",
      "1425 [Loss: 0.108542]\n",
      "1426 [Loss: 0.103839]\n",
      "1427 [Loss: 0.115527]\n",
      "1428 [Loss: 0.098199]\n",
      "1429 [Loss: 0.107955]\n",
      "1430 [Loss: 0.111539]\n",
      "1431 [Loss: 0.112523]\n",
      "1432 [Loss: 0.108972]\n",
      "1433 [Loss: 0.115534]\n",
      "1434 [Loss: 0.110224]\n",
      "1435 [Loss: 0.110411]\n",
      "1436 [Loss: 0.100691]\n",
      "1437 [Loss: 0.110358]\n",
      "1438 [Loss: 0.109446]\n",
      "1439 [Loss: 0.106733]\n",
      "1440 [Loss: 0.105074]\n",
      "1441 [Loss: 0.110247]\n",
      "1442 [Loss: 0.111474]\n",
      "1443 [Loss: 0.111412]\n",
      "1444 [Loss: 0.108584]\n",
      "1445 [Loss: 0.110804]\n",
      "1446 [Loss: 0.098855]\n",
      "1447 [Loss: 0.106508]\n",
      "1448 [Loss: 0.120347]\n",
      "1449 [Loss: 0.120372]\n",
      "1450 [Loss: 0.118790]\n",
      "1451 [Loss: 0.111316]\n",
      "1452 [Loss: 0.109757]\n",
      "1453 [Loss: 0.116784]\n",
      "1454 [Loss: 0.114345]\n",
      "1455 [Loss: 0.104272]\n",
      "1456 [Loss: 0.098206]\n",
      "1457 [Loss: 0.107973]\n",
      "1458 [Loss: 0.104192]\n",
      "1459 [Loss: 0.106678]\n",
      "1460 [Loss: 0.118556]\n",
      "1461 [Loss: 0.105969]\n",
      "1462 [Loss: 0.113905]\n",
      "1463 [Loss: 0.108355]\n",
      "1464 [Loss: 0.109122]\n",
      "1465 [Loss: 0.103595]\n",
      "1466 [Loss: 0.108442]\n",
      "1467 [Loss: 0.115790]\n",
      "1468 [Loss: 0.100363]\n",
      "1469 [Loss: 0.103637]\n",
      "1470 [Loss: 0.105915]\n",
      "1471 [Loss: 0.101402]\n",
      "1472 [Loss: 0.107166]\n",
      "1473 [Loss: 0.102964]\n",
      "1474 [Loss: 0.116154]\n",
      "1475 [Loss: 0.121717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1476 [Loss: 0.115937]\n",
      "1477 [Loss: 0.116502]\n",
      "1478 [Loss: 0.102824]\n",
      "1479 [Loss: 0.113788]\n",
      "1480 [Loss: 0.114495]\n",
      "1481 [Loss: 0.109185]\n",
      "1482 [Loss: 0.108112]\n",
      "1483 [Loss: 0.107337]\n",
      "1484 [Loss: 0.107419]\n",
      "1485 [Loss: 0.108566]\n",
      "1486 [Loss: 0.103731]\n",
      "1487 [Loss: 0.110469]\n",
      "1488 [Loss: 0.107582]\n",
      "1489 [Loss: 0.106683]\n",
      "1490 [Loss: 0.100942]\n",
      "1491 [Loss: 0.104371]\n",
      "1492 [Loss: 0.109781]\n",
      "1493 [Loss: 0.104718]\n",
      "1494 [Loss: 0.106734]\n",
      "1495 [Loss: 0.106349]\n",
      "1496 [Loss: 0.101583]\n",
      "1497 [Loss: 0.099794]\n",
      "1498 [Loss: 0.107389]\n",
      "1499 [Loss: 0.105527]\n",
      "1500 [Loss: 0.111191]\n",
      "1501 [Loss: 0.108376]\n",
      "1502 [Loss: 0.113384]\n",
      "1503 [Loss: 0.108716]\n",
      "1504 [Loss: 0.106548]\n",
      "1505 [Loss: 0.107696]\n",
      "1506 [Loss: 0.106963]\n",
      "1507 [Loss: 0.103226]\n",
      "1508 [Loss: 0.108281]\n",
      "1509 [Loss: 0.104496]\n",
      "1510 [Loss: 0.108214]\n",
      "1511 [Loss: 0.108826]\n",
      "1512 [Loss: 0.106851]\n",
      "1513 [Loss: 0.119434]\n",
      "1514 [Loss: 0.115091]\n",
      "1515 [Loss: 0.110114]\n",
      "1516 [Loss: 0.107554]\n",
      "1517 [Loss: 0.112564]\n",
      "1518 [Loss: 0.107253]\n",
      "1519 [Loss: 0.104927]\n",
      "1520 [Loss: 0.105250]\n",
      "1521 [Loss: 0.114947]\n",
      "1522 [Loss: 0.114221]\n",
      "1523 [Loss: 0.104301]\n",
      "1524 [Loss: 0.106407]\n",
      "1525 [Loss: 0.104061]\n",
      "1526 [Loss: 0.108243]\n",
      "1527 [Loss: 0.104858]\n",
      "1528 [Loss: 0.103099]\n",
      "1529 [Loss: 0.105345]\n",
      "1530 [Loss: 0.113241]\n",
      "1531 [Loss: 0.103663]\n",
      "1532 [Loss: 0.105305]\n",
      "1533 [Loss: 0.107652]\n",
      "1534 [Loss: 0.105179]\n",
      "1535 [Loss: 0.101211]\n",
      "1536 [Loss: 0.100010]\n",
      "1537 [Loss: 0.093444]\n",
      "1538 [Loss: 0.113221]\n",
      "1539 [Loss: 0.109011]\n",
      "1540 [Loss: 0.102670]\n",
      "1541 [Loss: 0.104897]\n",
      "1542 [Loss: 0.101761]\n",
      "1543 [Loss: 0.106659]\n",
      "1544 [Loss: 0.109444]\n",
      "1545 [Loss: 0.112020]\n",
      "1546 [Loss: 0.106708]\n",
      "1547 [Loss: 0.103484]\n",
      "1548 [Loss: 0.107768]\n",
      "1549 [Loss: 0.107949]\n",
      "1550 [Loss: 0.102473]\n",
      "1551 [Loss: 0.105530]\n",
      "1552 [Loss: 0.102838]\n",
      "1553 [Loss: 0.100982]\n",
      "1554 [Loss: 0.105647]\n",
      "1555 [Loss: 0.097311]\n",
      "1556 [Loss: 0.102965]\n",
      "1557 [Loss: 0.105169]\n",
      "1558 [Loss: 0.110793]\n",
      "1559 [Loss: 0.103702]\n",
      "1560 [Loss: 0.103169]\n",
      "1561 [Loss: 0.104234]\n",
      "1562 [Loss: 0.105738]\n",
      "1563 [Loss: 0.101836]\n",
      "1564 [Loss: 0.104748]\n",
      "1565 [Loss: 0.108506]\n",
      "1566 [Loss: 0.106575]\n",
      "1567 [Loss: 0.113205]\n",
      "1568 [Loss: 0.100519]\n",
      "1569 [Loss: 0.112171]\n",
      "1570 [Loss: 0.112645]\n",
      "1571 [Loss: 0.102668]\n",
      "1572 [Loss: 0.112252]\n",
      "1573 [Loss: 0.105529]\n",
      "1574 [Loss: 0.111175]\n",
      "1575 [Loss: 0.110727]\n",
      "1576 [Loss: 0.112250]\n",
      "1577 [Loss: 0.107451]\n",
      "1578 [Loss: 0.104084]\n",
      "1579 [Loss: 0.103136]\n",
      "1580 [Loss: 0.105404]\n",
      "1581 [Loss: 0.111428]\n",
      "1582 [Loss: 0.107807]\n",
      "1583 [Loss: 0.106868]\n",
      "1584 [Loss: 0.107242]\n",
      "1585 [Loss: 0.108857]\n",
      "1586 [Loss: 0.110112]\n",
      "1587 [Loss: 0.106685]\n",
      "1588 [Loss: 0.112073]\n",
      "1589 [Loss: 0.101804]\n",
      "1590 [Loss: 0.103694]\n",
      "1591 [Loss: 0.105170]\n",
      "1592 [Loss: 0.106744]\n",
      "1593 [Loss: 0.103340]\n",
      "1594 [Loss: 0.104475]\n",
      "1595 [Loss: 0.108323]\n",
      "1596 [Loss: 0.106373]\n",
      "1597 [Loss: 0.115062]\n",
      "1598 [Loss: 0.111970]\n",
      "1599 [Loss: 0.100060]\n",
      "1600 [Loss: 0.107070]\n",
      "1601 [Loss: 0.101800]\n",
      "1602 [Loss: 0.106644]\n",
      "1603 [Loss: 0.107671]\n",
      "1604 [Loss: 0.107404]\n",
      "1605 [Loss: 0.107832]\n",
      "1606 [Loss: 0.108218]\n",
      "1607 [Loss: 0.101787]\n",
      "1608 [Loss: 0.099896]\n",
      "1609 [Loss: 0.099398]\n",
      "1610 [Loss: 0.105632]\n",
      "1611 [Loss: 0.111866]\n",
      "1612 [Loss: 0.108935]\n",
      "1613 [Loss: 0.106881]\n",
      "1614 [Loss: 0.107527]\n",
      "1615 [Loss: 0.108130]\n",
      "1616 [Loss: 0.105746]\n",
      "1617 [Loss: 0.100011]\n",
      "1618 [Loss: 0.103586]\n",
      "1619 [Loss: 0.100293]\n",
      "1620 [Loss: 0.109514]\n",
      "1621 [Loss: 0.104642]\n",
      "1622 [Loss: 0.110170]\n",
      "1623 [Loss: 0.106313]\n",
      "1624 [Loss: 0.103216]\n",
      "1625 [Loss: 0.100250]\n",
      "1626 [Loss: 0.104372]\n",
      "1627 [Loss: 0.106776]\n",
      "1628 [Loss: 0.105444]\n",
      "1629 [Loss: 0.102366]\n",
      "1630 [Loss: 0.110742]\n",
      "1631 [Loss: 0.101937]\n",
      "1632 [Loss: 0.109369]\n",
      "1633 [Loss: 0.109404]\n",
      "1634 [Loss: 0.108641]\n",
      "1635 [Loss: 0.102174]\n",
      "1636 [Loss: 0.108223]\n",
      "1637 [Loss: 0.104021]\n",
      "1638 [Loss: 0.107392]\n",
      "1639 [Loss: 0.111416]\n",
      "1640 [Loss: 0.108209]\n",
      "1641 [Loss: 0.100081]\n",
      "1642 [Loss: 0.111983]\n",
      "1643 [Loss: 0.111076]\n",
      "1644 [Loss: 0.103041]\n",
      "1645 [Loss: 0.102036]\n",
      "1646 [Loss: 0.102029]\n",
      "1647 [Loss: 0.107904]\n",
      "1648 [Loss: 0.104801]\n",
      "1649 [Loss: 0.110758]\n",
      "1650 [Loss: 0.106919]\n",
      "1651 [Loss: 0.102091]\n",
      "1652 [Loss: 0.102268]\n",
      "1653 [Loss: 0.107071]\n",
      "1654 [Loss: 0.106820]\n",
      "1655 [Loss: 0.097675]\n",
      "1656 [Loss: 0.106829]\n",
      "1657 [Loss: 0.108513]\n",
      "1658 [Loss: 0.105731]\n",
      "1659 [Loss: 0.107830]\n",
      "1660 [Loss: 0.104656]\n",
      "1661 [Loss: 0.113424]\n",
      "1662 [Loss: 0.105938]\n",
      "1663 [Loss: 0.109492]\n",
      "1664 [Loss: 0.110204]\n",
      "1665 [Loss: 0.110741]\n",
      "1666 [Loss: 0.097876]\n",
      "1667 [Loss: 0.110887]\n",
      "1668 [Loss: 0.097074]\n",
      "1669 [Loss: 0.104082]\n",
      "1670 [Loss: 0.104332]\n",
      "1671 [Loss: 0.107191]\n",
      "1672 [Loss: 0.106007]\n",
      "1673 [Loss: 0.111280]\n",
      "1674 [Loss: 0.106302]\n",
      "1675 [Loss: 0.107031]\n",
      "1676 [Loss: 0.110954]\n",
      "1677 [Loss: 0.106429]\n",
      "1678 [Loss: 0.100663]\n",
      "1679 [Loss: 0.094774]\n",
      "1680 [Loss: 0.104151]\n",
      "1681 [Loss: 0.108241]\n",
      "1682 [Loss: 0.109198]\n",
      "1683 [Loss: 0.104676]\n",
      "1684 [Loss: 0.112170]\n",
      "1685 [Loss: 0.101728]\n",
      "1686 [Loss: 0.103127]\n",
      "1687 [Loss: 0.113568]\n",
      "1688 [Loss: 0.108804]\n",
      "1689 [Loss: 0.112286]\n",
      "1690 [Loss: 0.103931]\n",
      "1691 [Loss: 0.096774]\n",
      "1692 [Loss: 0.103770]\n",
      "1693 [Loss: 0.101060]\n",
      "1694 [Loss: 0.104419]\n",
      "1695 [Loss: 0.106578]\n",
      "1696 [Loss: 0.112556]\n",
      "1697 [Loss: 0.109365]\n",
      "1698 [Loss: 0.108901]\n",
      "1699 [Loss: 0.104218]\n",
      "1700 [Loss: 0.103349]\n",
      "1701 [Loss: 0.108849]\n",
      "1702 [Loss: 0.113892]\n",
      "1703 [Loss: 0.111889]\n",
      "1704 [Loss: 0.105042]\n",
      "1705 [Loss: 0.106733]\n",
      "1706 [Loss: 0.112538]\n",
      "1707 [Loss: 0.099158]\n",
      "1708 [Loss: 0.098095]\n",
      "1709 [Loss: 0.103413]\n",
      "1710 [Loss: 0.099858]\n",
      "1711 [Loss: 0.100048]\n",
      "1712 [Loss: 0.110220]\n",
      "1713 [Loss: 0.096927]\n",
      "1714 [Loss: 0.101028]\n",
      "1715 [Loss: 0.094899]\n",
      "1716 [Loss: 0.113746]\n",
      "1717 [Loss: 0.114285]\n",
      "1718 [Loss: 0.105293]\n",
      "1719 [Loss: 0.099510]\n",
      "1720 [Loss: 0.107084]\n",
      "1721 [Loss: 0.103626]\n",
      "1722 [Loss: 0.106164]\n",
      "1723 [Loss: 0.105799]\n",
      "1724 [Loss: 0.116254]\n",
      "1725 [Loss: 0.113164]\n",
      "1726 [Loss: 0.112392]\n",
      "1727 [Loss: 0.103538]\n",
      "1728 [Loss: 0.107701]\n",
      "1729 [Loss: 0.098683]\n",
      "1730 [Loss: 0.106107]\n",
      "1731 [Loss: 0.107028]\n",
      "1732 [Loss: 0.110599]\n",
      "1733 [Loss: 0.099869]\n",
      "1734 [Loss: 0.102546]\n",
      "1735 [Loss: 0.101357]\n",
      "1736 [Loss: 0.101776]\n",
      "1737 [Loss: 0.101169]\n",
      "1738 [Loss: 0.100422]\n",
      "1739 [Loss: 0.104928]\n",
      "1740 [Loss: 0.103503]\n",
      "1741 [Loss: 0.100645]\n",
      "1742 [Loss: 0.097861]\n",
      "1743 [Loss: 0.100408]\n",
      "1744 [Loss: 0.109436]\n",
      "1745 [Loss: 0.102832]\n",
      "1746 [Loss: 0.101048]\n",
      "1747 [Loss: 0.102162]\n",
      "1748 [Loss: 0.113924]\n",
      "1749 [Loss: 0.102157]\n",
      "1750 [Loss: 0.101894]\n",
      "1751 [Loss: 0.103233]\n",
      "1752 [Loss: 0.106516]\n",
      "1753 [Loss: 0.109626]\n",
      "1754 [Loss: 0.105070]\n",
      "1755 [Loss: 0.104515]\n",
      "1756 [Loss: 0.104262]\n",
      "1757 [Loss: 0.101542]\n",
      "1758 [Loss: 0.100864]\n",
      "1759 [Loss: 0.105183]\n",
      "1760 [Loss: 0.102613]\n",
      "1761 [Loss: 0.099003]\n",
      "1762 [Loss: 0.105480]\n",
      "1763 [Loss: 0.109991]\n",
      "1764 [Loss: 0.106087]\n",
      "1765 [Loss: 0.102356]\n",
      "1766 [Loss: 0.112041]\n",
      "1767 [Loss: 0.103992]\n",
      "1768 [Loss: 0.110874]\n",
      "1769 [Loss: 0.100685]\n",
      "1770 [Loss: 0.101096]\n",
      "1771 [Loss: 0.103215]\n",
      "1772 [Loss: 0.101941]\n",
      "1773 [Loss: 0.109774]\n",
      "1774 [Loss: 0.101711]\n",
      "1775 [Loss: 0.109240]\n",
      "1776 [Loss: 0.101546]\n",
      "1777 [Loss: 0.106059]\n",
      "1778 [Loss: 0.104147]\n",
      "1779 [Loss: 0.104390]\n",
      "1780 [Loss: 0.104465]\n",
      "1781 [Loss: 0.098453]\n",
      "1782 [Loss: 0.099088]\n",
      "1783 [Loss: 0.106391]\n",
      "1784 [Loss: 0.103701]\n",
      "1785 [Loss: 0.100913]\n",
      "1786 [Loss: 0.101017]\n",
      "1787 [Loss: 0.106727]\n",
      "1788 [Loss: 0.099956]\n",
      "1789 [Loss: 0.107899]\n",
      "1790 [Loss: 0.103525]\n",
      "1791 [Loss: 0.109682]\n",
      "1792 [Loss: 0.103192]\n",
      "1793 [Loss: 0.106256]\n",
      "1794 [Loss: 0.103182]\n",
      "1795 [Loss: 0.107680]\n",
      "1796 [Loss: 0.099662]\n",
      "1797 [Loss: 0.110839]\n",
      "1798 [Loss: 0.105731]\n",
      "1799 [Loss: 0.106833]\n",
      "1800 [Loss: 0.101529]\n",
      "1801 [Loss: 0.104723]\n",
      "1802 [Loss: 0.100384]\n",
      "1803 [Loss: 0.094605]\n",
      "1804 [Loss: 0.098028]\n",
      "1805 [Loss: 0.104819]\n",
      "1806 [Loss: 0.106439]\n",
      "1807 [Loss: 0.102888]\n",
      "1808 [Loss: 0.107870]\n",
      "1809 [Loss: 0.103928]\n",
      "1810 [Loss: 0.107922]\n",
      "1811 [Loss: 0.107148]\n",
      "1812 [Loss: 0.101949]\n",
      "1813 [Loss: 0.100517]\n",
      "1814 [Loss: 0.099071]\n",
      "1815 [Loss: 0.106273]\n",
      "1816 [Loss: 0.103625]\n",
      "1817 [Loss: 0.103638]\n",
      "1818 [Loss: 0.099464]\n",
      "1819 [Loss: 0.106421]\n",
      "1820 [Loss: 0.102214]\n",
      "1821 [Loss: 0.105203]\n",
      "1822 [Loss: 0.096598]\n",
      "1823 [Loss: 0.104556]\n",
      "1824 [Loss: 0.100189]\n",
      "1825 [Loss: 0.108757]\n",
      "1826 [Loss: 0.101533]\n",
      "1827 [Loss: 0.105504]\n",
      "1828 [Loss: 0.103310]\n",
      "1829 [Loss: 0.100915]\n",
      "1830 [Loss: 0.105189]\n",
      "1831 [Loss: 0.107182]\n",
      "1832 [Loss: 0.103541]\n",
      "1833 [Loss: 0.098301]\n",
      "1834 [Loss: 0.104654]\n",
      "1835 [Loss: 0.101903]\n",
      "1836 [Loss: 0.103871]\n",
      "1837 [Loss: 0.101457]\n",
      "1838 [Loss: 0.098177]\n",
      "1839 [Loss: 0.101801]\n",
      "1840 [Loss: 0.100940]\n",
      "1841 [Loss: 0.104887]\n",
      "1842 [Loss: 0.117158]\n",
      "1843 [Loss: 0.104823]\n",
      "1844 [Loss: 0.100043]\n",
      "1845 [Loss: 0.100953]\n",
      "1846 [Loss: 0.106036]\n",
      "1847 [Loss: 0.100712]\n",
      "1848 [Loss: 0.105780]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849 [Loss: 0.098260]\n",
      "1850 [Loss: 0.102976]\n",
      "1851 [Loss: 0.097567]\n",
      "1852 [Loss: 0.099043]\n",
      "1853 [Loss: 0.103026]\n",
      "1854 [Loss: 0.102825]\n",
      "1855 [Loss: 0.102406]\n",
      "1856 [Loss: 0.099752]\n",
      "1857 [Loss: 0.106741]\n",
      "1858 [Loss: 0.116904]\n",
      "1859 [Loss: 0.107901]\n",
      "1860 [Loss: 0.104293]\n",
      "1861 [Loss: 0.106150]\n",
      "1862 [Loss: 0.105923]\n",
      "1863 [Loss: 0.106207]\n",
      "1864 [Loss: 0.103184]\n",
      "1865 [Loss: 0.101279]\n",
      "1866 [Loss: 0.108295]\n",
      "1867 [Loss: 0.110235]\n",
      "1868 [Loss: 0.104553]\n",
      "1869 [Loss: 0.095272]\n",
      "1870 [Loss: 0.101080]\n",
      "1871 [Loss: 0.100368]\n",
      "1872 [Loss: 0.100539]\n",
      "1873 [Loss: 0.105428]\n",
      "1874 [Loss: 0.099845]\n",
      "1875 [Loss: 0.099757]\n",
      "1876 [Loss: 0.107932]\n",
      "1877 [Loss: 0.103001]\n",
      "1878 [Loss: 0.100839]\n",
      "1879 [Loss: 0.109172]\n",
      "1880 [Loss: 0.097056]\n",
      "1881 [Loss: 0.104441]\n",
      "1882 [Loss: 0.106293]\n",
      "1883 [Loss: 0.100897]\n",
      "1884 [Loss: 0.103442]\n",
      "1885 [Loss: 0.105250]\n",
      "1886 [Loss: 0.101976]\n",
      "1887 [Loss: 0.096208]\n",
      "1888 [Loss: 0.100270]\n",
      "1889 [Loss: 0.102165]\n",
      "1890 [Loss: 0.108957]\n",
      "1891 [Loss: 0.102218]\n",
      "1892 [Loss: 0.100224]\n",
      "1893 [Loss: 0.103901]\n",
      "1894 [Loss: 0.098587]\n",
      "1895 [Loss: 0.100117]\n",
      "1896 [Loss: 0.109758]\n",
      "1897 [Loss: 0.106170]\n",
      "1898 [Loss: 0.101567]\n",
      "1899 [Loss: 0.100383]\n",
      "1900 [Loss: 0.105408]\n",
      "1901 [Loss: 0.105404]\n",
      "1902 [Loss: 0.102091]\n",
      "1903 [Loss: 0.100134]\n",
      "1904 [Loss: 0.097950]\n",
      "1905 [Loss: 0.102457]\n",
      "1906 [Loss: 0.106244]\n",
      "1907 [Loss: 0.102615]\n",
      "1908 [Loss: 0.101576]\n",
      "1909 [Loss: 0.099712]\n",
      "1910 [Loss: 0.097537]\n",
      "1911 [Loss: 0.098848]\n",
      "1912 [Loss: 0.104635]\n",
      "1913 [Loss: 0.109126]\n",
      "1914 [Loss: 0.108250]\n",
      "1915 [Loss: 0.102950]\n",
      "1916 [Loss: 0.103254]\n",
      "1917 [Loss: 0.098019]\n",
      "1918 [Loss: 0.092853]\n",
      "1919 [Loss: 0.103935]\n",
      "1920 [Loss: 0.100232]\n",
      "1921 [Loss: 0.104807]\n",
      "1922 [Loss: 0.103558]\n",
      "1923 [Loss: 0.110203]\n",
      "1924 [Loss: 0.109497]\n",
      "1925 [Loss: 0.108978]\n",
      "1926 [Loss: 0.102034]\n",
      "1927 [Loss: 0.110603]\n",
      "1928 [Loss: 0.103879]\n",
      "1929 [Loss: 0.109289]\n",
      "1930 [Loss: 0.098067]\n",
      "1931 [Loss: 0.097670]\n",
      "1932 [Loss: 0.108855]\n",
      "1933 [Loss: 0.105359]\n",
      "1934 [Loss: 0.110335]\n",
      "1935 [Loss: 0.108688]\n",
      "1936 [Loss: 0.101984]\n",
      "1937 [Loss: 0.099900]\n",
      "1938 [Loss: 0.100654]\n",
      "1939 [Loss: 0.099000]\n",
      "1940 [Loss: 0.102178]\n",
      "1941 [Loss: 0.103218]\n",
      "1942 [Loss: 0.094048]\n",
      "1943 [Loss: 0.104806]\n",
      "1944 [Loss: 0.098341]\n",
      "1945 [Loss: 0.107133]\n",
      "1946 [Loss: 0.101523]\n",
      "1947 [Loss: 0.107216]\n",
      "1948 [Loss: 0.100364]\n",
      "1949 [Loss: 0.102961]\n",
      "1950 [Loss: 0.099155]\n",
      "1951 [Loss: 0.097063]\n",
      "1952 [Loss: 0.103225]\n",
      "1953 [Loss: 0.105860]\n",
      "1954 [Loss: 0.104221]\n",
      "1955 [Loss: 0.097713]\n",
      "1956 [Loss: 0.098839]\n",
      "1957 [Loss: 0.106836]\n",
      "1958 [Loss: 0.105892]\n",
      "1959 [Loss: 0.093498]\n",
      "1960 [Loss: 0.102835]\n",
      "1961 [Loss: 0.100665]\n",
      "1962 [Loss: 0.099740]\n",
      "1963 [Loss: 0.108580]\n",
      "1964 [Loss: 0.103458]\n",
      "1965 [Loss: 0.098527]\n",
      "1966 [Loss: 0.102159]\n",
      "1967 [Loss: 0.106737]\n",
      "1968 [Loss: 0.106354]\n",
      "1969 [Loss: 0.101906]\n",
      "1970 [Loss: 0.106854]\n",
      "1971 [Loss: 0.095891]\n",
      "1972 [Loss: 0.101033]\n",
      "1973 [Loss: 0.098232]\n",
      "1974 [Loss: 0.099666]\n",
      "1975 [Loss: 0.098207]\n",
      "1976 [Loss: 0.100288]\n",
      "1977 [Loss: 0.102331]\n",
      "1978 [Loss: 0.098732]\n",
      "1979 [Loss: 0.107453]\n",
      "1980 [Loss: 0.103294]\n",
      "1981 [Loss: 0.101483]\n",
      "1982 [Loss: 0.101311]\n",
      "1983 [Loss: 0.095389]\n",
      "1984 [Loss: 0.099353]\n",
      "1985 [Loss: 0.099317]\n",
      "1986 [Loss: 0.101155]\n",
      "1987 [Loss: 0.100436]\n",
      "1988 [Loss: 0.100271]\n",
      "1989 [Loss: 0.098033]\n",
      "1990 [Loss: 0.108033]\n",
      "1991 [Loss: 0.098471]\n",
      "1992 [Loss: 0.098787]\n",
      "1993 [Loss: 0.100215]\n",
      "1994 [Loss: 0.111563]\n",
      "1995 [Loss: 0.109453]\n",
      "1996 [Loss: 0.102777]\n",
      "1997 [Loss: 0.097827]\n",
      "1998 [Loss: 0.101249]\n",
      "1999 [Loss: 0.101718]\n",
      "2000 [Loss: 0.104520]\n",
      "2001 [Loss: 0.100354]\n",
      "2002 [Loss: 0.093424]\n",
      "2003 [Loss: 0.103266]\n",
      "2004 [Loss: 0.099960]\n",
      "2005 [Loss: 0.107327]\n",
      "2006 [Loss: 0.102790]\n",
      "2007 [Loss: 0.101139]\n",
      "2008 [Loss: 0.099447]\n",
      "2009 [Loss: 0.104226]\n",
      "2010 [Loss: 0.099827]\n",
      "2011 [Loss: 0.102506]\n",
      "2012 [Loss: 0.105755]\n",
      "2013 [Loss: 0.104644]\n",
      "2014 [Loss: 0.093893]\n",
      "2015 [Loss: 0.101415]\n",
      "2016 [Loss: 0.099998]\n",
      "2017 [Loss: 0.104961]\n",
      "2018 [Loss: 0.099870]\n",
      "2019 [Loss: 0.097383]\n",
      "2020 [Loss: 0.097859]\n",
      "2021 [Loss: 0.104947]\n",
      "2022 [Loss: 0.102559]\n",
      "2023 [Loss: 0.095675]\n",
      "2024 [Loss: 0.104781]\n",
      "2025 [Loss: 0.101108]\n",
      "2026 [Loss: 0.100549]\n",
      "2027 [Loss: 0.109486]\n",
      "2028 [Loss: 0.095576]\n",
      "2029 [Loss: 0.098282]\n",
      "2030 [Loss: 0.104707]\n",
      "2031 [Loss: 0.103229]\n",
      "2032 [Loss: 0.106025]\n",
      "2033 [Loss: 0.098166]\n",
      "2034 [Loss: 0.106135]\n",
      "2035 [Loss: 0.099171]\n",
      "2036 [Loss: 0.108605]\n",
      "2037 [Loss: 0.095327]\n",
      "2038 [Loss: 0.107218]\n",
      "2039 [Loss: 0.103248]\n",
      "2040 [Loss: 0.106914]\n",
      "2041 [Loss: 0.104603]\n",
      "2042 [Loss: 0.102595]\n",
      "2043 [Loss: 0.102956]\n",
      "2044 [Loss: 0.102797]\n",
      "2045 [Loss: 0.094313]\n",
      "2046 [Loss: 0.102753]\n",
      "2047 [Loss: 0.102528]\n",
      "2048 [Loss: 0.108999]\n",
      "2049 [Loss: 0.098603]\n",
      "2050 [Loss: 0.097735]\n",
      "2051 [Loss: 0.104142]\n",
      "2052 [Loss: 0.104895]\n",
      "2053 [Loss: 0.103272]\n",
      "2054 [Loss: 0.098207]\n",
      "2055 [Loss: 0.099215]\n",
      "2056 [Loss: 0.101318]\n",
      "2057 [Loss: 0.094529]\n",
      "2058 [Loss: 0.100660]\n",
      "2059 [Loss: 0.102610]\n",
      "2060 [Loss: 0.099749]\n",
      "2061 [Loss: 0.103772]\n",
      "2062 [Loss: 0.104311]\n",
      "2063 [Loss: 0.106169]\n",
      "2064 [Loss: 0.095826]\n",
      "2065 [Loss: 0.096474]\n",
      "2066 [Loss: 0.105494]\n",
      "2067 [Loss: 0.099052]\n",
      "2068 [Loss: 0.099534]\n",
      "2069 [Loss: 0.096980]\n",
      "2070 [Loss: 0.098079]\n",
      "2071 [Loss: 0.107578]\n",
      "2072 [Loss: 0.100096]\n",
      "2073 [Loss: 0.099693]\n",
      "2074 [Loss: 0.098490]\n",
      "2075 [Loss: 0.104984]\n",
      "2076 [Loss: 0.102820]\n",
      "2077 [Loss: 0.099220]\n",
      "2078 [Loss: 0.096265]\n",
      "2079 [Loss: 0.103679]\n",
      "2080 [Loss: 0.107111]\n",
      "2081 [Loss: 0.100498]\n",
      "2082 [Loss: 0.101211]\n",
      "2083 [Loss: 0.103985]\n",
      "2084 [Loss: 0.107786]\n",
      "2085 [Loss: 0.096545]\n",
      "2086 [Loss: 0.106522]\n",
      "2087 [Loss: 0.098478]\n",
      "2088 [Loss: 0.095381]\n",
      "2089 [Loss: 0.099407]\n",
      "2090 [Loss: 0.097512]\n",
      "2091 [Loss: 0.100147]\n",
      "2092 [Loss: 0.102010]\n",
      "2093 [Loss: 0.098215]\n",
      "2094 [Loss: 0.104250]\n",
      "2095 [Loss: 0.109751]\n",
      "2096 [Loss: 0.090270]\n",
      "2097 [Loss: 0.101255]\n",
      "2098 [Loss: 0.098247]\n",
      "2099 [Loss: 0.102518]\n",
      "2100 [Loss: 0.101783]\n",
      "2101 [Loss: 0.102022]\n",
      "2102 [Loss: 0.097611]\n",
      "2103 [Loss: 0.103374]\n",
      "2104 [Loss: 0.105548]\n",
      "2105 [Loss: 0.099225]\n",
      "2106 [Loss: 0.103588]\n",
      "2107 [Loss: 0.103227]\n",
      "2108 [Loss: 0.097090]\n",
      "2109 [Loss: 0.097345]\n",
      "2110 [Loss: 0.103678]\n",
      "2111 [Loss: 0.105986]\n",
      "2112 [Loss: 0.096061]\n",
      "2113 [Loss: 0.102368]\n",
      "2114 [Loss: 0.102479]\n",
      "2115 [Loss: 0.101377]\n",
      "2116 [Loss: 0.104187]\n",
      "2117 [Loss: 0.101702]\n",
      "2118 [Loss: 0.103801]\n",
      "2119 [Loss: 0.096221]\n",
      "2120 [Loss: 0.110075]\n",
      "2121 [Loss: 0.102520]\n",
      "2122 [Loss: 0.102542]\n",
      "2123 [Loss: 0.098185]\n",
      "2124 [Loss: 0.103384]\n",
      "2125 [Loss: 0.100537]\n",
      "2126 [Loss: 0.095346]\n",
      "2127 [Loss: 0.097655]\n",
      "2128 [Loss: 0.100057]\n",
      "2129 [Loss: 0.101459]\n",
      "2130 [Loss: 0.103836]\n",
      "2131 [Loss: 0.102245]\n",
      "2132 [Loss: 0.098115]\n",
      "2133 [Loss: 0.099196]\n",
      "2134 [Loss: 0.102908]\n",
      "2135 [Loss: 0.097892]\n",
      "2136 [Loss: 0.101054]\n",
      "2137 [Loss: 0.095722]\n",
      "2138 [Loss: 0.100219]\n",
      "2139 [Loss: 0.104256]\n",
      "2140 [Loss: 0.095832]\n",
      "2141 [Loss: 0.095170]\n",
      "2142 [Loss: 0.104094]\n",
      "2143 [Loss: 0.103062]\n",
      "2144 [Loss: 0.103625]\n",
      "2145 [Loss: 0.105655]\n",
      "2146 [Loss: 0.105095]\n",
      "2147 [Loss: 0.097323]\n",
      "2148 [Loss: 0.093808]\n",
      "2149 [Loss: 0.108541]\n",
      "2150 [Loss: 0.098682]\n",
      "2151 [Loss: 0.099164]\n",
      "2152 [Loss: 0.096611]\n",
      "2153 [Loss: 0.097768]\n",
      "2154 [Loss: 0.095342]\n",
      "2155 [Loss: 0.100078]\n",
      "2156 [Loss: 0.104976]\n",
      "2157 [Loss: 0.104491]\n",
      "2158 [Loss: 0.107379]\n",
      "2159 [Loss: 0.094573]\n",
      "2160 [Loss: 0.097775]\n",
      "2161 [Loss: 0.096122]\n",
      "2162 [Loss: 0.102958]\n",
      "2163 [Loss: 0.099964]\n",
      "2164 [Loss: 0.105189]\n",
      "2165 [Loss: 0.094584]\n",
      "2166 [Loss: 0.104757]\n",
      "2167 [Loss: 0.101000]\n",
      "2168 [Loss: 0.100354]\n",
      "2169 [Loss: 0.102877]\n",
      "2170 [Loss: 0.105471]\n",
      "2171 [Loss: 0.100157]\n",
      "2172 [Loss: 0.097897]\n",
      "2173 [Loss: 0.097850]\n",
      "2174 [Loss: 0.108254]\n",
      "2175 [Loss: 0.106696]\n",
      "2176 [Loss: 0.102626]\n",
      "2177 [Loss: 0.100717]\n",
      "2178 [Loss: 0.102301]\n",
      "2179 [Loss: 0.099986]\n",
      "2180 [Loss: 0.105929]\n",
      "2181 [Loss: 0.106345]\n",
      "2182 [Loss: 0.099817]\n",
      "2183 [Loss: 0.104279]\n",
      "2184 [Loss: 0.104251]\n",
      "2185 [Loss: 0.103802]\n",
      "2186 [Loss: 0.103357]\n",
      "2187 [Loss: 0.087097]\n",
      "2188 [Loss: 0.100642]\n",
      "2189 [Loss: 0.099924]\n",
      "2190 [Loss: 0.104076]\n",
      "2191 [Loss: 0.099662]\n",
      "2192 [Loss: 0.099975]\n",
      "2193 [Loss: 0.098728]\n",
      "2194 [Loss: 0.102786]\n",
      "2195 [Loss: 0.097127]\n",
      "2196 [Loss: 0.095844]\n",
      "2197 [Loss: 0.099478]\n",
      "2198 [Loss: 0.103499]\n",
      "2199 [Loss: 0.097358]\n",
      "2200 [Loss: 0.105948]\n",
      "2201 [Loss: 0.102291]\n",
      "2202 [Loss: 0.094370]\n",
      "2203 [Loss: 0.094313]\n",
      "2204 [Loss: 0.100861]\n",
      "2205 [Loss: 0.107015]\n",
      "2206 [Loss: 0.096397]\n",
      "2207 [Loss: 0.102198]\n",
      "2208 [Loss: 0.106382]\n",
      "2209 [Loss: 0.105374]\n",
      "2210 [Loss: 0.103255]\n",
      "2211 [Loss: 0.100838]\n",
      "2212 [Loss: 0.098953]\n",
      "2213 [Loss: 0.096483]\n",
      "2214 [Loss: 0.101118]\n",
      "2215 [Loss: 0.105886]\n",
      "2216 [Loss: 0.105567]\n",
      "2217 [Loss: 0.102719]\n",
      "2218 [Loss: 0.095315]\n",
      "2219 [Loss: 0.102460]\n",
      "2220 [Loss: 0.097292]\n",
      "2221 [Loss: 0.096920]\n",
      "2222 [Loss: 0.100541]\n",
      "2223 [Loss: 0.097715]\n",
      "2224 [Loss: 0.098889]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2225 [Loss: 0.100544]\n",
      "2226 [Loss: 0.104079]\n",
      "2227 [Loss: 0.108714]\n",
      "2228 [Loss: 0.101805]\n",
      "2229 [Loss: 0.103179]\n",
      "2230 [Loss: 0.100215]\n",
      "2231 [Loss: 0.108365]\n",
      "2232 [Loss: 0.108440]\n",
      "2233 [Loss: 0.101893]\n",
      "2234 [Loss: 0.103247]\n",
      "2235 [Loss: 0.102488]\n",
      "2236 [Loss: 0.096919]\n",
      "2237 [Loss: 0.103002]\n",
      "2238 [Loss: 0.108367]\n",
      "2239 [Loss: 0.096639]\n",
      "2240 [Loss: 0.101767]\n",
      "2241 [Loss: 0.096670]\n",
      "2242 [Loss: 0.095047]\n",
      "2243 [Loss: 0.098805]\n",
      "2244 [Loss: 0.096693]\n",
      "2245 [Loss: 0.102555]\n",
      "2246 [Loss: 0.102926]\n",
      "2247 [Loss: 0.094559]\n",
      "2248 [Loss: 0.099698]\n",
      "2249 [Loss: 0.096779]\n",
      "2250 [Loss: 0.103604]\n",
      "2251 [Loss: 0.098885]\n",
      "2252 [Loss: 0.099107]\n",
      "2253 [Loss: 0.102415]\n",
      "2254 [Loss: 0.099436]\n",
      "2255 [Loss: 0.098435]\n",
      "2256 [Loss: 0.101688]\n",
      "2257 [Loss: 0.102761]\n",
      "2258 [Loss: 0.100647]\n",
      "2259 [Loss: 0.104845]\n",
      "2260 [Loss: 0.098717]\n",
      "2261 [Loss: 0.100288]\n",
      "2262 [Loss: 0.098885]\n",
      "2263 [Loss: 0.096543]\n",
      "2264 [Loss: 0.106548]\n",
      "2265 [Loss: 0.103460]\n",
      "2266 [Loss: 0.104976]\n",
      "2267 [Loss: 0.100070]\n",
      "2268 [Loss: 0.097412]\n",
      "2269 [Loss: 0.104453]\n",
      "2270 [Loss: 0.103272]\n",
      "2271 [Loss: 0.097825]\n",
      "2272 [Loss: 0.102980]\n",
      "2273 [Loss: 0.100900]\n",
      "2274 [Loss: 0.094837]\n",
      "2275 [Loss: 0.096816]\n",
      "2276 [Loss: 0.101707]\n",
      "2277 [Loss: 0.096015]\n",
      "2278 [Loss: 0.094872]\n",
      "2279 [Loss: 0.099700]\n",
      "2280 [Loss: 0.098818]\n",
      "2281 [Loss: 0.102678]\n",
      "2282 [Loss: 0.103277]\n",
      "2283 [Loss: 0.100784]\n",
      "2284 [Loss: 0.097643]\n",
      "2285 [Loss: 0.096844]\n",
      "2286 [Loss: 0.110794]\n",
      "2287 [Loss: 0.102796]\n",
      "2288 [Loss: 0.103736]\n",
      "2289 [Loss: 0.090109]\n",
      "2290 [Loss: 0.101493]\n",
      "2291 [Loss: 0.095172]\n",
      "2292 [Loss: 0.097896]\n",
      "2293 [Loss: 0.097732]\n",
      "2294 [Loss: 0.099609]\n",
      "2295 [Loss: 0.103111]\n",
      "2296 [Loss: 0.103520]\n",
      "2297 [Loss: 0.093410]\n",
      "2298 [Loss: 0.105134]\n",
      "2299 [Loss: 0.102704]\n",
      "2300 [Loss: 0.105975]\n",
      "2301 [Loss: 0.100437]\n",
      "2302 [Loss: 0.099742]\n",
      "2303 [Loss: 0.105640]\n",
      "2304 [Loss: 0.097893]\n",
      "2305 [Loss: 0.093522]\n",
      "2306 [Loss: 0.098098]\n",
      "2307 [Loss: 0.096728]\n",
      "2308 [Loss: 0.099398]\n",
      "2309 [Loss: 0.099450]\n",
      "2310 [Loss: 0.100569]\n",
      "2311 [Loss: 0.098015]\n",
      "2312 [Loss: 0.098487]\n",
      "2313 [Loss: 0.102085]\n",
      "2314 [Loss: 0.098889]\n",
      "2315 [Loss: 0.102464]\n",
      "2316 [Loss: 0.100992]\n",
      "2317 [Loss: 0.098526]\n",
      "2318 [Loss: 0.099267]\n",
      "2319 [Loss: 0.097410]\n",
      "2320 [Loss: 0.108536]\n",
      "2321 [Loss: 0.103841]\n",
      "2322 [Loss: 0.096407]\n",
      "2323 [Loss: 0.104640]\n",
      "2324 [Loss: 0.101823]\n",
      "2325 [Loss: 0.095058]\n",
      "2326 [Loss: 0.099859]\n",
      "2327 [Loss: 0.104249]\n",
      "2328 [Loss: 0.095265]\n",
      "2329 [Loss: 0.098791]\n",
      "2330 [Loss: 0.096777]\n",
      "2331 [Loss: 0.101292]\n",
      "2332 [Loss: 0.098921]\n",
      "2333 [Loss: 0.103557]\n",
      "2334 [Loss: 0.100658]\n",
      "2335 [Loss: 0.094089]\n",
      "2336 [Loss: 0.101650]\n",
      "2337 [Loss: 0.099166]\n",
      "2338 [Loss: 0.100804]\n",
      "2339 [Loss: 0.098893]\n",
      "2340 [Loss: 0.097119]\n",
      "2341 [Loss: 0.097646]\n",
      "2342 [Loss: 0.091705]\n",
      "2343 [Loss: 0.098918]\n",
      "2344 [Loss: 0.102423]\n",
      "2345 [Loss: 0.098119]\n",
      "2346 [Loss: 0.105338]\n",
      "2347 [Loss: 0.095351]\n",
      "2348 [Loss: 0.099766]\n",
      "2349 [Loss: 0.103878]\n",
      "2350 [Loss: 0.104986]\n",
      "2351 [Loss: 0.098954]\n",
      "2352 [Loss: 0.097458]\n",
      "2353 [Loss: 0.099581]\n",
      "2354 [Loss: 0.101575]\n",
      "2355 [Loss: 0.096625]\n",
      "2356 [Loss: 0.099516]\n",
      "2357 [Loss: 0.105264]\n",
      "2358 [Loss: 0.095131]\n",
      "2359 [Loss: 0.095893]\n",
      "2360 [Loss: 0.094994]\n",
      "2361 [Loss: 0.098806]\n",
      "2362 [Loss: 0.100522]\n",
      "2363 [Loss: 0.096741]\n",
      "2364 [Loss: 0.097321]\n",
      "2365 [Loss: 0.097469]\n",
      "2366 [Loss: 0.089874]\n",
      "2367 [Loss: 0.102846]\n",
      "2368 [Loss: 0.116621]\n",
      "2369 [Loss: 0.104687]\n",
      "2370 [Loss: 0.099887]\n",
      "2371 [Loss: 0.097640]\n",
      "2372 [Loss: 0.093765]\n",
      "2373 [Loss: 0.097098]\n",
      "2374 [Loss: 0.101720]\n",
      "2375 [Loss: 0.106598]\n",
      "2376 [Loss: 0.098149]\n",
      "2377 [Loss: 0.093681]\n",
      "2378 [Loss: 0.097988]\n",
      "2379 [Loss: 0.105013]\n",
      "2380 [Loss: 0.106859]\n",
      "2381 [Loss: 0.099206]\n",
      "2382 [Loss: 0.100971]\n",
      "2383 [Loss: 0.099830]\n",
      "2384 [Loss: 0.098535]\n",
      "2385 [Loss: 0.090852]\n",
      "2386 [Loss: 0.105217]\n",
      "2387 [Loss: 0.098558]\n",
      "2388 [Loss: 0.103084]\n",
      "2389 [Loss: 0.098852]\n",
      "2390 [Loss: 0.102171]\n",
      "2391 [Loss: 0.100344]\n",
      "2392 [Loss: 0.102385]\n",
      "2393 [Loss: 0.089365]\n",
      "2394 [Loss: 0.094561]\n",
      "2395 [Loss: 0.093904]\n",
      "2396 [Loss: 0.095818]\n",
      "2397 [Loss: 0.094716]\n",
      "2398 [Loss: 0.096386]\n",
      "2399 [Loss: 0.097118]\n",
      "2400 [Loss: 0.095909]\n",
      "2401 [Loss: 0.091818]\n",
      "2402 [Loss: 0.096003]\n",
      "2403 [Loss: 0.098220]\n",
      "2404 [Loss: 0.098788]\n",
      "2405 [Loss: 0.099955]\n",
      "2406 [Loss: 0.096335]\n",
      "2407 [Loss: 0.096491]\n",
      "2408 [Loss: 0.098629]\n",
      "2409 [Loss: 0.094605]\n",
      "2410 [Loss: 0.102694]\n",
      "2411 [Loss: 0.100401]\n",
      "2412 [Loss: 0.100273]\n",
      "2413 [Loss: 0.094973]\n",
      "2414 [Loss: 0.102638]\n",
      "2415 [Loss: 0.098514]\n",
      "2416 [Loss: 0.099711]\n",
      "2417 [Loss: 0.095535]\n",
      "2418 [Loss: 0.097625]\n",
      "2419 [Loss: 0.096696]\n",
      "2420 [Loss: 0.098165]\n",
      "2421 [Loss: 0.095101]\n",
      "2422 [Loss: 0.094774]\n",
      "2423 [Loss: 0.104562]\n",
      "2424 [Loss: 0.099096]\n",
      "2425 [Loss: 0.104769]\n",
      "2426 [Loss: 0.096939]\n",
      "2427 [Loss: 0.095117]\n",
      "2428 [Loss: 0.097194]\n",
      "2429 [Loss: 0.101018]\n",
      "2430 [Loss: 0.098296]\n",
      "2431 [Loss: 0.097240]\n",
      "2432 [Loss: 0.098591]\n",
      "2433 [Loss: 0.104502]\n",
      "2434 [Loss: 0.101920]\n",
      "2435 [Loss: 0.100277]\n",
      "2436 [Loss: 0.102630]\n",
      "2437 [Loss: 0.097040]\n",
      "2438 [Loss: 0.107898]\n",
      "2439 [Loss: 0.102431]\n",
      "2440 [Loss: 0.096852]\n",
      "2441 [Loss: 0.092754]\n",
      "2442 [Loss: 0.099434]\n",
      "2443 [Loss: 0.100948]\n",
      "2444 [Loss: 0.096765]\n",
      "2445 [Loss: 0.095417]\n",
      "2446 [Loss: 0.104772]\n",
      "2447 [Loss: 0.103609]\n",
      "2448 [Loss: 0.096201]\n",
      "2449 [Loss: 0.098887]\n",
      "2450 [Loss: 0.097949]\n",
      "2451 [Loss: 0.093119]\n",
      "2452 [Loss: 0.099764]\n",
      "2453 [Loss: 0.107308]\n",
      "2454 [Loss: 0.097287]\n",
      "2455 [Loss: 0.096262]\n",
      "2456 [Loss: 0.103419]\n",
      "2457 [Loss: 0.096677]\n",
      "2458 [Loss: 0.095264]\n",
      "2459 [Loss: 0.095892]\n",
      "2460 [Loss: 0.102620]\n",
      "2461 [Loss: 0.097343]\n",
      "2462 [Loss: 0.100021]\n",
      "2463 [Loss: 0.099167]\n",
      "2464 [Loss: 0.099619]\n",
      "2465 [Loss: 0.094564]\n",
      "2466 [Loss: 0.096090]\n",
      "2467 [Loss: 0.097111]\n",
      "2468 [Loss: 0.099802]\n",
      "2469 [Loss: 0.101389]\n",
      "2470 [Loss: 0.098647]\n",
      "2471 [Loss: 0.097648]\n",
      "2472 [Loss: 0.097696]\n",
      "2473 [Loss: 0.097516]\n",
      "2474 [Loss: 0.098103]\n",
      "2475 [Loss: 0.093760]\n",
      "2476 [Loss: 0.096955]\n",
      "2477 [Loss: 0.102383]\n",
      "2478 [Loss: 0.097033]\n",
      "2479 [Loss: 0.103384]\n",
      "2480 [Loss: 0.109951]\n",
      "2481 [Loss: 0.099198]\n",
      "2482 [Loss: 0.098942]\n",
      "2483 [Loss: 0.099362]\n",
      "2484 [Loss: 0.098521]\n",
      "2485 [Loss: 0.101070]\n",
      "2486 [Loss: 0.100384]\n",
      "2487 [Loss: 0.094195]\n",
      "2488 [Loss: 0.098372]\n",
      "2489 [Loss: 0.101439]\n",
      "2490 [Loss: 0.097148]\n",
      "2491 [Loss: 0.099287]\n",
      "2492 [Loss: 0.097012]\n",
      "2493 [Loss: 0.098784]\n",
      "2494 [Loss: 0.091735]\n",
      "2495 [Loss: 0.096424]\n",
      "2496 [Loss: 0.097620]\n",
      "2497 [Loss: 0.100470]\n",
      "2498 [Loss: 0.103597]\n",
      "2499 [Loss: 0.097612]\n",
      "2500 [Loss: 0.101209]\n",
      "2501 [Loss: 0.098638]\n",
      "2502 [Loss: 0.100508]\n",
      "2503 [Loss: 0.101398]\n",
      "2504 [Loss: 0.094502]\n",
      "2505 [Loss: 0.095567]\n",
      "2506 [Loss: 0.100909]\n",
      "2507 [Loss: 0.094202]\n",
      "2508 [Loss: 0.089619]\n",
      "2509 [Loss: 0.100194]\n",
      "2510 [Loss: 0.098280]\n",
      "2511 [Loss: 0.099639]\n",
      "2512 [Loss: 0.104508]\n",
      "2513 [Loss: 0.098562]\n",
      "2514 [Loss: 0.097895]\n",
      "2515 [Loss: 0.099685]\n",
      "2516 [Loss: 0.091474]\n",
      "2517 [Loss: 0.097317]\n",
      "2518 [Loss: 0.101174]\n",
      "2519 [Loss: 0.100914]\n",
      "2520 [Loss: 0.092208]\n",
      "2521 [Loss: 0.099403]\n",
      "2522 [Loss: 0.103948]\n",
      "2523 [Loss: 0.097853]\n",
      "2524 [Loss: 0.099301]\n",
      "2525 [Loss: 0.106853]\n",
      "2526 [Loss: 0.103264]\n",
      "2527 [Loss: 0.098704]\n",
      "2528 [Loss: 0.103940]\n",
      "2529 [Loss: 0.095816]\n",
      "2530 [Loss: 0.089509]\n",
      "2531 [Loss: 0.092509]\n",
      "2532 [Loss: 0.088341]\n",
      "2533 [Loss: 0.101284]\n",
      "2534 [Loss: 0.095287]\n",
      "2535 [Loss: 0.099909]\n",
      "2536 [Loss: 0.103245]\n",
      "2537 [Loss: 0.100558]\n",
      "2538 [Loss: 0.103036]\n",
      "2539 [Loss: 0.099237]\n",
      "2540 [Loss: 0.100943]\n",
      "2541 [Loss: 0.099122]\n",
      "2542 [Loss: 0.100834]\n",
      "2543 [Loss: 0.098203]\n",
      "2544 [Loss: 0.101867]\n",
      "2545 [Loss: 0.091199]\n",
      "2546 [Loss: 0.101693]\n",
      "2547 [Loss: 0.096614]\n",
      "2548 [Loss: 0.100390]\n",
      "2549 [Loss: 0.097467]\n",
      "2550 [Loss: 0.100434]\n",
      "2551 [Loss: 0.102411]\n",
      "2552 [Loss: 0.097101]\n",
      "2553 [Loss: 0.098273]\n",
      "2554 [Loss: 0.098067]\n",
      "2555 [Loss: 0.102636]\n",
      "2556 [Loss: 0.104265]\n",
      "2557 [Loss: 0.098894]\n",
      "2558 [Loss: 0.096118]\n",
      "2559 [Loss: 0.100237]\n",
      "2560 [Loss: 0.097832]\n",
      "2561 [Loss: 0.095405]\n",
      "2562 [Loss: 0.090562]\n",
      "2563 [Loss: 0.097948]\n",
      "2564 [Loss: 0.092639]\n",
      "2565 [Loss: 0.101809]\n",
      "2566 [Loss: 0.096568]\n",
      "2567 [Loss: 0.099476]\n",
      "2568 [Loss: 0.096774]\n",
      "2569 [Loss: 0.099921]\n",
      "2570 [Loss: 0.091924]\n",
      "2571 [Loss: 0.094156]\n",
      "2572 [Loss: 0.101259]\n",
      "2573 [Loss: 0.101595]\n",
      "2574 [Loss: 0.095358]\n",
      "2575 [Loss: 0.096757]\n",
      "2576 [Loss: 0.100678]\n",
      "2577 [Loss: 0.099333]\n",
      "2578 [Loss: 0.102003]\n",
      "2579 [Loss: 0.088830]\n",
      "2580 [Loss: 0.097874]\n",
      "2581 [Loss: 0.099019]\n",
      "2582 [Loss: 0.098704]\n",
      "2583 [Loss: 0.099899]\n",
      "2584 [Loss: 0.104296]\n",
      "2585 [Loss: 0.097360]\n",
      "2586 [Loss: 0.100581]\n",
      "2587 [Loss: 0.099764]\n",
      "2588 [Loss: 0.097831]\n",
      "2589 [Loss: 0.101016]\n",
      "2590 [Loss: 0.096858]\n",
      "2591 [Loss: 0.092098]\n",
      "2592 [Loss: 0.095245]\n",
      "2593 [Loss: 0.101381]\n",
      "2594 [Loss: 0.097840]\n",
      "2595 [Loss: 0.095672]\n",
      "2596 [Loss: 0.092126]\n",
      "2597 [Loss: 0.097352]\n",
      "2598 [Loss: 0.100242]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2599 [Loss: 0.098797]\n",
      "2600 [Loss: 0.099169]\n",
      "2601 [Loss: 0.096854]\n",
      "2602 [Loss: 0.102289]\n",
      "2603 [Loss: 0.103043]\n",
      "2604 [Loss: 0.100830]\n",
      "2605 [Loss: 0.098417]\n",
      "2606 [Loss: 0.103238]\n",
      "2607 [Loss: 0.099014]\n",
      "2608 [Loss: 0.099559]\n",
      "2609 [Loss: 0.100816]\n",
      "2610 [Loss: 0.098785]\n",
      "2611 [Loss: 0.095517]\n",
      "2612 [Loss: 0.098302]\n",
      "2613 [Loss: 0.098985]\n",
      "2614 [Loss: 0.097986]\n",
      "2615 [Loss: 0.101501]\n",
      "2616 [Loss: 0.101522]\n",
      "2617 [Loss: 0.102186]\n",
      "2618 [Loss: 0.090959]\n",
      "2619 [Loss: 0.093666]\n",
      "2620 [Loss: 0.099284]\n",
      "2621 [Loss: 0.094286]\n",
      "2622 [Loss: 0.094155]\n",
      "2623 [Loss: 0.099738]\n",
      "2624 [Loss: 0.098743]\n",
      "2625 [Loss: 0.105550]\n",
      "2626 [Loss: 0.095100]\n",
      "2627 [Loss: 0.101707]\n",
      "2628 [Loss: 0.088887]\n",
      "2629 [Loss: 0.099210]\n",
      "2630 [Loss: 0.101064]\n",
      "2631 [Loss: 0.097033]\n",
      "2632 [Loss: 0.099014]\n",
      "2633 [Loss: 0.105256]\n",
      "2634 [Loss: 0.093334]\n",
      "2635 [Loss: 0.094554]\n",
      "2636 [Loss: 0.099187]\n",
      "2637 [Loss: 0.096456]\n",
      "2638 [Loss: 0.095615]\n",
      "2639 [Loss: 0.094803]\n",
      "2640 [Loss: 0.103120]\n",
      "2641 [Loss: 0.101729]\n",
      "2642 [Loss: 0.098310]\n",
      "2643 [Loss: 0.095733]\n",
      "2644 [Loss: 0.096958]\n",
      "2645 [Loss: 0.095682]\n",
      "2646 [Loss: 0.102220]\n",
      "2647 [Loss: 0.096547]\n",
      "2648 [Loss: 0.096630]\n",
      "2649 [Loss: 0.104037]\n",
      "2650 [Loss: 0.095211]\n",
      "2651 [Loss: 0.102143]\n",
      "2652 [Loss: 0.095186]\n",
      "2653 [Loss: 0.098742]\n",
      "2654 [Loss: 0.096204]\n",
      "2655 [Loss: 0.096934]\n",
      "2656 [Loss: 0.096525]\n",
      "2657 [Loss: 0.100626]\n",
      "2658 [Loss: 0.106512]\n",
      "2659 [Loss: 0.102402]\n",
      "2660 [Loss: 0.090196]\n",
      "2661 [Loss: 0.098360]\n",
      "2662 [Loss: 0.093054]\n",
      "2663 [Loss: 0.100895]\n",
      "2664 [Loss: 0.103337]\n",
      "2665 [Loss: 0.097418]\n",
      "2666 [Loss: 0.096987]\n",
      "2667 [Loss: 0.097403]\n",
      "2668 [Loss: 0.103030]\n",
      "2669 [Loss: 0.097820]\n",
      "2670 [Loss: 0.091543]\n",
      "2671 [Loss: 0.099971]\n",
      "2672 [Loss: 0.105724]\n",
      "2673 [Loss: 0.104239]\n",
      "2674 [Loss: 0.095401]\n",
      "2675 [Loss: 0.099631]\n",
      "2676 [Loss: 0.101353]\n",
      "2677 [Loss: 0.096916]\n",
      "2678 [Loss: 0.099039]\n",
      "2679 [Loss: 0.089487]\n",
      "2680 [Loss: 0.100526]\n",
      "2681 [Loss: 0.102230]\n",
      "2682 [Loss: 0.094737]\n",
      "2683 [Loss: 0.099467]\n",
      "2684 [Loss: 0.096238]\n",
      "2685 [Loss: 0.094095]\n",
      "2686 [Loss: 0.097484]\n",
      "2687 [Loss: 0.096762]\n",
      "2688 [Loss: 0.097685]\n",
      "2689 [Loss: 0.093528]\n",
      "2690 [Loss: 0.097910]\n",
      "2691 [Loss: 0.099100]\n",
      "2692 [Loss: 0.094892]\n",
      "2693 [Loss: 0.097671]\n",
      "2694 [Loss: 0.100765]\n",
      "2695 [Loss: 0.095998]\n",
      "2696 [Loss: 0.098521]\n",
      "2697 [Loss: 0.092297]\n",
      "2698 [Loss: 0.099052]\n",
      "2699 [Loss: 0.096838]\n",
      "2700 [Loss: 0.094882]\n",
      "2701 [Loss: 0.096571]\n",
      "2702 [Loss: 0.097720]\n",
      "2703 [Loss: 0.103058]\n",
      "2704 [Loss: 0.094502]\n",
      "2705 [Loss: 0.095021]\n",
      "2706 [Loss: 0.095368]\n",
      "2707 [Loss: 0.092661]\n",
      "2708 [Loss: 0.098144]\n",
      "2709 [Loss: 0.097413]\n",
      "2710 [Loss: 0.095510]\n",
      "2711 [Loss: 0.094599]\n",
      "2712 [Loss: 0.095097]\n",
      "2713 [Loss: 0.097464]\n",
      "2714 [Loss: 0.099383]\n",
      "2715 [Loss: 0.101872]\n",
      "2716 [Loss: 0.090604]\n",
      "2717 [Loss: 0.097669]\n",
      "2718 [Loss: 0.103430]\n",
      "2719 [Loss: 0.102135]\n",
      "2720 [Loss: 0.095083]\n",
      "2721 [Loss: 0.096523]\n",
      "2722 [Loss: 0.097615]\n",
      "2723 [Loss: 0.096079]\n",
      "2724 [Loss: 0.096608]\n",
      "2725 [Loss: 0.094790]\n",
      "2726 [Loss: 0.096876]\n",
      "2727 [Loss: 0.097033]\n",
      "2728 [Loss: 0.100511]\n",
      "2729 [Loss: 0.098501]\n",
      "2730 [Loss: 0.096424]\n",
      "2731 [Loss: 0.100572]\n",
      "2732 [Loss: 0.094387]\n",
      "2733 [Loss: 0.096742]\n",
      "2734 [Loss: 0.099594]\n",
      "2735 [Loss: 0.099980]\n",
      "2736 [Loss: 0.098384]\n",
      "2737 [Loss: 0.091693]\n",
      "2738 [Loss: 0.098560]\n",
      "2739 [Loss: 0.100927]\n",
      "2740 [Loss: 0.097725]\n",
      "2741 [Loss: 0.100075]\n",
      "2742 [Loss: 0.103038]\n",
      "2743 [Loss: 0.102467]\n",
      "2744 [Loss: 0.099170]\n",
      "2745 [Loss: 0.103606]\n",
      "2746 [Loss: 0.095173]\n",
      "2747 [Loss: 0.090753]\n",
      "2748 [Loss: 0.098326]\n",
      "2749 [Loss: 0.095298]\n",
      "2750 [Loss: 0.098154]\n",
      "2751 [Loss: 0.098843]\n",
      "2752 [Loss: 0.093539]\n",
      "2753 [Loss: 0.104397]\n",
      "2754 [Loss: 0.089901]\n",
      "2755 [Loss: 0.093900]\n",
      "2756 [Loss: 0.095867]\n",
      "2757 [Loss: 0.093990]\n",
      "2758 [Loss: 0.094457]\n",
      "2759 [Loss: 0.104773]\n",
      "2760 [Loss: 0.096006]\n",
      "2761 [Loss: 0.095527]\n",
      "2762 [Loss: 0.095833]\n",
      "2763 [Loss: 0.094871]\n",
      "2764 [Loss: 0.096851]\n",
      "2765 [Loss: 0.099583]\n",
      "2766 [Loss: 0.090734]\n",
      "2767 [Loss: 0.095410]\n",
      "2768 [Loss: 0.094777]\n",
      "2769 [Loss: 0.091556]\n",
      "2770 [Loss: 0.102633]\n",
      "2771 [Loss: 0.106957]\n",
      "2772 [Loss: 0.096509]\n",
      "2773 [Loss: 0.094992]\n",
      "2774 [Loss: 0.096870]\n",
      "2775 [Loss: 0.097381]\n",
      "2776 [Loss: 0.096059]\n",
      "2777 [Loss: 0.094403]\n",
      "2778 [Loss: 0.097430]\n",
      "2779 [Loss: 0.095708]\n",
      "2780 [Loss: 0.092533]\n",
      "2781 [Loss: 0.098618]\n",
      "2782 [Loss: 0.094600]\n",
      "2783 [Loss: 0.096517]\n",
      "2784 [Loss: 0.101464]\n",
      "2785 [Loss: 0.091076]\n",
      "2786 [Loss: 0.096109]\n",
      "2787 [Loss: 0.099170]\n",
      "2788 [Loss: 0.099542]\n",
      "2789 [Loss: 0.096450]\n",
      "2790 [Loss: 0.098965]\n",
      "2791 [Loss: 0.097314]\n",
      "2792 [Loss: 0.097860]\n",
      "2793 [Loss: 0.104381]\n",
      "2794 [Loss: 0.094722]\n",
      "2795 [Loss: 0.095425]\n",
      "2796 [Loss: 0.093284]\n",
      "2797 [Loss: 0.096906]\n",
      "2798 [Loss: 0.092317]\n",
      "2799 [Loss: 0.096346]\n",
      "2800 [Loss: 0.095427]\n",
      "2801 [Loss: 0.097523]\n",
      "2802 [Loss: 0.105613]\n",
      "2803 [Loss: 0.099495]\n",
      "2804 [Loss: 0.094780]\n",
      "2805 [Loss: 0.095198]\n",
      "2806 [Loss: 0.099299]\n",
      "2807 [Loss: 0.099931]\n",
      "2808 [Loss: 0.097844]\n",
      "2809 [Loss: 0.097938]\n",
      "2810 [Loss: 0.092581]\n",
      "2811 [Loss: 0.100484]\n",
      "2812 [Loss: 0.101098]\n",
      "2813 [Loss: 0.101413]\n",
      "2814 [Loss: 0.100434]\n",
      "2815 [Loss: 0.100305]\n",
      "2816 [Loss: 0.100185]\n",
      "2817 [Loss: 0.093708]\n",
      "2818 [Loss: 0.088888]\n",
      "2819 [Loss: 0.091417]\n",
      "2820 [Loss: 0.096397]\n",
      "2821 [Loss: 0.095104]\n",
      "2822 [Loss: 0.093788]\n",
      "2823 [Loss: 0.093651]\n",
      "2824 [Loss: 0.091507]\n",
      "2825 [Loss: 0.089503]\n",
      "2826 [Loss: 0.093867]\n",
      "2827 [Loss: 0.096576]\n",
      "2828 [Loss: 0.097563]\n",
      "2829 [Loss: 0.096808]\n",
      "2830 [Loss: 0.097444]\n",
      "2831 [Loss: 0.097056]\n",
      "2832 [Loss: 0.096353]\n",
      "2833 [Loss: 0.092423]\n",
      "2834 [Loss: 0.098804]\n",
      "2835 [Loss: 0.094966]\n",
      "2836 [Loss: 0.101115]\n",
      "2837 [Loss: 0.098848]\n",
      "2838 [Loss: 0.100233]\n",
      "2839 [Loss: 0.098214]\n",
      "2840 [Loss: 0.098822]\n",
      "2841 [Loss: 0.098332]\n",
      "2842 [Loss: 0.093692]\n",
      "2843 [Loss: 0.098810]\n",
      "2844 [Loss: 0.092341]\n",
      "2845 [Loss: 0.101119]\n",
      "2846 [Loss: 0.102682]\n",
      "2847 [Loss: 0.095902]\n",
      "2848 [Loss: 0.099241]\n",
      "2849 [Loss: 0.100294]\n",
      "2850 [Loss: 0.092541]\n",
      "2851 [Loss: 0.097710]\n",
      "2852 [Loss: 0.097154]\n",
      "2853 [Loss: 0.104648]\n",
      "2854 [Loss: 0.102318]\n",
      "2855 [Loss: 0.104852]\n",
      "2856 [Loss: 0.100250]\n",
      "2857 [Loss: 0.102247]\n",
      "2858 [Loss: 0.096330]\n",
      "2859 [Loss: 0.095248]\n",
      "2860 [Loss: 0.095773]\n",
      "2861 [Loss: 0.091560]\n",
      "2862 [Loss: 0.101012]\n",
      "2863 [Loss: 0.092328]\n",
      "2864 [Loss: 0.088339]\n",
      "2865 [Loss: 0.096146]\n",
      "2866 [Loss: 0.101407]\n",
      "2867 [Loss: 0.098177]\n",
      "2868 [Loss: 0.100273]\n",
      "2869 [Loss: 0.096574]\n",
      "2870 [Loss: 0.097929]\n",
      "2871 [Loss: 0.096630]\n",
      "2872 [Loss: 0.097674]\n",
      "2873 [Loss: 0.103274]\n",
      "2874 [Loss: 0.095241]\n",
      "2875 [Loss: 0.093056]\n",
      "2876 [Loss: 0.091292]\n",
      "2877 [Loss: 0.096252]\n",
      "2878 [Loss: 0.091943]\n",
      "2879 [Loss: 0.099663]\n",
      "2880 [Loss: 0.104333]\n",
      "2881 [Loss: 0.096261]\n",
      "2882 [Loss: 0.098538]\n",
      "2883 [Loss: 0.087817]\n",
      "2884 [Loss: 0.094031]\n",
      "2885 [Loss: 0.094797]\n",
      "2886 [Loss: 0.092823]\n",
      "2887 [Loss: 0.097685]\n",
      "2888 [Loss: 0.095300]\n",
      "2889 [Loss: 0.090448]\n",
      "2890 [Loss: 0.095202]\n",
      "2891 [Loss: 0.099258]\n",
      "2892 [Loss: 0.097797]\n",
      "2893 [Loss: 0.096618]\n",
      "2894 [Loss: 0.102299]\n",
      "2895 [Loss: 0.094791]\n",
      "2896 [Loss: 0.094616]\n",
      "2897 [Loss: 0.097236]\n",
      "2898 [Loss: 0.095600]\n",
      "2899 [Loss: 0.095520]\n",
      "2900 [Loss: 0.096957]\n",
      "2901 [Loss: 0.096367]\n",
      "2902 [Loss: 0.095455]\n",
      "2903 [Loss: 0.100172]\n",
      "2904 [Loss: 0.094469]\n",
      "2905 [Loss: 0.098306]\n",
      "2906 [Loss: 0.098093]\n",
      "2907 [Loss: 0.096472]\n",
      "2908 [Loss: 0.092832]\n",
      "2909 [Loss: 0.095911]\n",
      "2910 [Loss: 0.101601]\n",
      "2911 [Loss: 0.098502]\n",
      "2912 [Loss: 0.089423]\n",
      "2913 [Loss: 0.097942]\n",
      "2914 [Loss: 0.094355]\n",
      "2915 [Loss: 0.092045]\n",
      "2916 [Loss: 0.093649]\n",
      "2917 [Loss: 0.094421]\n",
      "2918 [Loss: 0.098312]\n",
      "2919 [Loss: 0.095846]\n",
      "2920 [Loss: 0.097850]\n",
      "2921 [Loss: 0.095593]\n",
      "2922 [Loss: 0.100627]\n",
      "2923 [Loss: 0.089551]\n",
      "2924 [Loss: 0.103661]\n",
      "2925 [Loss: 0.096142]\n",
      "2926 [Loss: 0.095573]\n",
      "2927 [Loss: 0.094014]\n",
      "2928 [Loss: 0.094576]\n",
      "2929 [Loss: 0.094079]\n",
      "2930 [Loss: 0.096452]\n",
      "2931 [Loss: 0.096656]\n",
      "2932 [Loss: 0.099930]\n",
      "2933 [Loss: 0.099548]\n",
      "2934 [Loss: 0.092408]\n",
      "2935 [Loss: 0.094376]\n",
      "2936 [Loss: 0.095736]\n",
      "2937 [Loss: 0.100642]\n",
      "2938 [Loss: 0.097121]\n",
      "2939 [Loss: 0.099444]\n",
      "2940 [Loss: 0.097984]\n",
      "2941 [Loss: 0.092202]\n",
      "2942 [Loss: 0.095729]\n",
      "2943 [Loss: 0.095094]\n",
      "2944 [Loss: 0.094221]\n",
      "2945 [Loss: 0.094817]\n",
      "2946 [Loss: 0.096620]\n",
      "2947 [Loss: 0.097387]\n",
      "2948 [Loss: 0.098426]\n",
      "2949 [Loss: 0.105643]\n",
      "2950 [Loss: 0.092252]\n",
      "2951 [Loss: 0.093105]\n",
      "2952 [Loss: 0.094347]\n",
      "2953 [Loss: 0.097234]\n",
      "2954 [Loss: 0.098328]\n",
      "2955 [Loss: 0.101157]\n",
      "2956 [Loss: 0.093794]\n",
      "2957 [Loss: 0.100714]\n",
      "2958 [Loss: 0.102050]\n",
      "2959 [Loss: 0.092656]\n",
      "2960 [Loss: 0.096839]\n",
      "2961 [Loss: 0.101609]\n",
      "2962 [Loss: 0.098383]\n",
      "2963 [Loss: 0.099742]\n",
      "2964 [Loss: 0.094670]\n",
      "2965 [Loss: 0.097060]\n",
      "2966 [Loss: 0.093090]\n",
      "2967 [Loss: 0.096465]\n",
      "2968 [Loss: 0.100380]\n",
      "2969 [Loss: 0.098281]\n",
      "2970 [Loss: 0.098275]\n",
      "2971 [Loss: 0.096407]\n",
      "2972 [Loss: 0.096674]\n",
      "2973 [Loss: 0.095671]\n",
      "2974 [Loss: 0.097445]\n",
      "2975 [Loss: 0.095595]\n",
      "2976 [Loss: 0.100318]\n",
      "2977 [Loss: 0.098901]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2978 [Loss: 0.101094]\n",
      "2979 [Loss: 0.096126]\n",
      "2980 [Loss: 0.096415]\n",
      "2981 [Loss: 0.093437]\n",
      "2982 [Loss: 0.091709]\n",
      "2983 [Loss: 0.094970]\n",
      "2984 [Loss: 0.091047]\n",
      "2985 [Loss: 0.093318]\n",
      "2986 [Loss: 0.095861]\n",
      "2987 [Loss: 0.088555]\n",
      "2988 [Loss: 0.088598]\n",
      "2989 [Loss: 0.096005]\n",
      "2990 [Loss: 0.099772]\n",
      "2991 [Loss: 0.096776]\n",
      "2992 [Loss: 0.096118]\n",
      "2993 [Loss: 0.097303]\n",
      "2994 [Loss: 0.100839]\n",
      "2995 [Loss: 0.091224]\n",
      "2996 [Loss: 0.098480]\n",
      "2997 [Loss: 0.097321]\n",
      "2998 [Loss: 0.097830]\n",
      "2999 [Loss: 0.099260]\n",
      "3000 [Loss: 0.098280]\n",
      "3001 [Loss: 0.092412]\n",
      "3002 [Loss: 0.092850]\n",
      "3003 [Loss: 0.095777]\n",
      "3004 [Loss: 0.096238]\n",
      "3005 [Loss: 0.099124]\n",
      "3006 [Loss: 0.094268]\n",
      "3007 [Loss: 0.097802]\n",
      "3008 [Loss: 0.091880]\n",
      "3009 [Loss: 0.088796]\n",
      "3010 [Loss: 0.098985]\n",
      "3011 [Loss: 0.095231]\n",
      "3012 [Loss: 0.098590]\n",
      "3013 [Loss: 0.099127]\n",
      "3014 [Loss: 0.093845]\n",
      "3015 [Loss: 0.096544]\n",
      "3016 [Loss: 0.097511]\n",
      "3017 [Loss: 0.096726]\n",
      "3018 [Loss: 0.096843]\n",
      "3019 [Loss: 0.092672]\n",
      "3020 [Loss: 0.091941]\n",
      "3021 [Loss: 0.093273]\n",
      "3022 [Loss: 0.093265]\n",
      "3023 [Loss: 0.094990]\n",
      "3024 [Loss: 0.099187]\n",
      "3025 [Loss: 0.101507]\n",
      "3026 [Loss: 0.094712]\n",
      "3027 [Loss: 0.098931]\n",
      "3028 [Loss: 0.096162]\n",
      "3029 [Loss: 0.096171]\n",
      "3030 [Loss: 0.095142]\n",
      "3031 [Loss: 0.094418]\n",
      "3032 [Loss: 0.096814]\n",
      "3033 [Loss: 0.091595]\n",
      "3034 [Loss: 0.098181]\n",
      "3035 [Loss: 0.100472]\n",
      "3036 [Loss: 0.096916]\n",
      "3037 [Loss: 0.097261]\n",
      "3038 [Loss: 0.101136]\n",
      "3039 [Loss: 0.096735]\n",
      "3040 [Loss: 0.095886]\n",
      "3041 [Loss: 0.091838]\n",
      "3042 [Loss: 0.090779]\n",
      "3043 [Loss: 0.091949]\n",
      "3044 [Loss: 0.095256]\n",
      "3045 [Loss: 0.098083]\n",
      "3046 [Loss: 0.089536]\n",
      "3047 [Loss: 0.092540]\n",
      "3048 [Loss: 0.094347]\n",
      "3049 [Loss: 0.092206]\n",
      "3050 [Loss: 0.097427]\n",
      "3051 [Loss: 0.097058]\n",
      "3052 [Loss: 0.096481]\n",
      "3053 [Loss: 0.095551]\n",
      "3054 [Loss: 0.098899]\n",
      "3055 [Loss: 0.095874]\n",
      "3056 [Loss: 0.095987]\n",
      "3057 [Loss: 0.090249]\n",
      "3058 [Loss: 0.093500]\n",
      "3059 [Loss: 0.100101]\n",
      "3060 [Loss: 0.100779]\n",
      "3061 [Loss: 0.097818]\n",
      "3062 [Loss: 0.095168]\n",
      "3063 [Loss: 0.093726]\n",
      "3064 [Loss: 0.096300]\n",
      "3065 [Loss: 0.091391]\n",
      "3066 [Loss: 0.091701]\n",
      "3067 [Loss: 0.093451]\n",
      "3068 [Loss: 0.094647]\n",
      "3069 [Loss: 0.090440]\n",
      "3070 [Loss: 0.090375]\n",
      "3071 [Loss: 0.092463]\n",
      "3072 [Loss: 0.093203]\n",
      "3073 [Loss: 0.092694]\n",
      "3074 [Loss: 0.091541]\n",
      "3075 [Loss: 0.101609]\n",
      "3076 [Loss: 0.093513]\n",
      "3077 [Loss: 0.093704]\n",
      "3078 [Loss: 0.096382]\n",
      "3079 [Loss: 0.095408]\n",
      "3080 [Loss: 0.099093]\n",
      "3081 [Loss: 0.098057]\n",
      "3082 [Loss: 0.097064]\n",
      "3083 [Loss: 0.090094]\n",
      "3084 [Loss: 0.094204]\n",
      "3085 [Loss: 0.090467]\n",
      "3086 [Loss: 0.097915]\n",
      "3087 [Loss: 0.095906]\n",
      "3088 [Loss: 0.099435]\n",
      "3089 [Loss: 0.096447]\n",
      "3090 [Loss: 0.094421]\n",
      "3091 [Loss: 0.090220]\n",
      "3092 [Loss: 0.101618]\n",
      "3093 [Loss: 0.094034]\n",
      "3094 [Loss: 0.099578]\n",
      "3095 [Loss: 0.100005]\n",
      "3096 [Loss: 0.096324]\n",
      "3097 [Loss: 0.100229]\n",
      "3098 [Loss: 0.097470]\n",
      "3099 [Loss: 0.086661]\n",
      "3100 [Loss: 0.097238]\n",
      "3101 [Loss: 0.098163]\n",
      "3102 [Loss: 0.095209]\n",
      "3103 [Loss: 0.090895]\n",
      "3104 [Loss: 0.101970]\n",
      "3105 [Loss: 0.093902]\n",
      "3106 [Loss: 0.092874]\n",
      "3107 [Loss: 0.097348]\n",
      "3108 [Loss: 0.092529]\n",
      "3109 [Loss: 0.102524]\n",
      "3110 [Loss: 0.098789]\n",
      "3111 [Loss: 0.094081]\n",
      "3112 [Loss: 0.095971]\n",
      "3113 [Loss: 0.093848]\n",
      "3114 [Loss: 0.090798]\n",
      "3115 [Loss: 0.097049]\n",
      "3116 [Loss: 0.093906]\n",
      "3117 [Loss: 0.099160]\n",
      "3118 [Loss: 0.098314]\n",
      "3119 [Loss: 0.094874]\n",
      "3120 [Loss: 0.097136]\n",
      "3121 [Loss: 0.092346]\n",
      "3122 [Loss: 0.093967]\n",
      "3123 [Loss: 0.094995]\n",
      "3124 [Loss: 0.092470]\n",
      "3125 [Loss: 0.096178]\n",
      "3126 [Loss: 0.096711]\n",
      "3127 [Loss: 0.091945]\n",
      "3128 [Loss: 0.095230]\n",
      "3129 [Loss: 0.100339]\n",
      "3130 [Loss: 0.097952]\n",
      "3131 [Loss: 0.095253]\n",
      "3132 [Loss: 0.094155]\n",
      "3133 [Loss: 0.091994]\n",
      "3134 [Loss: 0.092802]\n",
      "3135 [Loss: 0.096474]\n",
      "3136 [Loss: 0.092501]\n",
      "3137 [Loss: 0.091929]\n",
      "3138 [Loss: 0.093115]\n",
      "3139 [Loss: 0.098321]\n",
      "3140 [Loss: 0.090438]\n",
      "3141 [Loss: 0.089202]\n",
      "3142 [Loss: 0.095741]\n",
      "3143 [Loss: 0.102168]\n",
      "3144 [Loss: 0.096007]\n",
      "3145 [Loss: 0.092582]\n",
      "3146 [Loss: 0.097253]\n",
      "3147 [Loss: 0.098475]\n",
      "3148 [Loss: 0.102395]\n",
      "3149 [Loss: 0.095546]\n",
      "3150 [Loss: 0.095406]\n",
      "3151 [Loss: 0.092984]\n",
      "3152 [Loss: 0.088294]\n",
      "3153 [Loss: 0.096685]\n",
      "3154 [Loss: 0.095401]\n",
      "3155 [Loss: 0.098306]\n",
      "3156 [Loss: 0.089250]\n",
      "3157 [Loss: 0.092059]\n",
      "3158 [Loss: 0.094375]\n",
      "3159 [Loss: 0.096632]\n",
      "3160 [Loss: 0.093943]\n",
      "3161 [Loss: 0.092811]\n",
      "3162 [Loss: 0.094422]\n",
      "3163 [Loss: 0.099274]\n",
      "3164 [Loss: 0.089571]\n",
      "3165 [Loss: 0.097438]\n",
      "3166 [Loss: 0.094065]\n",
      "3167 [Loss: 0.095583]\n",
      "3168 [Loss: 0.091866]\n",
      "3169 [Loss: 0.093796]\n",
      "3170 [Loss: 0.100701]\n",
      "3171 [Loss: 0.091394]\n",
      "3172 [Loss: 0.089988]\n",
      "3173 [Loss: 0.096306]\n",
      "3174 [Loss: 0.095279]\n",
      "3175 [Loss: 0.100319]\n",
      "3176 [Loss: 0.100828]\n",
      "3177 [Loss: 0.092406]\n",
      "3178 [Loss: 0.101604]\n",
      "3179 [Loss: 0.099296]\n",
      "3180 [Loss: 0.098182]\n",
      "3181 [Loss: 0.088288]\n",
      "3182 [Loss: 0.097928]\n",
      "3183 [Loss: 0.095191]\n",
      "3184 [Loss: 0.099310]\n",
      "3185 [Loss: 0.092926]\n",
      "3186 [Loss: 0.085177]\n",
      "3187 [Loss: 0.098861]\n",
      "3188 [Loss: 0.095971]\n",
      "3189 [Loss: 0.092849]\n",
      "3190 [Loss: 0.091361]\n",
      "3191 [Loss: 0.094196]\n",
      "3192 [Loss: 0.091392]\n",
      "3193 [Loss: 0.098277]\n",
      "3194 [Loss: 0.092474]\n",
      "3195 [Loss: 0.092383]\n",
      "3196 [Loss: 0.100221]\n",
      "3197 [Loss: 0.098827]\n",
      "3198 [Loss: 0.091245]\n",
      "3199 [Loss: 0.093171]\n",
      "3200 [Loss: 0.096006]\n",
      "3201 [Loss: 0.090815]\n",
      "3202 [Loss: 0.089091]\n",
      "3203 [Loss: 0.094830]\n",
      "3204 [Loss: 0.096687]\n",
      "3205 [Loss: 0.099765]\n",
      "3206 [Loss: 0.098715]\n",
      "3207 [Loss: 0.096079]\n",
      "3208 [Loss: 0.088698]\n",
      "3209 [Loss: 0.100735]\n",
      "3210 [Loss: 0.099285]\n",
      "3211 [Loss: 0.096531]\n",
      "3212 [Loss: 0.094841]\n",
      "3213 [Loss: 0.102648]\n",
      "3214 [Loss: 0.097335]\n",
      "3215 [Loss: 0.095660]\n",
      "3216 [Loss: 0.099187]\n",
      "3217 [Loss: 0.095562]\n",
      "3218 [Loss: 0.095943]\n",
      "3219 [Loss: 0.094613]\n",
      "3220 [Loss: 0.095270]\n",
      "3221 [Loss: 0.096094]\n",
      "3222 [Loss: 0.094736]\n",
      "3223 [Loss: 0.097726]\n",
      "3224 [Loss: 0.098553]\n",
      "3225 [Loss: 0.095711]\n",
      "3226 [Loss: 0.094612]\n",
      "3227 [Loss: 0.094178]\n",
      "3228 [Loss: 0.091766]\n",
      "3229 [Loss: 0.094475]\n",
      "3230 [Loss: 0.095954]\n",
      "3231 [Loss: 0.094049]\n",
      "3232 [Loss: 0.098586]\n",
      "3233 [Loss: 0.097672]\n",
      "3234 [Loss: 0.092279]\n",
      "3235 [Loss: 0.100921]\n",
      "3236 [Loss: 0.097746]\n",
      "3237 [Loss: 0.093811]\n",
      "3238 [Loss: 0.100300]\n",
      "3239 [Loss: 0.095929]\n",
      "3240 [Loss: 0.093430]\n",
      "3241 [Loss: 0.097514]\n",
      "3242 [Loss: 0.098462]\n",
      "3243 [Loss: 0.092043]\n",
      "3244 [Loss: 0.099696]\n",
      "3245 [Loss: 0.096707]\n",
      "3246 [Loss: 0.090842]\n",
      "3247 [Loss: 0.092830]\n",
      "3248 [Loss: 0.096397]\n",
      "3249 [Loss: 0.092672]\n",
      "3250 [Loss: 0.092742]\n",
      "3251 [Loss: 0.096571]\n",
      "3252 [Loss: 0.091621]\n",
      "3253 [Loss: 0.093863]\n",
      "3254 [Loss: 0.090051]\n",
      "3255 [Loss: 0.093279]\n",
      "3256 [Loss: 0.098083]\n",
      "3257 [Loss: 0.093822]\n",
      "3258 [Loss: 0.097309]\n",
      "3259 [Loss: 0.096597]\n",
      "3260 [Loss: 0.096790]\n",
      "3261 [Loss: 0.092566]\n",
      "3262 [Loss: 0.098116]\n",
      "3263 [Loss: 0.095605]\n",
      "3264 [Loss: 0.092424]\n",
      "3265 [Loss: 0.099740]\n",
      "3266 [Loss: 0.101195]\n",
      "3267 [Loss: 0.102870]\n",
      "3268 [Loss: 0.099774]\n",
      "3269 [Loss: 0.095222]\n",
      "3270 [Loss: 0.096315]\n",
      "3271 [Loss: 0.096579]\n",
      "3272 [Loss: 0.092935]\n",
      "3273 [Loss: 0.100561]\n",
      "3274 [Loss: 0.099751]\n",
      "3275 [Loss: 0.091443]\n",
      "3276 [Loss: 0.093557]\n",
      "3277 [Loss: 0.090367]\n",
      "3278 [Loss: 0.094600]\n",
      "3279 [Loss: 0.092420]\n",
      "3280 [Loss: 0.099914]\n",
      "3281 [Loss: 0.093744]\n",
      "3282 [Loss: 0.100531]\n",
      "3283 [Loss: 0.098121]\n",
      "3284 [Loss: 0.101666]\n",
      "3285 [Loss: 0.094777]\n",
      "3286 [Loss: 0.095243]\n",
      "3287 [Loss: 0.091733]\n",
      "3288 [Loss: 0.091066]\n",
      "3289 [Loss: 0.097605]\n",
      "3290 [Loss: 0.095573]\n",
      "3291 [Loss: 0.091787]\n",
      "3292 [Loss: 0.093384]\n",
      "3293 [Loss: 0.096433]\n",
      "3294 [Loss: 0.096953]\n",
      "3295 [Loss: 0.098299]\n",
      "3296 [Loss: 0.096683]\n",
      "3297 [Loss: 0.094370]\n",
      "3298 [Loss: 0.096157]\n",
      "3299 [Loss: 0.100853]\n",
      "3300 [Loss: 0.097382]\n",
      "3301 [Loss: 0.094319]\n",
      "3302 [Loss: 0.092258]\n",
      "3303 [Loss: 0.095661]\n",
      "3304 [Loss: 0.095302]\n",
      "3305 [Loss: 0.098009]\n",
      "3306 [Loss: 0.097784]\n",
      "3307 [Loss: 0.096014]\n",
      "3308 [Loss: 0.099522]\n",
      "3309 [Loss: 0.091568]\n",
      "3310 [Loss: 0.095520]\n",
      "3311 [Loss: 0.094762]\n",
      "3312 [Loss: 0.087312]\n",
      "3313 [Loss: 0.100898]\n",
      "3314 [Loss: 0.088477]\n",
      "3315 [Loss: 0.094848]\n",
      "3316 [Loss: 0.096923]\n",
      "3317 [Loss: 0.094255]\n",
      "3318 [Loss: 0.093054]\n",
      "3319 [Loss: 0.096301]\n",
      "3320 [Loss: 0.090104]\n",
      "3321 [Loss: 0.096305]\n",
      "3322 [Loss: 0.096163]\n",
      "3323 [Loss: 0.095072]\n",
      "3324 [Loss: 0.088724]\n",
      "3325 [Loss: 0.089863]\n",
      "3326 [Loss: 0.094765]\n",
      "3327 [Loss: 0.085194]\n",
      "3328 [Loss: 0.095156]\n",
      "3329 [Loss: 0.101533]\n",
      "3330 [Loss: 0.101739]\n",
      "3331 [Loss: 0.096656]\n",
      "3332 [Loss: 0.092452]\n",
      "3333 [Loss: 0.091738]\n",
      "3334 [Loss: 0.099487]\n",
      "3335 [Loss: 0.094670]\n",
      "3336 [Loss: 0.097512]\n",
      "3337 [Loss: 0.098722]\n",
      "3338 [Loss: 0.096388]\n",
      "3339 [Loss: 0.094473]\n",
      "3340 [Loss: 0.098421]\n",
      "3341 [Loss: 0.097516]\n",
      "3342 [Loss: 0.099069]\n",
      "3343 [Loss: 0.100543]\n",
      "3344 [Loss: 0.090241]\n",
      "3345 [Loss: 0.090016]\n",
      "3346 [Loss: 0.093330]\n",
      "3347 [Loss: 0.097007]\n",
      "3348 [Loss: 0.096954]\n",
      "3349 [Loss: 0.097422]\n",
      "3350 [Loss: 0.096742]\n",
      "3351 [Loss: 0.099008]\n",
      "3352 [Loss: 0.097828]\n",
      "3353 [Loss: 0.092127]\n",
      "3354 [Loss: 0.098625]\n",
      "3355 [Loss: 0.094776]\n",
      "3356 [Loss: 0.089990]\n",
      "3357 [Loss: 0.096125]\n",
      "3358 [Loss: 0.098767]\n",
      "3359 [Loss: 0.090994]\n",
      "3360 [Loss: 0.091214]\n",
      "3361 [Loss: 0.097202]\n",
      "3362 [Loss: 0.099240]\n",
      "3363 [Loss: 0.097050]\n",
      "3364 [Loss: 0.094841]\n",
      "3365 [Loss: 0.094470]\n",
      "3366 [Loss: 0.097262]\n",
      "3367 [Loss: 0.096103]\n",
      "3368 [Loss: 0.092904]\n",
      "3369 [Loss: 0.091096]\n",
      "3370 [Loss: 0.095940]\n",
      "3371 [Loss: 0.096866]\n",
      "3372 [Loss: 0.096494]\n",
      "3373 [Loss: 0.096903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3374 [Loss: 0.092538]\n",
      "3375 [Loss: 0.090979]\n",
      "3376 [Loss: 0.095440]\n",
      "3377 [Loss: 0.100972]\n",
      "3378 [Loss: 0.098676]\n",
      "3379 [Loss: 0.098397]\n",
      "3380 [Loss: 0.103408]\n",
      "3381 [Loss: 0.087404]\n",
      "3382 [Loss: 0.100004]\n",
      "3383 [Loss: 0.093074]\n",
      "3384 [Loss: 0.098001]\n",
      "3385 [Loss: 0.089874]\n",
      "3386 [Loss: 0.096827]\n",
      "3387 [Loss: 0.094133]\n",
      "3388 [Loss: 0.091455]\n",
      "3389 [Loss: 0.091591]\n",
      "3390 [Loss: 0.095282]\n",
      "3391 [Loss: 0.097653]\n",
      "3392 [Loss: 0.101372]\n",
      "3393 [Loss: 0.102443]\n",
      "3394 [Loss: 0.094639]\n",
      "3395 [Loss: 0.092788]\n",
      "3396 [Loss: 0.092580]\n",
      "3397 [Loss: 0.093613]\n",
      "3398 [Loss: 0.096135]\n",
      "3399 [Loss: 0.097405]\n",
      "3400 [Loss: 0.089558]\n",
      "3401 [Loss: 0.097715]\n",
      "3402 [Loss: 0.097950]\n",
      "3403 [Loss: 0.096074]\n",
      "3404 [Loss: 0.095933]\n",
      "3405 [Loss: 0.089527]\n",
      "3406 [Loss: 0.092208]\n",
      "3407 [Loss: 0.088594]\n",
      "3408 [Loss: 0.092033]\n",
      "3409 [Loss: 0.095026]\n",
      "3410 [Loss: 0.087054]\n",
      "3411 [Loss: 0.098846]\n",
      "3412 [Loss: 0.097120]\n",
      "3413 [Loss: 0.093962]\n",
      "3414 [Loss: 0.085827]\n",
      "3415 [Loss: 0.096126]\n",
      "3416 [Loss: 0.091615]\n",
      "3417 [Loss: 0.095609]\n",
      "3418 [Loss: 0.089811]\n",
      "3419 [Loss: 0.091005]\n",
      "3420 [Loss: 0.095284]\n",
      "3421 [Loss: 0.102566]\n",
      "3422 [Loss: 0.094316]\n",
      "3423 [Loss: 0.093250]\n",
      "3424 [Loss: 0.097697]\n",
      "3425 [Loss: 0.090029]\n",
      "3426 [Loss: 0.099056]\n",
      "3427 [Loss: 0.093976]\n",
      "3428 [Loss: 0.093183]\n",
      "3429 [Loss: 0.093525]\n",
      "3430 [Loss: 0.098999]\n",
      "3431 [Loss: 0.094285]\n",
      "3432 [Loss: 0.093005]\n",
      "3433 [Loss: 0.098162]\n",
      "3434 [Loss: 0.094477]\n",
      "3435 [Loss: 0.097224]\n",
      "3436 [Loss: 0.099292]\n",
      "3437 [Loss: 0.091353]\n",
      "3438 [Loss: 0.090685]\n",
      "3439 [Loss: 0.099877]\n",
      "3440 [Loss: 0.093261]\n",
      "3441 [Loss: 0.090809]\n",
      "3442 [Loss: 0.095517]\n",
      "3443 [Loss: 0.099402]\n",
      "3444 [Loss: 0.093052]\n",
      "3445 [Loss: 0.093662]\n",
      "3446 [Loss: 0.094109]\n",
      "3447 [Loss: 0.094831]\n",
      "3448 [Loss: 0.097925]\n",
      "3449 [Loss: 0.096773]\n",
      "3450 [Loss: 0.101826]\n",
      "3451 [Loss: 0.103380]\n",
      "3452 [Loss: 0.097272]\n",
      "3453 [Loss: 0.091922]\n",
      "3454 [Loss: 0.092165]\n",
      "3455 [Loss: 0.092522]\n",
      "3456 [Loss: 0.095245]\n",
      "3457 [Loss: 0.095441]\n",
      "3458 [Loss: 0.089591]\n",
      "3459 [Loss: 0.093397]\n",
      "3460 [Loss: 0.095689]\n",
      "3461 [Loss: 0.086703]\n",
      "3462 [Loss: 0.090756]\n",
      "3463 [Loss: 0.091876]\n",
      "3464 [Loss: 0.089437]\n",
      "3465 [Loss: 0.096053]\n",
      "3466 [Loss: 0.095480]\n",
      "3467 [Loss: 0.094891]\n",
      "3468 [Loss: 0.095488]\n",
      "3469 [Loss: 0.089122]\n",
      "3470 [Loss: 0.094949]\n",
      "3471 [Loss: 0.094394]\n",
      "3472 [Loss: 0.091826]\n",
      "3473 [Loss: 0.088938]\n",
      "3474 [Loss: 0.095322]\n",
      "3475 [Loss: 0.096419]\n",
      "3476 [Loss: 0.089706]\n",
      "3477 [Loss: 0.091340]\n",
      "3478 [Loss: 0.092409]\n",
      "3479 [Loss: 0.089822]\n",
      "3480 [Loss: 0.096608]\n",
      "3481 [Loss: 0.093575]\n",
      "3482 [Loss: 0.097806]\n",
      "3483 [Loss: 0.094787]\n",
      "3484 [Loss: 0.095596]\n",
      "3485 [Loss: 0.096488]\n",
      "3486 [Loss: 0.094772]\n",
      "3487 [Loss: 0.096825]\n",
      "3488 [Loss: 0.096406]\n",
      "3489 [Loss: 0.099512]\n",
      "3490 [Loss: 0.101348]\n",
      "3491 [Loss: 0.097102]\n",
      "3492 [Loss: 0.098079]\n",
      "3493 [Loss: 0.091513]\n",
      "3494 [Loss: 0.097174]\n",
      "3495 [Loss: 0.091699]\n",
      "3496 [Loss: 0.094854]\n",
      "3497 [Loss: 0.093269]\n",
      "3498 [Loss: 0.091874]\n",
      "3499 [Loss: 0.096602]\n",
      "3500 [Loss: 0.097289]\n",
      "3501 [Loss: 0.101772]\n",
      "3502 [Loss: 0.091218]\n",
      "3503 [Loss: 0.094428]\n",
      "3504 [Loss: 0.090869]\n",
      "3505 [Loss: 0.100673]\n",
      "3506 [Loss: 0.097283]\n",
      "3507 [Loss: 0.095974]\n",
      "3508 [Loss: 0.094677]\n",
      "3509 [Loss: 0.096661]\n",
      "3510 [Loss: 0.089934]\n",
      "3511 [Loss: 0.092593]\n",
      "3512 [Loss: 0.097618]\n",
      "3513 [Loss: 0.094317]\n",
      "3514 [Loss: 0.091955]\n",
      "3515 [Loss: 0.098905]\n",
      "3516 [Loss: 0.094327]\n",
      "3517 [Loss: 0.094680]\n",
      "3518 [Loss: 0.094178]\n",
      "3519 [Loss: 0.100090]\n",
      "3520 [Loss: 0.090383]\n",
      "3521 [Loss: 0.094769]\n",
      "3522 [Loss: 0.096126]\n",
      "3523 [Loss: 0.089852]\n",
      "3524 [Loss: 0.091626]\n",
      "3525 [Loss: 0.095440]\n",
      "3526 [Loss: 0.094283]\n",
      "3527 [Loss: 0.099587]\n",
      "3528 [Loss: 0.096068]\n",
      "3529 [Loss: 0.098169]\n",
      "3530 [Loss: 0.090754]\n",
      "3531 [Loss: 0.094281]\n",
      "3532 [Loss: 0.095814]\n",
      "3533 [Loss: 0.089472]\n",
      "3534 [Loss: 0.089008]\n",
      "3535 [Loss: 0.095358]\n",
      "3536 [Loss: 0.088683]\n",
      "3537 [Loss: 0.093187]\n",
      "3538 [Loss: 0.099134]\n",
      "3539 [Loss: 0.093370]\n",
      "3540 [Loss: 0.091448]\n",
      "3541 [Loss: 0.090044]\n",
      "3542 [Loss: 0.088490]\n",
      "3543 [Loss: 0.094444]\n",
      "3544 [Loss: 0.094951]\n",
      "3545 [Loss: 0.095054]\n",
      "3546 [Loss: 0.090615]\n",
      "3547 [Loss: 0.089075]\n",
      "3548 [Loss: 0.092905]\n",
      "3549 [Loss: 0.090802]\n",
      "3550 [Loss: 0.096716]\n",
      "3551 [Loss: 0.097991]\n",
      "3552 [Loss: 0.095070]\n",
      "3553 [Loss: 0.095395]\n",
      "3554 [Loss: 0.093820]\n",
      "3555 [Loss: 0.094518]\n",
      "3556 [Loss: 0.093925]\n",
      "3557 [Loss: 0.091768]\n",
      "3558 [Loss: 0.100057]\n",
      "3559 [Loss: 0.097454]\n",
      "3560 [Loss: 0.094411]\n",
      "3561 [Loss: 0.096458]\n",
      "3562 [Loss: 0.096810]\n",
      "3563 [Loss: 0.091178]\n",
      "3564 [Loss: 0.093011]\n",
      "3565 [Loss: 0.094977]\n",
      "3566 [Loss: 0.093533]\n",
      "3567 [Loss: 0.094102]\n",
      "3568 [Loss: 0.090413]\n",
      "3569 [Loss: 0.093443]\n",
      "3570 [Loss: 0.092791]\n",
      "3571 [Loss: 0.091890]\n",
      "3572 [Loss: 0.096085]\n",
      "3573 [Loss: 0.094391]\n",
      "3574 [Loss: 0.092194]\n",
      "3575 [Loss: 0.089360]\n",
      "3576 [Loss: 0.098539]\n",
      "3577 [Loss: 0.091020]\n",
      "3578 [Loss: 0.089873]\n",
      "3579 [Loss: 0.092246]\n",
      "3580 [Loss: 0.096294]\n",
      "3581 [Loss: 0.090790]\n",
      "3582 [Loss: 0.090124]\n",
      "3583 [Loss: 0.091031]\n",
      "3584 [Loss: 0.093824]\n",
      "3585 [Loss: 0.096370]\n",
      "3586 [Loss: 0.092891]\n",
      "3587 [Loss: 0.089535]\n",
      "3588 [Loss: 0.091672]\n",
      "3589 [Loss: 0.092694]\n",
      "3590 [Loss: 0.093703]\n",
      "3591 [Loss: 0.093480]\n",
      "3592 [Loss: 0.096654]\n",
      "3593 [Loss: 0.092018]\n",
      "3594 [Loss: 0.097218]\n",
      "3595 [Loss: 0.093261]\n",
      "3596 [Loss: 0.090409]\n",
      "3597 [Loss: 0.089787]\n",
      "3598 [Loss: 0.092033]\n",
      "3599 [Loss: 0.094331]\n",
      "3600 [Loss: 0.094476]\n",
      "3601 [Loss: 0.096606]\n",
      "3602 [Loss: 0.098417]\n",
      "3603 [Loss: 0.097622]\n",
      "3604 [Loss: 0.092406]\n",
      "3605 [Loss: 0.090142]\n",
      "3606 [Loss: 0.096776]\n",
      "3607 [Loss: 0.096518]\n",
      "3608 [Loss: 0.093914]\n",
      "3609 [Loss: 0.093937]\n",
      "3610 [Loss: 0.095075]\n",
      "3611 [Loss: 0.099984]\n",
      "3612 [Loss: 0.093046]\n",
      "3613 [Loss: 0.091850]\n",
      "3614 [Loss: 0.097016]\n",
      "3615 [Loss: 0.094303]\n",
      "3616 [Loss: 0.088858]\n",
      "3617 [Loss: 0.094057]\n",
      "3618 [Loss: 0.095985]\n",
      "3619 [Loss: 0.095492]\n",
      "3620 [Loss: 0.093078]\n",
      "3621 [Loss: 0.096465]\n",
      "3622 [Loss: 0.091712]\n",
      "3623 [Loss: 0.088430]\n",
      "3624 [Loss: 0.096091]\n",
      "3625 [Loss: 0.094557]\n",
      "3626 [Loss: 0.096690]\n",
      "3627 [Loss: 0.090749]\n",
      "3628 [Loss: 0.081899]\n",
      "3629 [Loss: 0.095698]\n",
      "3630 [Loss: 0.097300]\n",
      "3631 [Loss: 0.095282]\n",
      "3632 [Loss: 0.096761]\n",
      "3633 [Loss: 0.091368]\n",
      "3634 [Loss: 0.093719]\n",
      "3635 [Loss: 0.097305]\n",
      "3636 [Loss: 0.098920]\n",
      "3637 [Loss: 0.092594]\n",
      "3638 [Loss: 0.086140]\n",
      "3639 [Loss: 0.097122]\n",
      "3640 [Loss: 0.094169]\n",
      "3641 [Loss: 0.093448]\n",
      "3642 [Loss: 0.089428]\n",
      "3643 [Loss: 0.098821]\n",
      "3644 [Loss: 0.087359]\n",
      "3645 [Loss: 0.093180]\n",
      "3646 [Loss: 0.095926]\n",
      "3647 [Loss: 0.092579]\n",
      "3648 [Loss: 0.092491]\n",
      "3649 [Loss: 0.096886]\n",
      "3650 [Loss: 0.088837]\n",
      "3651 [Loss: 0.091044]\n",
      "3652 [Loss: 0.099656]\n",
      "3653 [Loss: 0.095135]\n",
      "3654 [Loss: 0.088704]\n",
      "3655 [Loss: 0.092694]\n",
      "3656 [Loss: 0.092716]\n",
      "3657 [Loss: 0.095932]\n",
      "3658 [Loss: 0.091102]\n",
      "3659 [Loss: 0.093628]\n",
      "3660 [Loss: 0.091350]\n",
      "3661 [Loss: 0.093730]\n",
      "3662 [Loss: 0.093959]\n",
      "3663 [Loss: 0.095274]\n",
      "3664 [Loss: 0.088886]\n",
      "3665 [Loss: 0.091578]\n",
      "3666 [Loss: 0.096020]\n",
      "3667 [Loss: 0.099531]\n",
      "3668 [Loss: 0.096064]\n",
      "3669 [Loss: 0.090879]\n",
      "3670 [Loss: 0.092637]\n",
      "3671 [Loss: 0.092754]\n",
      "3672 [Loss: 0.093379]\n",
      "3673 [Loss: 0.099165]\n",
      "3674 [Loss: 0.096650]\n",
      "3675 [Loss: 0.088363]\n",
      "3676 [Loss: 0.090627]\n",
      "3677 [Loss: 0.093203]\n",
      "3678 [Loss: 0.094944]\n",
      "3679 [Loss: 0.091834]\n",
      "3680 [Loss: 0.089846]\n",
      "3681 [Loss: 0.096999]\n",
      "3682 [Loss: 0.088974]\n",
      "3683 [Loss: 0.087107]\n",
      "3684 [Loss: 0.092363]\n",
      "3685 [Loss: 0.094205]\n",
      "3686 [Loss: 0.091260]\n",
      "3687 [Loss: 0.093880]\n",
      "3688 [Loss: 0.088841]\n",
      "3689 [Loss: 0.095727]\n",
      "3690 [Loss: 0.090786]\n",
      "3691 [Loss: 0.099505]\n",
      "3692 [Loss: 0.093211]\n",
      "3693 [Loss: 0.098370]\n",
      "3694 [Loss: 0.089898]\n",
      "3695 [Loss: 0.092928]\n",
      "3696 [Loss: 0.090519]\n",
      "3697 [Loss: 0.094648]\n",
      "3698 [Loss: 0.091125]\n",
      "3699 [Loss: 0.092136]\n",
      "3700 [Loss: 0.089354]\n",
      "3701 [Loss: 0.091705]\n",
      "3702 [Loss: 0.088990]\n",
      "3703 [Loss: 0.095973]\n",
      "3704 [Loss: 0.090419]\n",
      "3705 [Loss: 0.093809]\n",
      "3706 [Loss: 0.097339]\n",
      "3707 [Loss: 0.098535]\n",
      "3708 [Loss: 0.098184]\n",
      "3709 [Loss: 0.099783]\n",
      "3710 [Loss: 0.096013]\n",
      "3711 [Loss: 0.094944]\n",
      "3712 [Loss: 0.094119]\n",
      "3713 [Loss: 0.095437]\n",
      "3714 [Loss: 0.091258]\n",
      "3715 [Loss: 0.096376]\n",
      "3716 [Loss: 0.092788]\n",
      "3717 [Loss: 0.090587]\n",
      "3718 [Loss: 0.104205]\n",
      "3719 [Loss: 0.095412]\n",
      "3720 [Loss: 0.092693]\n",
      "3721 [Loss: 0.096771]\n",
      "3722 [Loss: 0.095228]\n",
      "3723 [Loss: 0.095534]\n",
      "3724 [Loss: 0.093657]\n",
      "3725 [Loss: 0.097032]\n",
      "3726 [Loss: 0.098165]\n",
      "3727 [Loss: 0.098625]\n",
      "3728 [Loss: 0.095835]\n",
      "3729 [Loss: 0.096340]\n",
      "3730 [Loss: 0.090820]\n",
      "3731 [Loss: 0.091019]\n",
      "3732 [Loss: 0.095497]\n",
      "3733 [Loss: 0.099599]\n",
      "3734 [Loss: 0.096977]\n",
      "3735 [Loss: 0.093087]\n",
      "3736 [Loss: 0.091911]\n",
      "3737 [Loss: 0.085938]\n",
      "3738 [Loss: 0.096574]\n",
      "3739 [Loss: 0.099107]\n",
      "3740 [Loss: 0.090553]\n",
      "3741 [Loss: 0.093094]\n",
      "3742 [Loss: 0.087360]\n",
      "3743 [Loss: 0.092977]\n",
      "3744 [Loss: 0.083003]\n",
      "3745 [Loss: 0.093555]\n",
      "3746 [Loss: 0.091946]\n",
      "3747 [Loss: 0.096235]\n",
      "3748 [Loss: 0.092183]\n",
      "3749 [Loss: 0.091746]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750 [Loss: 0.098625]\n",
      "3751 [Loss: 0.093548]\n",
      "3752 [Loss: 0.096307]\n",
      "3753 [Loss: 0.090475]\n",
      "3754 [Loss: 0.092152]\n",
      "3755 [Loss: 0.093589]\n",
      "3756 [Loss: 0.091540]\n",
      "3757 [Loss: 0.091773]\n",
      "3758 [Loss: 0.093663]\n",
      "3759 [Loss: 0.098386]\n",
      "3760 [Loss: 0.100632]\n",
      "3761 [Loss: 0.097403]\n",
      "3762 [Loss: 0.097586]\n",
      "3763 [Loss: 0.090805]\n",
      "3764 [Loss: 0.093120]\n",
      "3765 [Loss: 0.087779]\n",
      "3766 [Loss: 0.093961]\n",
      "3767 [Loss: 0.089667]\n",
      "3768 [Loss: 0.091582]\n",
      "3769 [Loss: 0.092321]\n",
      "3770 [Loss: 0.088732]\n",
      "3771 [Loss: 0.090907]\n",
      "3772 [Loss: 0.089068]\n",
      "3773 [Loss: 0.089825]\n",
      "3774 [Loss: 0.090729]\n",
      "3775 [Loss: 0.090656]\n",
      "3776 [Loss: 0.094670]\n",
      "3777 [Loss: 0.091684]\n",
      "3778 [Loss: 0.092186]\n",
      "3779 [Loss: 0.093261]\n",
      "3780 [Loss: 0.098761]\n",
      "3781 [Loss: 0.095284]\n",
      "3782 [Loss: 0.093712]\n",
      "3783 [Loss: 0.088744]\n",
      "3784 [Loss: 0.094542]\n",
      "3785 [Loss: 0.088712]\n",
      "3786 [Loss: 0.090211]\n",
      "3787 [Loss: 0.093039]\n",
      "3788 [Loss: 0.084348]\n",
      "3789 [Loss: 0.092403]\n",
      "3790 [Loss: 0.087948]\n",
      "3791 [Loss: 0.097174]\n",
      "3792 [Loss: 0.088599]\n",
      "3793 [Loss: 0.091464]\n",
      "3794 [Loss: 0.089950]\n",
      "3795 [Loss: 0.094817]\n",
      "3796 [Loss: 0.092758]\n",
      "3797 [Loss: 0.099481]\n",
      "3798 [Loss: 0.098662]\n",
      "3799 [Loss: 0.096585]\n",
      "3800 [Loss: 0.099098]\n",
      "3801 [Loss: 0.096405]\n",
      "3802 [Loss: 0.086683]\n",
      "3803 [Loss: 0.091964]\n",
      "3804 [Loss: 0.094814]\n",
      "3805 [Loss: 0.091924]\n",
      "3806 [Loss: 0.094799]\n",
      "3807 [Loss: 0.089862]\n",
      "3808 [Loss: 0.096774]\n",
      "3809 [Loss: 0.096597]\n",
      "3810 [Loss: 0.096609]\n",
      "3811 [Loss: 0.094288]\n",
      "3812 [Loss: 0.092722]\n",
      "3813 [Loss: 0.095918]\n",
      "3814 [Loss: 0.087798]\n",
      "3815 [Loss: 0.098551]\n",
      "3816 [Loss: 0.092705]\n",
      "3817 [Loss: 0.094959]\n",
      "3818 [Loss: 0.090781]\n",
      "3819 [Loss: 0.091730]\n",
      "3820 [Loss: 0.091875]\n",
      "3821 [Loss: 0.091894]\n",
      "3822 [Loss: 0.093134]\n",
      "3823 [Loss: 0.093428]\n",
      "3824 [Loss: 0.096911]\n",
      "3825 [Loss: 0.098012]\n",
      "3826 [Loss: 0.087594]\n",
      "3827 [Loss: 0.096515]\n",
      "3828 [Loss: 0.096835]\n",
      "3829 [Loss: 0.093553]\n",
      "3830 [Loss: 0.089204]\n",
      "3831 [Loss: 0.092691]\n",
      "3832 [Loss: 0.093311]\n",
      "3833 [Loss: 0.093749]\n",
      "3834 [Loss: 0.093662]\n",
      "3835 [Loss: 0.095233]\n",
      "3836 [Loss: 0.092573]\n",
      "3837 [Loss: 0.090284]\n",
      "3838 [Loss: 0.097332]\n",
      "3839 [Loss: 0.093367]\n",
      "3840 [Loss: 0.097873]\n",
      "3841 [Loss: 0.094205]\n",
      "3842 [Loss: 0.092871]\n",
      "3843 [Loss: 0.091873]\n",
      "3844 [Loss: 0.099075]\n",
      "3845 [Loss: 0.095471]\n",
      "3846 [Loss: 0.093087]\n",
      "3847 [Loss: 0.094436]\n",
      "3848 [Loss: 0.093047]\n",
      "3849 [Loss: 0.087406]\n",
      "3850 [Loss: 0.089652]\n",
      "3851 [Loss: 0.094032]\n",
      "3852 [Loss: 0.094006]\n",
      "3853 [Loss: 0.092975]\n",
      "3854 [Loss: 0.091822]\n",
      "3855 [Loss: 0.104094]\n",
      "3856 [Loss: 0.097109]\n",
      "3857 [Loss: 0.091243]\n",
      "3858 [Loss: 0.092752]\n",
      "3859 [Loss: 0.095067]\n",
      "3860 [Loss: 0.093484]\n",
      "3861 [Loss: 0.097403]\n",
      "3862 [Loss: 0.096162]\n",
      "3863 [Loss: 0.093872]\n",
      "3864 [Loss: 0.090511]\n",
      "3865 [Loss: 0.094977]\n",
      "3866 [Loss: 0.096586]\n",
      "3867 [Loss: 0.092509]\n",
      "3868 [Loss: 0.090595]\n",
      "3869 [Loss: 0.097830]\n",
      "3870 [Loss: 0.095238]\n",
      "3871 [Loss: 0.096525]\n",
      "3872 [Loss: 0.090057]\n",
      "3873 [Loss: 0.103412]\n",
      "3874 [Loss: 0.095260]\n",
      "3875 [Loss: 0.091138]\n",
      "3876 [Loss: 0.094380]\n",
      "3877 [Loss: 0.090487]\n",
      "3878 [Loss: 0.092600]\n",
      "3879 [Loss: 0.094781]\n",
      "3880 [Loss: 0.092963]\n",
      "3881 [Loss: 0.093761]\n",
      "3882 [Loss: 0.089295]\n",
      "3883 [Loss: 0.092884]\n",
      "3884 [Loss: 0.092687]\n",
      "3885 [Loss: 0.095672]\n",
      "3886 [Loss: 0.098109]\n",
      "3887 [Loss: 0.093967]\n",
      "3888 [Loss: 0.087269]\n",
      "3889 [Loss: 0.097725]\n",
      "3890 [Loss: 0.093620]\n",
      "3891 [Loss: 0.095535]\n",
      "3892 [Loss: 0.093993]\n",
      "3893 [Loss: 0.092711]\n",
      "3894 [Loss: 0.096291]\n",
      "3895 [Loss: 0.087953]\n",
      "3896 [Loss: 0.084983]\n",
      "3897 [Loss: 0.091931]\n",
      "3898 [Loss: 0.092753]\n",
      "3899 [Loss: 0.086771]\n",
      "3900 [Loss: 0.091075]\n",
      "3901 [Loss: 0.084771]\n",
      "3902 [Loss: 0.094124]\n",
      "3903 [Loss: 0.094138]\n",
      "3904 [Loss: 0.098908]\n",
      "3905 [Loss: 0.090631]\n",
      "3906 [Loss: 0.085598]\n",
      "3907 [Loss: 0.098969]\n",
      "3908 [Loss: 0.091512]\n",
      "3909 [Loss: 0.089915]\n",
      "3910 [Loss: 0.100287]\n",
      "3911 [Loss: 0.090849]\n",
      "3912 [Loss: 0.096654]\n",
      "3913 [Loss: 0.093711]\n",
      "3914 [Loss: 0.096721]\n",
      "3915 [Loss: 0.099428]\n",
      "3916 [Loss: 0.092533]\n",
      "3917 [Loss: 0.092978]\n",
      "3918 [Loss: 0.093046]\n",
      "3919 [Loss: 0.088322]\n",
      "3920 [Loss: 0.094273]\n",
      "3921 [Loss: 0.084900]\n",
      "3922 [Loss: 0.092260]\n",
      "3923 [Loss: 0.093855]\n",
      "3924 [Loss: 0.085086]\n",
      "3925 [Loss: 0.094357]\n",
      "3926 [Loss: 0.092115]\n",
      "3927 [Loss: 0.094764]\n",
      "3928 [Loss: 0.089018]\n",
      "3929 [Loss: 0.093975]\n",
      "3930 [Loss: 0.090610]\n",
      "3931 [Loss: 0.092718]\n",
      "3932 [Loss: 0.095744]\n",
      "3933 [Loss: 0.094804]\n",
      "3934 [Loss: 0.092077]\n",
      "3935 [Loss: 0.094985]\n",
      "3936 [Loss: 0.085249]\n",
      "3937 [Loss: 0.090731]\n",
      "3938 [Loss: 0.090824]\n",
      "3939 [Loss: 0.094284]\n",
      "3940 [Loss: 0.089266]\n",
      "3941 [Loss: 0.090355]\n",
      "3942 [Loss: 0.100625]\n",
      "3943 [Loss: 0.088251]\n",
      "3944 [Loss: 0.096631]\n",
      "3945 [Loss: 0.084537]\n",
      "3946 [Loss: 0.090487]\n",
      "3947 [Loss: 0.087680]\n",
      "3948 [Loss: 0.092203]\n",
      "3949 [Loss: 0.088305]\n",
      "3950 [Loss: 0.092611]\n",
      "3951 [Loss: 0.090891]\n",
      "3952 [Loss: 0.095511]\n",
      "3953 [Loss: 0.092423]\n",
      "3954 [Loss: 0.091502]\n",
      "3955 [Loss: 0.093804]\n",
      "3956 [Loss: 0.091745]\n",
      "3957 [Loss: 0.092352]\n",
      "3958 [Loss: 0.098596]\n",
      "3959 [Loss: 0.097511]\n",
      "3960 [Loss: 0.092949]\n",
      "3961 [Loss: 0.092672]\n",
      "3962 [Loss: 0.093487]\n",
      "3963 [Loss: 0.096492]\n",
      "3964 [Loss: 0.100515]\n",
      "3965 [Loss: 0.096664]\n",
      "3966 [Loss: 0.095570]\n",
      "3967 [Loss: 0.086896]\n",
      "3968 [Loss: 0.090569]\n",
      "3969 [Loss: 0.095843]\n",
      "3970 [Loss: 0.095097]\n",
      "3971 [Loss: 0.100112]\n",
      "3972 [Loss: 0.094566]\n",
      "3973 [Loss: 0.091109]\n",
      "3974 [Loss: 0.094095]\n",
      "3975 [Loss: 0.087746]\n",
      "3976 [Loss: 0.092698]\n",
      "3977 [Loss: 0.086470]\n",
      "3978 [Loss: 0.096234]\n",
      "3979 [Loss: 0.092798]\n",
      "3980 [Loss: 0.090461]\n",
      "3981 [Loss: 0.091820]\n",
      "3982 [Loss: 0.094560]\n",
      "3983 [Loss: 0.097717]\n",
      "3984 [Loss: 0.092012]\n",
      "3985 [Loss: 0.091673]\n",
      "3986 [Loss: 0.089068]\n",
      "3987 [Loss: 0.095024]\n",
      "3988 [Loss: 0.092547]\n",
      "3989 [Loss: 0.095356]\n",
      "3990 [Loss: 0.098371]\n",
      "3991 [Loss: 0.090696]\n",
      "3992 [Loss: 0.088989]\n",
      "3993 [Loss: 0.095908]\n",
      "3994 [Loss: 0.094457]\n",
      "3995 [Loss: 0.093202]\n",
      "3996 [Loss: 0.095029]\n",
      "3997 [Loss: 0.089868]\n",
      "3998 [Loss: 0.090904]\n",
      "3999 [Loss: 0.087970]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "\t#create the output image directory\n",
    "\tif (os.path.isdir('images')==0):\n",
    "\t\tos.mkdir('images')\n",
    "\n",
    "\t#choose dataset\n",
    "\tdataset_name = 'mnist'#\n",
    "\n",
    "\t#create AE model\n",
    "\tarchitecture = 'convolutional'#'mlp'#'convolutional'#\n",
    "\tae_conv = autoencoder(dataset_name,architecture)#,\n",
    "\n",
    "\tae_conv.train(epochs=ae_conv.epochs, batch_size=64, sample_interval=100)\n",
    "\tplt.plot(ae_conv.error_list[30:])\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = 5\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_train = X_train/255.0\n",
    "noise = np.expand_dims(np.random.normal(scale=20/255, size=(n_images,X_train.shape[1],X_train.shape[2])),\n",
    "                                   axis=3)\n",
    "idx = np.random.randint(0, X_train.shape[0], n_images)\n",
    "test_imgs = X_train[idx,:,:,:] + noise\n",
    "n_images = test_imgs.shape[0]\n",
    "ae_mlp, ae_conv = load_model('ae_mlp.h5'), load_model('ae_convolutional.h5')\n",
    "output_imgs1 = ae_mlp.predict(test_imgs)\n",
    "output_imgs2 = ae_conv.predict(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams['axes.labelpad'] = 40 \n",
    "r = 3\n",
    "c = n_images\n",
    "fig, axs = plt.subplots(r, c)\n",
    "#fig.set_figheight(15)\n",
    "#fig.set_figwidth(15)\n",
    "for j in range(c):\n",
    "    axs[0,j].imshow(test_imgs[j, :,:,0], cmap='gray')\n",
    "    axs[0,j].set_xticks([])\n",
    "    axs[0,j].set_yticks([])\n",
    "    axs[1,j].imshow(output_imgs1[j, :,:,0], cmap='gray')\n",
    "    axs[1,j].set_xticks([])\n",
    "    axs[1,j].set_yticks([])\n",
    "    axs[2,j].imshow(output_imgs2[j, :,:,0], cmap='gray')\n",
    "    axs[2,j].set_xticks([])\n",
    "    axs[2,j].set_yticks([])\n",
    "\n",
    "axs[0,0].set_ylabel('Original', rotation=0, size='large')\n",
    "axs[1,0].set_ylabel('MLP', rotation=0, size='large')\n",
    "axs[2,0].set_ylabel('Conv', rotation=0, size='large')\n",
    "\n",
    "#fig.tight_layout()\n",
    "plt.savefig('results_z10.eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams['axes.labelpad'] = 40 \n",
    "r = 3\n",
    "c = n_images\n",
    "fig, axs = plt.subplots(r, c)\n",
    "#fig.set_figheight(15)\n",
    "#fig.set_figwidth(15)\n",
    "for j in range(c):\n",
    "    axs[0,j].imshow(test_imgs[j, :,:,0], cmap='gray')\n",
    "    axs[0,j].set_xticks([])\n",
    "    axs[0,j].set_yticks([])\n",
    "    axs[1,j].imshow(output_imgs1[j, :,:,0], cmap='gray')\n",
    "    axs[1,j].set_xticks([])\n",
    "    axs[1,j].set_yticks([])\n",
    "    axs[2,j].imshow(output_imgs2[j, :,:,0], cmap='gray')\n",
    "    axs[2,j].set_xticks([])\n",
    "    axs[2,j].set_yticks([])\n",
    "\n",
    "axs[0,0].set_ylabel('Original', rotation=0, size='large')\n",
    "axs[1,0].set_ylabel('MLP', rotation=0, size='large')\n",
    "axs[2,0].set_ylabel('Conv', rotation=0, size='large')\n",
    "\n",
    "#fig.tight_layout()\n",
    "plt.savefig('results_z100.eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams['axes.labelpad'] = 40 \n",
    "r = 3\n",
    "c = n_images\n",
    "fig, axs = plt.subplots(r, c)\n",
    "#fig.set_figheight(15)\n",
    "#fig.set_figwidth(15)\n",
    "for j in range(c):\n",
    "    axs[0,j].imshow(test_imgs[j, :,:,0], cmap='gray')\n",
    "    axs[0,j].set_xticks([])\n",
    "    axs[0,j].set_yticks([])\n",
    "    axs[1,j].imshow(output_imgs1[j, :,:,0], cmap='gray')\n",
    "    axs[1,j].set_xticks([])\n",
    "    axs[1,j].set_yticks([])\n",
    "    axs[2,j].imshow(output_imgs2[j, :,:,0], cmap='gray')\n",
    "    axs[2,j].set_xticks([])\n",
    "    axs[2,j].set_yticks([])\n",
    "\n",
    "axs[0,0].set_ylabel('Original', rotation=0, size='large')\n",
    "axs[1,0].set_ylabel('MLP', rotation=0, size='large')\n",
    "axs[2,0].set_ylabel('Conv', rotation=0, size='large')\n",
    "\n",
    "#fig.tight_layout()\n",
    "plt.savefig('results_z10_noisy.eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams['axes.labelpad'] = 40 \n",
    "r = 3\n",
    "c = n_images\n",
    "fig, axs = plt.subplots(r, c)\n",
    "#fig.set_figheight(15)\n",
    "#fig.set_figwidth(15)\n",
    "for j in range(c):\n",
    "    axs[0,j].imshow(test_imgs[j, :,:,0], cmap='gray')\n",
    "    axs[0,j].set_xticks([])\n",
    "    axs[0,j].set_yticks([])\n",
    "    axs[1,j].imshow(output_imgs1[j, :,:,0], cmap='gray')\n",
    "    axs[1,j].set_xticks([])\n",
    "    axs[1,j].set_yticks([])\n",
    "    axs[2,j].imshow(output_imgs2[j, :,:,0], cmap='gray')\n",
    "    axs[2,j].set_xticks([])\n",
    "    axs[2,j].set_yticks([])\n",
    "\n",
    "axs[0,0].set_ylabel('Original', rotation=0, size='large')\n",
    "axs[1,0].set_ylabel('MLP', rotation=0, size='large')\n",
    "axs[2,0].set_ylabel('Conv', rotation=0, size='large')\n",
    "\n",
    "#fig.tight_layout()\n",
    "plt.savefig('results_z100_noisy.eps', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class variational_autoencoder():\n",
    "    def __init__(self,dataset_name='mnist',architecture='mlp'):\n",
    "        \n",
    "        X_train = self.load_data(dataset_name)\n",
    "        optimizer = 'adadelta'#Adam(0.0002, 0.5) #\n",
    "\n",
    "        # image parameters\n",
    "        self.epochs = 30000\n",
    "        self.error_list = np.zeros((self.epochs,1))\n",
    "        self.img_rows = X_train.shape[1]\n",
    "        self.img_cols = X_train.shape[2]\n",
    "        self.img_channels = X_train.shape[3]\n",
    "        self.img_size = X_train.shape[1] * X_train.shape[2] * X_train.shape[3]\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.img_channels)\n",
    "        self.z_dim = 10\n",
    "        self.architecture = architecture\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.vae = self.build_vae()\n",
    "        self.vae.summary()\n",
    "\n",
    "    def build_vae(self):\n",
    "\n",
    "        n_pixels = self.img_rows*self.img_cols*self.img_channels\n",
    "\n",
    "        #BEGIN INSERT CODE\n",
    "        #encoder\n",
    "        input_img = ...\n",
    "        z = ...\n",
    "        # mean and variance parameters\n",
    "        z_mean = ...\n",
    "        z_log_var = ...\n",
    "\n",
    "        #sample the latent vector\n",
    "        z_rand = Lambda(sampling, output_shape=(self.z_dim,))([z_mean, z_log_var])\n",
    "        #save the encoder\n",
    "        self.encoder = Model(input_img, [z_mean, z_log_var, z_rand], name='encoder')\n",
    "\n",
    "        #build decoder\n",
    "        latent_inputs = Input(shape=(self.z_dim,), name='z_sampling')\n",
    "        y = Dense(512)(latent_inputs)\n",
    "        output_img = ...\n",
    "        self.decoder = Model(latent_inputs, output_img, name='decoder')\n",
    "\n",
    "        #build encoder + decoder (total model)\n",
    "        output_img = self.decoder(self.encoder(input_img)[2])\n",
    "        vae = Model(input_img, output_img, name='vae_mlp')\n",
    "\n",
    "        #create the total model\n",
    "        vae = Model(input_img,output_img)\n",
    "        #define the loss\n",
    "        vae_loss = self.vae_loss(input_img_flatten, output_img,z_mean,z_log_var)\n",
    "        vae.add_loss(vae_loss)\n",
    "        vae.compile(optimizer='adam')\n",
    "\n",
    "        return vae\n",
    "\n",
    "    def vae_loss(self,x,y,z_mean,z_log_var):\n",
    "        # BEGIN INSERT CODE\n",
    "\n",
    "        # reconstruction loss\n",
    "        reconstruction_loss = ...\n",
    "        # KL divergence\n",
    "        kl_loss = ...\n",
    "        # total loss\n",
    "        vae_loss = ...\n",
    "\n",
    "        # END FILL IN CODE\n",
    "        return vae_loss\n",
    "\n",
    "    def sampling(self,args):\n",
    "        #Reparameterization trick\n",
    "        z_mean, z_log_var = args\n",
    "        batch_size = K.shape(z_mean)[0]\n",
    "        # sample random vector of size (batch_size,z_dim)\n",
    "        epsilon = K.random_normal(shape=(batch_size,self.z_dim))\n",
    "        z_sigma = K.exp(0.5 * z_log_var)\n",
    "        z_epsilon = Multiply()([z_sigma,epsilon])\n",
    "        z_rand = Add()([z_mean,z_epsilon])\n",
    "        return z_rand\n",
    "\n",
    "    def load_data(self,dataset_name):\n",
    "        # Load the dataset\n",
    "        if(dataset_name == 'mnist'):\n",
    "            (X_train, _), (_, _) = mnist.load_data()\n",
    "        else:\n",
    "            print('Error, unknown database')\n",
    "\n",
    "        # normalise images between 0 and 1\n",
    "        X_train = X_train/255.0\n",
    "        #add a channel dimension, if need be (for mnist data)\n",
    "        if(X_train.ndim ==3):\n",
    "            X_train = np.expand_dims(X_train, axis=3)\n",
    "        return X_train\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "        \n",
    "        #load dataset\n",
    "        X_train = self.load_data(self.dataset_name)\n",
    "\n",
    "        for i in range(0,epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train variational autoencoder\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            curr_batch = X_train[idx,:,:,:]\n",
    "            # Autoencoder training\n",
    "            loss = self.vae.train_on_batch(curr_batch,None)\n",
    "\n",
    "            # print the losses\n",
    "            print(\"%d [Loss: %f]\" % (i, loss))\n",
    "            self.error_list[i] = loss\n",
    "\n",
    "            # Save some random generated images and the models at every sample_interval iterations\n",
    "            if (i % sample_interval == 0):\n",
    "                n_images = 5\n",
    "                idx = np.random.randint(0, X_train.shape[0], n_images)\n",
    "                test_imgs = X_train[idx,:,:,:]\n",
    "                self.reconstruct_images(test_imgs,'images/'+self.dataset_name+'_reconstruction_%06d.png' % i)\n",
    "                self.sample_images('images/'+self.dataset_name+'_random_samples_%06d.png' % i)\n",
    "\n",
    "    def reconstruct_images(self, test_imgs, image_filename):\n",
    "        n_images = test_imgs.shape[0]\n",
    "        #get output images\n",
    "        output_imgs = np.reshape(self.vae.predict( test_imgs ),(n_images,self.img_rows,self.img_cols,self.img_channels))\n",
    "        r = 2\n",
    "        c = n_images\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        for j in range(c):\n",
    "            #black and white images\n",
    "            axs[0,j].imshow(test_imgs[j, :,:,0], cmap='gray')\n",
    "            axs[0,j].axis('off')\n",
    "            axs[1,j].imshow(output_imgs[j, :,:,0], cmap='gray')\n",
    "            axs[1,j].axis('off')\n",
    "        fig.savefig(image_filename)\n",
    "        plt.close()\n",
    "\n",
    "    def sample_images(self, image_filename):\n",
    "\n",
    "        n_images = 8    #number of random images to sample\n",
    "        #get output images\n",
    "        z_sample = np.random.normal(0,1,(n_images,self.z_dim))\n",
    "        r = 1\n",
    "        c = n_images\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        for j in range(c):\n",
    "            x_decoded = np.reshape(self.decoder.predict(z_sample) , (n_images,self.img_rows,self.img_cols,self.img_channels))\n",
    "            #black and white images\n",
    "            axs[j].imshow(x_decoded[j, :,:,0], cmap='gray')\n",
    "            axs[j].axis('off')\n",
    "        fig.savefig(image_filename)\n",
    "        plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
